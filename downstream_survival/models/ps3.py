import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from .clam_mlp import ClamMLP
from typing import Dict, List, Tuple

class PS3(ClamMLP):
    """
    PS3 model, using Cross Attention mechanism for multimodal fusion

    Configuration parameters:
    - n_classes: Number of classes
    - input_dim: Input dimension
    - model_size: Model size ('small', 'big', '128*64', '64*32', '32*16', '16*8', '8*4', '4*2', '2*1')
    - dropout: Dropout rate
    - gate: Whether to use gated attention
    - inst_number: Number of positive/negative samples
    - instance_loss_fn: Instance loss function
    - subtyping: Whether it's a subtyping problem
    - cross_attn_dim: Cross Attention dimension, defaults to output_dim
    - num_heads: Number of attention heads (currently implemented as single head, reserved for future expansion)
    - cross_attn_dropout: Cross Attention dropout rate, default 0.1
    """
    
    def __init__(self, config):
        """
        Initialize PS3 model, set Cross Attention related parameters

        @param {Dict} config - Model configuration dictionary, containing all parameters required by the model
        """
        super().__init__(config)
        self.modality_order = sorted(self.used_modality)
        self.token_norm = nn.LayerNorm(self.output_dim).to(self.device)  # Token normalization
        self.qkv_proj = nn.Linear(self.output_dim, 3 * self.output_dim).to(self.device)
        self.modality_mlp_layers = nn.ModuleDict(
            {
                channel: nn.Linear(self.output_dim, self.output_dim) 
                for channel in self.modality_order
            }
        ).to(self.device)
        
        self.modality_fusion_layer = nn.Sequential(
                nn.Linear(len(self.modality_order) * self.output_dim, self.size[1]),
                nn.ReLU(),
                nn.Dropout(self.dropout),
                nn.Linear(self.size[1], self.n_classes)
        ).to(self.device)
        
    
    def forward(self, input_data, label):
        """
        ç»Ÿä¸€çš„å‰å‘ä¼ æ’­æ¥å£ï¼Œä½¿ç”¨ Cross Attention è¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾èåˆ
        
        Process:
        1. Extract features for each modality (WSI/TMA use CLAM, other modalities use transfer layer)
        2. Generate Q, K, V projections for each modality
        3. For each modality, use its Q to query all modalities' K, calculate attention weights
        4. Use attention weights to perform weighted summation of all modalities' V
        5. Concatenate all fused features and perform classification through fusion_prediction
        
        @param {torch.Tensor|Dict[str, torch.Tensor]} input_data - è¾“å…¥æ•°æ®
            - torch.Tensor: Single-modal features [N, D]
            - Dict[str, torch.Tensor]: Multimodal data dictionary, key is modality name
        @param {torch.Tensor} label - Label tensor for instance evaluation [1]
                
        @returns {Dict[str, Any]} Unified format result dictionary, contains:
            - Y_prob: Prediction probabilities [1, n_classes]
            - Y_hat: é¢„æµ‹ç±»åˆ« [1, 1]
            - å„æ¨¡æ€çš„ CLAM ç›¸å…³ç»“æœï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        """
        input_data, modalities_used_in_model = self._process_input_data(input_data)
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        result_kwargs = {}
        
        # æ”¶é›†æ‰€æœ‰æ¨¡æ€çš„ç‰¹å¾
        modality_features = {}
        for channel in modalities_used_in_model:
            features = None
            if channel == 'wsi=features':
                clam_result_kwargs = self._clam_forward(channel, input_data[channel], label)
                modality_features[channel] = clam_result_kwargs['features'].detach()
                for key, value in clam_result_kwargs.items():
                    result_kwargs[f'{channel}_{key}'] = value
            elif channel == 'tma=features':
                clam_result_kwargs = self._clam_forward(channel, input_data[channel], label)
                modality_features[channel] = clam_result_kwargs['features'].detach()
                for key, value in clam_result_kwargs.items():
                    result_kwargs[f'{channel}_{key}'] = value
            else:
                if channel not in self.transfer_layer:
                    self.transfer_layer[channel] = self.create_transfer_layer(input_data[channel].shape[1])
                modality_features[channel] = self.transfer_layer[channel](input_data[channel])
        
        # æ”¶é›†æ‰€æœ‰æ¨¡æ€ç‰¹å¾å¹¶æ‹¼æ¥: [num_modalities, output_dim]
        # æ¯ä¸ª modality_features[channel] æ˜¯ [1, output_dim]ï¼Œä½¿ç”¨ cat åœ¨ dim=0 ä¸Šæ‹¼æ¥
        h = torch.cat([modality_features[channel] for channel in self.modality_order], dim=0)  # [num_modalities, output_dim]
        # ğŸ”¹ Step 1: Token Normalization (å¯¹æ¯ä¸ªæ¨¡æ€çš„ token è¿›è¡Œ normalization)
        h = self.token_norm(h)  # [num_modalities, output_dim]
        
        # ğŸ”¹ Step 2: QKV Projection (å¹¶è¡Œè®¡ç®—æ‰€æœ‰æ¨¡æ€çš„ Q, K, V)
        qkv_h = self.qkv_proj(h)  # [num_modalities, 3 * output_dim]
        # ğŸ”¹ Step 3: Split Q, K, V
        q, k, v = qkv_h.chunk(3, dim=-1)  # æ¯ä¸ªéƒ½æ˜¯ [num_modalities, output_dim]
        
        # ğŸ”¹ Step 4: Cross Attention è®¡ç®—
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: Q @ K^T / sqrt(d_k)
        # q: [num_modalities, output_dim], k: [num_modalities, output_dim]
        # è¾“å‡º: [num_modalities, num_modalities] (æ¯ä¸ªæ¨¡æ€å¯¹æ‰€æœ‰æ¨¡æ€çš„æ³¨æ„åŠ›åˆ†æ•°)
        attn_scores = torch.mm(q, k.transpose(0, 1)) / np.sqrt(self.output_dim)  # [num_modalities, num_modalities]
        
        # åº”ç”¨ softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attention_weights = F.softmax(attn_scores, dim=-1)  # [num_modalities, num_modalities]
        
        # ä½¿ç”¨æ³¨æ„åŠ›æƒé‡å¯¹ V è¿›è¡ŒåŠ æƒæ±‚å’Œ
        # attention_weights: [num_modalities, num_modalities]
        # v: [num_modalities, output_dim]
        # è¾“å‡º: [num_modalities, output_dim] (æ¯ä¸ªæ¨¡æ€çš„èåˆåç‰¹å¾)
        h = torch.mm(attention_weights, v)  # [num_modalities, output_dim]
        
        # ğŸ”¹ Step 4.5: å¯¹æ¯ä¸ªæ¨¡æ€åº”ç”¨ç‹¬ç«‹çš„ MLP å’Œ normalization
        # ä¼˜åŒ–æ–¹æ¡ˆï¼šå…ˆåº”ç”¨æ‰€æœ‰ MLPï¼Œç„¶åä¸€æ¬¡æ€§åº”ç”¨ normalizationï¼ˆé«˜æ•ˆä¸”ç®€æ´ï¼‰
        # ç”±äºæ¯ä¸ªæ¨¡æ€çš„ MLP ä¸åŒï¼Œæ— æ³•å®Œå…¨å¹¶è¡ŒåŒ–ï¼Œä½† normalization å¯ä»¥æ‰¹é‡å¤„ç†
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ + cat æ¯”å¾ªç¯ + stack æ›´é«˜æ•ˆï¼ˆå‡å°‘ squeeze æ“ä½œï¼‰
        h_mlp_list = [
            self.modality_mlp_layers[channel](h[index:index+1, :])  # [1, output_dim]
            for index, channel in enumerate(self.modality_order)
        ]
        h_mlp = torch.cat(h_mlp_list, dim=0)  # [num_modalities, output_dim]
        # ä¸€æ¬¡æ€§å¯¹æ‰€æœ‰æ¨¡æ€åº”ç”¨ normalizationï¼ˆæ›´é«˜æ•ˆï¼Œä» num_modalities æ¬¡è°ƒç”¨å‡å°‘åˆ° 1 æ¬¡ï¼‰
        h = self.token_norm(h_mlp)  # [num_modalities, output_dim]
        
        # ğŸ”¹ Step 5: Flatten å¹¶æ‹¼æ¥æ‰€æœ‰æ¨¡æ€çš„èåˆç‰¹å¾
        h = h.view(1, -1)  # [1, num_modalities * output_dim]
        
        # ğŸ”¹ Step 6: é€šè¿‡èåˆé¢„æµ‹å±‚è¿›è¡Œåˆ†ç±»
        logits = self.modality_fusion_layer(h)  # [1, n_classes]
        Y_prob = F.softmax(logits, dim=1)
        Y_hat = torch.topk(logits, 1, dim=1)[1]
        
        # æ›´æ–°ç»“æœå­—å…¸
        result_kwargs['Y_prob'] = Y_prob
        result_kwargs['Y_hat'] = Y_hat
        
        return self._create_result_dict(logits, Y_prob, Y_hat, **result_kwargs)