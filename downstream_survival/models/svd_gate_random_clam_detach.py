import torch
import torch.nn as nn
import torch.nn.functional as F
from .svd_gate_random_clam import SVDGateRandomClam
import random
from typing import Dict, List, Tuple, Optional

class SVDGateRandomClamDetach(SVDGateRandomClam):
    """
    CLAM MLP Detach æ¨¡å‹
    
    é…ç½®å‚æ•°ï¼š
    - n_classes: ç±»åˆ«æ•°é‡
    - input_dim: è¾“å…¥ç»´åº¦
    - model_size: æ¨¡å‹å¤§å° ('small', 'big', '128*64', '64*32', '32*16', '16*8', '8*4', '4*2', '2*1')
    - dropout: dropoutç‡
    - gate: æ˜¯å¦ä½¿ç”¨é—¨æ§æ³¨æ„åŠ›
    - inst_number: æ­£è´Ÿæ ·æœ¬é‡‡æ ·æ•°é‡
    - instance_loss_fn: å®ä¾‹æŸå¤±å‡½æ•°
    - subtyping: æ˜¯å¦ä¸ºå­ç±»å‹é—®é¢˜
    """
    
    def __init__(self, config):
        super().__init__(config)

    def forward(self, input_data, label):
        """
        ç»Ÿä¸€çš„å‰å‘ä¼ æ’­æ¥å£
        
        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œå¯ä»¥æ˜¯ï¼š
                - torch.Tensor: å•æ¨¡æ€ç‰¹å¾ [N, D]
                - Dict[str, torch.Tensor]: å¤šæ¨¡æ€æ•°æ®å­—å…¸
            label: æ ‡ç­¾ï¼ˆç”¨äºå®ä¾‹è¯„ä¼°ï¼‰
                
        Returns:
            Dict[str, Any]: ç»Ÿä¸€æ ¼å¼çš„ç»“æœå­—å…¸
        """
        input_data, modalities_used_in_model = self._process_input_data(input_data)
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        result_kwargs = {
            "raw_features_dict": {k: v.detach().clone() for k, v in input_data.items()}
        }
        
        # åˆå§‹åŒ–èåˆç‰¹å¾
        features_dict = {}
        for channel in modalities_used_in_model:
            if channel == 'wsi=features':
                clam_result_kwargs = self._clam_forward(channel, input_data[channel], label)
                for key, value in clam_result_kwargs.items():
                    result_kwargs[f'{channel}_{key}'] = value
                features_dict[channel] = clam_result_kwargs['features'].detach()
            elif channel == 'tma=features':
                clam_result_kwargs = self._clam_forward(channel, input_data[channel], label)
                for key, value in clam_result_kwargs.items():
                    result_kwargs[f'{channel}_{key}'] = value
                features_dict[channel] = clam_result_kwargs['features'].detach()
            else:
                if channel not in self.transfer_layer:
                    self.transfer_layer[channel] = self.create_transfer_layer(input_data[channel].shape[1])
                features_dict[channel] = self.transfer_layer[channel](input_data[channel])
        
        # ğŸ“Š ä¿å­˜CLAMåçš„ç»“æœï¼ˆæ¯ä¸ªmodalityçš„å€¼ï¼‰
        result_kwargs['original_features_dict'] = {k: v.detach().clone() for k, v in features_dict.items()}
        
        if self.enable_svd:
            if not hasattr(self, 'alignment_features'):
                self.alignment_features = []
            features_dict = self.align_forward(features_dict)
            # ğŸ“Š ä¿å­˜SVDåçš„ç»“æœï¼ˆæ¯ä¸ªmodalityçš„å€¼ï¼‰
            result_kwargs['aligned_svd_features_dict'] = {k: v.detach().clone() for k, v in features_dict.items()}
            if self.return_svd_features:
                return {
                    'features': result_kwargs['original_features_dict'],
                    'aligned_features': features_dict,
                }
            self.alignment_features.append(features_dict)
            if self.enable_dynamic_gate:
                result = self.gated_forward(features_dict, label)
                # ğŸ“Š ä¿å­˜Dynamic Gateåçš„ç»“æœï¼ˆæ¯ä¸ªmodalityçš„å€¼ï¼‰
                result_kwargs['svd_gated_features_dict'] = {k: v.detach().clone() for k, v in result['gated_features'].items()}
                for key, value in result.items():
                    result_kwargs[f'gated_{key}'] = value
                features_dict = result['gated_features']
        else:
            if self.enable_dynamic_gate:
                result = self.gated_forward(features_dict, label)
                # ğŸ“Š ä¿å­˜Dynamic Gateåçš„ç»“æœï¼ˆæ¯ä¸ªmodalityçš„å€¼ï¼‰
                result_kwargs['svd_gated_features_dict'] = {k: v.detach().clone() for k, v in result['gated_features'].items()}
                for key, value in result.items():
                    result_kwargs[f'gated_{key}'] = value
                features_dict = result['gated_features']
                
        if self.enable_random_loss and self.training:
            sorted_features_dict_keys = sorted(features_dict.keys())
            drop_modality = random.sample(sorted_features_dict_keys, random.randint(1, len(features_dict)-1))
            # ğŸ“Š ä¿å­˜randomæ“ä½œçš„ä¿¡æ¯
            result_kwargs['random_sorted_keys'] = sorted_features_dict_keys
            result_kwargs['random_drop_modality'] = drop_modality
            result_kwargs['random_n'] = len(drop_modality)
            
            h_partial = []
            for modality in sorted_features_dict_keys:
                if modality not in drop_modality:
                    h_partial.append(features_dict[modality])
                else:
                    h_partial.append(torch.zeros_like(features_dict[modality]).to(self.device))
            h_partial = torch.cat(h_partial, dim=1).to(self.device)
            # ğŸ“Š ä¿å­˜randomåçš„h
            result_kwargs['h_random'] = h_partial.detach().clone()
            logits = self.fusion_prediction(h_partial.detach())
            result_kwargs['random_partial_loss'] = self.base_loss_fn(logits, label)
            
        h = torch.cat([features_dict[mod] for mod in sorted(features_dict.keys())], dim=1).to(self.device)
        # ğŸ“Š ä¿å­˜æœ€ç»ˆh
        result_kwargs['h'] = h.detach().clone()
        
        logits = self.fusion_prediction(h.detach())
        Y_prob = F.softmax(logits, dim = 1)
        Y_hat = torch.topk(logits, 1, dim = 1)[1]
        
        # æ›´æ–°ç»“æœå­—å…¸
        result_kwargs['Y_prob'] = Y_prob
        result_kwargs['Y_hat'] = Y_hat
        
        return self._create_result_dict(logits, Y_prob, Y_hat, **result_kwargs)
