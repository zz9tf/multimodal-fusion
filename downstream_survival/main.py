#!/usr/bin/env python3
"""
å¤šæ¨¡æ€ç”Ÿå­˜çŠ¶æ€é¢„æµ‹ä¸»ç¨‹åº
ä¸“æ³¨äº WSI + TMA å¤šæ¨¡æ€ç”Ÿå­˜çŠ¶æ€é¢„æµ‹ä»»åŠ¡
"""

from __future__ import print_function

import argparse
import os
from datetime import datetime
import pandas as pd
import numpy as np
import json
import torch
from torch.utils.data import Subset

# å†…ç½®å·¥å…·å‡½æ•°ï¼Œå‡å°‘å¤–éƒ¨ä¾èµ–
import pickle

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
# Add which folder your main.py is located as root_dir
root_dir = '/home/zheng/zheng/multimodal-fusion/downstream_survival'
sys.path.append(root_dir)

# å†…éƒ¨å¯¼å…¥
from trainer import Trainer
from datasets.multimodal_dataset import MultimodalDataset

def save_pkl(filename, data):
    """ä¿å­˜pickleæ–‡ä»¶"""
    with open(filename, 'wb') as f:
        pickle.dump(data, f)

def load_pkl(filename):
    """åŠ è½½pickleæ–‡ä»¶"""
    with open(filename, 'rb') as f:
        return pickle.load(f)

def _get_model_specific_config(args):
    """æ ¹æ®æ¨¡å‹ç±»å‹è·å–ç‰¹å®šé…ç½®"""
    model_type = args.model_type
    
    mil_config = {
        'model_size': args.model_size,
        'return_features': args.return_features,
    }
    clam_config = {
        'gate': args.gate,
        'base_weight': args.base_weight,
        'inst_loss_fn': args.inst_loss_fn,
        'model_size': args.model_size,
        'subtyping': args.subtyping,
        'inst_number': args.inst_number,
        'return_features': args.return_features,
        'attention_only': args.attention_only
    }
    auc_config = {
        'auc_loss_weight': args.auc_loss_weight,
    }
    transfer_layer_config = {
        'output_dim': args.output_dim,
    }
    svd_config = {
        'enable_svd': args.enable_svd,
        'alignment_layer_num': args.alignment_layer_num,
        'lambda1': args.lambda1,
        'lambda2': args.lambda2,
        'tau1': args.tau1,
        'tau2': args.tau2,
    }
    dynamic_gate_config = {
        'enable_dynamic_gate': args.enable_dynamic_gate,
        'confidence_weight': args.confidence_weight,
        'feature_weight_weight': args.feature_weight_weight,
    }
    random_loss_config = {
        'enable_random_loss': args.enable_random_loss,
        'weight_random_loss': args.weight_random_loss,
    }
    if model_type == 'mil':
        return {
            **mil_config,
        }
    elif model_type == 'clam':
        return {
            **clam_config,
        }
    elif model_type == 'auc_clam':
        return {
            **clam_config,
            **auc_config,
        }
    elif model_type == 'clam_mlp':
        return {
            **clam_config,
            **transfer_layer_config
        }
    elif model_type == 'clam_mlp_detach':
        return {
            **clam_config,
            **transfer_layer_config,
        }
    elif model_type == 'svd_gate_random_clam_detach':
        return {
            **clam_config,
            **transfer_layer_config,
            **svd_config,
            **dynamic_gate_config,
            **random_loss_config,
        }
    elif model_type == 'gate_shared_mil':
        return {
            **mil_config,
            **dynamic_gate_config,
        }
    elif model_type == 'gate_mil':
        return {
            **mil_config,
            **dynamic_gate_config,
        }
    elif model_type == 'gate_auc_mil':
        return {
            **mil_config,
            **dynamic_gate_config,
            **auc_config,
        }
    elif model_type == 'gate_mil_detach':
        return {
            **mil_config,
            **dynamic_gate_config,
        }
    else:
        # ä¸ºå…¶ä»–æ¨¡å‹ç±»å‹è¿”å›ç©ºé…ç½®ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
        return {}

def _parse_aligned_channels(aligned_channels_list):
    """è§£æå¯¹é½é€šé“å‚æ•°"""
    if not aligned_channels_list:
        return {}
    
    align_channels = {}
    for item in aligned_channels_list:
        if '=' in item:
            key, value = item.split('=', 1)
            align_channels[key] = value
        else:
            # å¦‚æœæ²¡æœ‰ç­‰å·ï¼Œå‡è®¾key=valueç›¸åŒ
            align_channels[item] = item
    
    return align_channels

def seed_torch(seed=7):
    """è®¾ç½®éšæœºç§å­"""
    import random
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if device.type == 'cuda':
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

def load_dataset_split(dataset_split_path):
    """
    ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†åˆ†å‰²ä¿¡æ¯
    
    Args:
        dataset_split_path (str): æ•°æ®é›†åˆ†å‰²JSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: åŒ…å«train/teståˆ†å‰²çš„å­—å…¸ï¼Œæ ¼å¼ä¸º {'train': [patient_ids], 'test': [patient_ids]}
    """
    if not os.path.exists(dataset_split_path):
        raise FileNotFoundError(f"æ•°æ®é›†åˆ†å‰²æ–‡ä»¶ä¸å­˜åœ¨: {dataset_split_path}")
    
    with open(dataset_split_path, 'r') as f:
        split_data = json.load(f)
    
    # å°†JSONæ•°æ®è½¬æ¢ä¸ºtrain/teståˆ†å‰²
    train_patients = []
    test_patients = []
    
    for item in split_data:
        patient_id = item['patient_id']
        dataset_type = item['dataset']
        
        if dataset_type == 'training':
            train_patients.append(patient_id)
        elif dataset_type == 'test':
            test_patients.append(patient_id)
    
    return {
        'train': train_patients,
        'test': test_patients
    }

def create_k_fold_splits(dataset, k=10, seed=42, fixed_test_split=None):
    """
    åˆ›å»ºk-foldäº¤å‰éªŒè¯åˆ†å‰²
    
    Args:
        dataset: æ•°æ®é›†å¯¹è±¡
        k (int): foldæ•°é‡
        seed (int): éšæœºç§å­
        fixed_test_split (dict, optional): å›ºå®šçš„æµ‹è¯•é›†åˆ†å‰²ï¼Œæ ¼å¼ä¸º {'train': [patient_ids], 'test': [patient_ids]}
    
    Returns:
        list: åŒ…å«æ¯ä¸ªfoldçš„train/val/testç´¢å¼•çš„åˆ—è¡¨
    """
    from sklearn.model_selection import StratifiedKFold
    
    # è·å–æ‰€æœ‰æ ·æœ¬çš„æ ‡ç­¾å’Œæ‚£è€…ID
    labels = []
    patient_ids = []
    
    for i in range(len(dataset)):
        # è·å–æ ·æœ¬æ•°æ®
        sample = dataset[i]
        
        # ä»æ•°æ®é›†ä¸­è·å–æ ‡ç­¾
        if hasattr(dataset, 'get_label'):
            label = dataset.get_label(i)
        elif isinstance(sample, dict) and 'label' in sample:
            label = sample['label']
        else:
            # å‡è®¾æ˜¯å…ƒç»„æ ¼å¼ (data, label)
            _, label = sample
        
        labels.append(label)
        
        # è·å–æ‚£è€…IDï¼ˆå‡è®¾æ•°æ®é›†æœ‰get_patient_idæ–¹æ³•ï¼Œæˆ–è€…ä»æ ·æœ¬ä¸­è·å–ï¼‰
        if hasattr(dataset, 'get_patient_id'):
            patient_id = dataset.get_patient_id(i)
        elif isinstance(sample, dict) and 'patient_id' in sample:
            patient_id = sample['patient_id']
        else:
            # å¦‚æœæ²¡æœ‰æ‚£è€…IDï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºID
            patient_id = str(i)
        patient_ids.append(patient_id)
    
    labels = np.array(labels)
    patient_ids = np.array(patient_ids)
    
    splits = []
    
    if fixed_test_split is not None:
        # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•é›†åˆ†å‰²
        print(f"ğŸ”’ ä½¿ç”¨å›ºå®šæµ‹è¯•é›†åˆ†å‰²")
        print(f"ğŸ“Š å›ºå®šè®­ç»ƒé›†æ‚£è€…æ•°: {len(fixed_test_split['train'])}")
        print(f"ğŸ“Š å›ºå®šæµ‹è¯•é›†æ‚£è€…æ•°: {len(fixed_test_split['test'])}")
        
        # æ‰¾åˆ°æµ‹è¯•é›†å¯¹åº”çš„ç´¢å¼•
        test_indices = []
        for test_patient_id in fixed_test_split['test']:
            test_idx = np.where(patient_ids == test_patient_id)[0]
            if len(test_idx) > 0:
                test_indices.extend(test_idx)
        
        test_indices = np.array(test_indices)
        
        # æ‰¾åˆ°è®­ç»ƒé›†å¯¹åº”çš„ç´¢å¼•
        train_indices = []
        for train_patient_id in fixed_test_split['train']:
            train_idx = np.where(patient_ids == train_patient_id)[0]
            if len(train_idx) > 0:
                train_indices.extend(train_idx)
        
        train_indices = np.array(train_indices)
        
        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œk-foldäº¤å‰éªŒè¯
        train_labels = labels[train_indices]
        
        # åˆ›å»ºåˆ†å±‚k-foldåˆ†å‰²
        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)
        
        for fold_idx, (fold_train_idx, fold_val_idx) in enumerate(skf.split(train_indices, train_labels)):
            # è½¬æ¢ä¸ºå®é™…ç´¢å¼•
            actual_train_idx = train_indices[fold_train_idx]
            actual_val_idx = train_indices[fold_val_idx]
            
            splits.append({
                'train': actual_train_idx,
                'val': actual_val_idx,
                'test': test_indices  # æµ‹è¯•é›†å§‹ç»ˆç›¸åŒ
            })
    else:
        # åŸå§‹çš„åˆ†å‰²æ–¹å¼ï¼šå°†æµ‹è¯•é›†è¿›ä¸€æ­¥åˆ†ä¸ºéªŒè¯é›†å’Œæµ‹è¯•é›†
        print(f"ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿk-foldåˆ†å‰²")
        
        # åˆ›å»ºåˆ†å±‚k-foldåˆ†å‰²
        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)
        
        for fold_idx, (train_idx, test_idx) in enumerate(skf.split(range(len(dataset)), labels)):
            # å°†æµ‹è¯•é›†è¿›ä¸€æ­¥åˆ†ä¸ºéªŒè¯é›†å’Œæµ‹è¯•é›†
            test_labels = labels[test_idx]
            val_test_skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)
            val_idx, test_idx_final = next(val_test_skf.split(test_idx, test_labels))
            
            # è½¬æ¢ä¸ºå®é™…ç´¢å¼•
            val_idx = test_idx[val_idx]
            test_idx_final = test_idx[test_idx_final]
            
            splits.append({
                'train': train_idx,
                'val': val_idx, 
                'test': test_idx_final
            })
    
    return splits

def parse_channels(channels):
    """
    è§£æchannelsåˆ—è¡¨ï¼Œå°†ç®€åŒ–çš„é€šé“åç§°æ˜ å°„ä¸ºå®Œæ•´çš„HDF5è·¯å¾„
    
    æ”¯æŒçš„é€šé“ç±»å‹ï¼š
    - WSI: 'wsi' -> 'wsi=features'
    - TMA Features: 'tma', 'cd163', 'cd3', 'cd56', 'cd68', 'cd8', 'he', 'mhc1', 'pdl1'
    - TMA Patches: 'tma_patches', 'cd163_patches', 'cd3_patches', etc.
    - Clinical: 'clinical', 'clinical_ori', 'clinical_mask', 'clinical_ori_mask'
    - Pathological: 'pathological', 'pathological_ori', 'pathological_mask', 'pathological_ori_mask'
    - Blood: 'blood', 'blood_ori', 'blood_mask', 'blood_ori_mask'
    - ICD: 'icd', 'icd_ori', 'icd_mask', 'icd_ori_mask'
    - TMA Cell Density: 'tma_cell_density', 'tma_cell_density_ori', 'tma_cell_density_mask', 'tma_cell_density_ori_mask'
    
    Args:
        channels (List[str]): é€šé“åç§°åˆ—è¡¨
        
    Returns:
        List[str]: è§£æåçš„å®Œæ•´é€šé“è·¯å¾„åˆ—è¡¨
        
    Raises:
        ValueError: å½“è¾“å…¥é€šé“åç§°æ— æ•ˆæ—¶
    """
    if not channels:
        return []
    
    # TMAé€šé“å®šä¹‰
    TMA_CHANNELS = ['cd163', 'cd3', 'cd56', 'cd68', 'cd8', 'he', 'mhc1', 'pdl1']
    
    # æ”¯æŒçš„é€šé“ç±»å‹æ˜ å°„
    CHANNEL_MAPPINGS = {
        # WSIé€šé“
        'wsi': ['wsi=features'],
        
        # TMA Featuresé€šé“
        'tma': [f'tma={channel}=features' for channel in TMA_CHANNELS],
        
        # TMA Patchesé€šé“
        'tma_patches': [f'tma={channel}=patches' for channel in TMA_CHANNELS],
        
        # Clinicalé€šé“
        'clinical': ['clinical=val'],
        'clinical_ori': ['clinical=ori_val'],
        'clinical_mask': ['clinical=val', 'clinical=mask'],
        'clinical_ori_mask': ['clinical=ori_val', 'clinical=mask'],
        
        # Pathologicalé€šé“
        'pathological': ['pathological=val'],
        'pathological_ori': ['pathological=ori_val'],
        'pathological_mask': ['pathological=val', 'pathological=mask'],
        'pathological_ori_mask': ['pathological=ori_val', 'pathological=mask'],
        
        # Bloodé€šé“
        'blood': ['blood=val'],
        'blood_ori': ['blood=ori_val'],
        'blood_mask': ['blood=val', 'blood=mask'],
        'blood_ori_mask': ['blood=ori_val', 'blood=mask'],
        
        # ICDé€šé“
        'icd': ['icd=val'],
        'icd_ori': ['icd=ori_val'],
        'icd_mask': ['icd=val', 'icd=mask'],
        'icd_ori_mask': ['icd=ori_val', 'icd=mask'],
        
        # TMA Cell Densityé€šé“
        'tma_cell_density': ['tma_cell_density=val'],
        'tma_cell_density_ori': ['tma_cell_density=ori_val'],
        'tma_cell_density_mask': ['tma_cell_density=val', 'tma_cell_density=mask'],
        'tma_cell_density_ori_mask': ['tma_cell_density=ori_val', 'tma_cell_density=mask'],
    }
    
    # æ·»åŠ å•ä¸ªTMAé€šé“çš„æ˜ å°„
    for channel in TMA_CHANNELS:
        CHANNEL_MAPPINGS[channel] = [f'tma={channel}=features']
        CHANNEL_MAPPINGS[f'{channel}_patches'] = [f'tma={channel}=patches']
    
    parsed_channels = []
    invalid_channels = []
    
    for channel in channels:
        if channel in CHANNEL_MAPPINGS:
            parsed_channels.extend(CHANNEL_MAPPINGS[channel])
        elif '=' in channel:  # å·²ç»æ˜¯å®Œæ•´è·¯å¾„æ ¼å¼
            parsed_channels.append(channel)
        else:
            invalid_channels.append(channel)
    
    # éªŒè¯æ— æ•ˆé€šé“
    if invalid_channels:
        available_channels = list(CHANNEL_MAPPINGS.keys())
        raise ValueError(
            f"âŒ æ— æ•ˆçš„é€šé“åç§°: {invalid_channels}\n"
            f"ğŸ“‹ æ”¯æŒçš„é€šé“ç±»å‹: {available_channels}\n"
            f"ğŸ’¡ æç¤º: é€šé“åç§°ä¸åŒºåˆ†å¤§å°å†™ï¼Œæ”¯æŒå•ä¸ªé€šé“æˆ–ç»„åˆé€šé“"
        )
    
    return parsed_channels

def get_available_channels():
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„é€šé“ç±»å‹åˆ—è¡¨
    
    Returns:
        Dict[str, List[str]]: æŒ‰ç±»åˆ«åˆ†ç»„çš„å¯ç”¨é€šé“å­—å…¸
    """
    TMA_CHANNELS = ['cd163', 'cd3', 'cd56', 'cd68', 'cd8', 'he', 'mhc1', 'pdl1']
    
    return {
        'WSIé€šé“': ['wsi'],
        'TMA Featuresé€šé“': ['tma'] + TMA_CHANNELS,
        'TMA Patchesé€šé“': ['tma_patches'] + [f'{ch}_patches' for ch in TMA_CHANNELS],
        'Clinicalé€šé“': ['clinical', 'clinical_ori', 'clinical_mask', 'clinical_ori_mask'],
        'Pathologicalé€šé“': ['pathological', 'pathological_ori', 'pathological_mask', 'pathological_ori_mask'],
        'Bloodé€šé“': ['blood', 'blood_ori', 'blood_mask', 'blood_ori_mask'],
        'ICDé€šé“': ['icd', 'icd_ori', 'icd_mask', 'icd_ori_mask'],
        'TMA Cell Densityé€šé“': ['tma_cell_density', 'tma_cell_density_ori', 'tma_cell_density_mask', 'tma_cell_density_ori_mask']
    }

def print_available_channels():
    """
    æ‰“å°æ‰€æœ‰å¯ç”¨çš„é€šé“ç±»å‹ï¼Œç”¨äºè°ƒè¯•å’Œå¸®åŠ©
    """
    channels = get_available_channels()
    print("ğŸ” å¯ç”¨çš„é€šé“ç±»å‹:")
    print("=" * 50)
    
    for category, channel_list in channels.items():
        print(f"\nğŸ“ {category}:")
        for channel in channel_list:
            print(f"  â€¢ {channel}")
    
    print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print("  â€¢ å•ä¸ªé€šé“: ['wsi', 'clinical']")
    print("  â€¢ ç»„åˆé€šé“: ['tma', 'blood_mask']")
    print("  â€¢ å®Œæ•´è·¯å¾„: ['wsi=features', 'clinical=val']")

def main(args, configs):
    """ä¸»å‡½æ•°"""
    # ä»é…ç½®ä¸­è·å–å‚æ•°
    experiment_config = configs['experiment_config']
    
    # åŠ è½½æ•°æ®é›†
    print('\nLoad Dataset')
    if not experiment_config['data_root_dir']:
        raise ValueError('data_root_dir is required')
    if not os.path.exists(experiment_config['data_root_dir']):
        raise ValueError('data_root_dir does not exist')
    
    print('data_root_dir: ', os.path.abspath(experiment_config['data_root_dir']))

    # åˆ›å»ºå¤šæ¨¡æ€æ•°æ®é›†
    print(f"Target channels: {experiment_config['target_channels']}")
    
    # æ„å»ºchannelsåˆ—è¡¨
    channels = args.target_channels
    
    # æµ‹è¯•parse_channelså‡½æ•°
    try:
        parsed_channels = parse_channels(channels)
        print(f"âœ… æˆåŠŸè§£æé€šé“: {len(parsed_channels)} ä¸ª")
        print(f"ğŸ“‹ åŸå§‹é€šé“: {channels}")
        print(f"ğŸ”— è§£æåé€šé“: {parsed_channels}")
    except ValueError as e:
        print(f"âŒ é€šé“è§£æé”™è¯¯: {e}")
        print_available_channels()
        return
    
    # æ„å»ºalign_channelsæ˜ å°„
    align_channels = _parse_aligned_channels(args.aligned_channels)
    
    print(f"Channels: {channels}")
    print(f"Align channels: {align_channels}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dataset = MultimodalDataset(
        csv_path=experiment_config['csv_path'],
        data_root_dir=experiment_config['data_root_dir'],
        channels=channels,
        align_channels=align_channels,
        alignment_model_path=experiment_config['alignment_model_path'],
        device=device,
        print_info=True
    )
    
    # åˆ›å»ºç»“æœç›®å½•
    if not os.path.isdir(args.results_dir):
        os.mkdir(args.results_dir)

    # åˆ›å»ºk-foldåˆ†å‰²
    print(f'\nCreating {args.k}-fold cross-validation splits...')
    print(f"ğŸ”§ åˆ†å‰²æ¨¡å¼: {args.split_mode}")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å›ºå®šæµ‹è¯•é›†
    fixed_test_split = None
    if args.split_mode == 'fixed':
        if not args.dataset_split_path:
            raise ValueError("âŒ ä½¿ç”¨å›ºå®šæµ‹è¯•é›†æ¨¡å¼æ—¶ï¼Œå¿…é¡»æä¾› --dataset_split_path å‚æ•°")
        print(f"ğŸ“ åŠ è½½å›ºå®šæµ‹è¯•é›†åˆ†å‰²: {args.dataset_split_path}")
        fixed_test_split = load_dataset_split(args.dataset_split_path)
        print(f"âœ… æˆåŠŸåŠ è½½å›ºå®šæµ‹è¯•é›†åˆ†å‰²")
    elif args.split_mode == 'random':
        print(f"ğŸ² ä½¿ç”¨éšæœºåˆ†å‰²æ¨¡å¼")
    else:
        raise ValueError(f"âŒ ä¸æ”¯æŒçš„åˆ†å‰²æ¨¡å¼: {args.split_mode}")
    
    splits = create_k_fold_splits(dataset, k=args.k, seed=args.seed, fixed_test_split=fixed_test_split)
    print(f'âœ… Created {len(splits)} folds')

    # ç¡®å®šfoldèŒƒå›´
    start = 0
    end = args.k

    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = Trainer(
        configs=configs,
        log_dir=os.path.join(args.results_dir, 'training_logs')
    )

    # å­˜å‚¨ç»“æœ
    all_test_auc = []
    all_val_auc = []
    all_test_acc = []
    all_val_acc = []
    folds = np.arange(start, end)
    
    # è®­ç»ƒæ¯ä¸ªfold
    for i in folds:
        print(f'\n{"="*60}')
        print(f'Training Fold {i+1}/{args.k}')
        print(f'{"="*60}')
        
        seed_torch(args.seed)
        
        # è·å–å½“å‰foldçš„åˆ†å‰²
        split = splits[i]
        train_idx = split['train']
        val_idx = split['val']
        test_idx = split['test']
        
        print(f'Train samples: {len(train_idx)}')
        print(f'Val samples: {len(val_idx)}')
        print(f'Test samples: {len(test_idx)}')
        
        # åˆ›å»ºå­æ•°æ®é›†
        train_dataset = Subset(dataset, train_idx)
        val_dataset = Subset(dataset, val_idx)
        test_dataset = Subset(dataset, test_idx)
        
        datasets = (train_dataset, val_dataset, test_dataset)
        
        # ä½¿ç”¨è®­ç»ƒå™¨è¿›è¡Œè®­ç»ƒ
        results, test_auc, val_auc, test_acc, val_acc = trainer.train_fold(
            datasets=datasets,
            fold_idx=i
        )
        
        all_test_auc.append(test_auc)
        all_val_auc.append(val_auc)
        all_test_acc.append(test_acc)
        all_val_acc.append(val_acc)
        
        # ä¿å­˜ç»“æœ
        filename = os.path.join(args.results_dir, 'split_{}_results.pkl'.format(i))
        save_pkl(filename, results)
        
        print(f'Fold {i+1} completed - Test AUC: {test_auc:.4f}, Val AUC: {val_auc:.4f}')

    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_df = pd.DataFrame({
        'folds': folds, 
        'test_auc': all_test_auc, 
        'val_auc': all_val_auc, 
        'test_acc': all_test_acc, 
        'val_acc': all_val_acc
    })

    if len(folds) != args.k:
        save_name = 'summary_partial_{}_{}.csv'.format(start, end)
    else:
        save_name = 'summary.csv'
    final_df.to_csv(os.path.join(args.results_dir, save_name))
    
    # ä¿å­˜è¯¦ç»†çš„è®­ç»ƒæ•°æ®ç”¨äºåç»­ç»˜å›¾
    detailed_results = {
        'configurations': configs,
        'fold_results': {
            'folds': folds.tolist(),
            'test_auc': all_test_auc,
            'val_auc': all_val_auc, 
            'test_acc': all_test_acc,
            'val_acc': all_val_acc
        },
        'summary_stats': {
            'mean_test_auc': float(np.mean(all_test_auc)),
            'std_test_auc': float(np.std(all_test_auc)),
            'mean_val_auc': float(np.mean(all_val_auc)),
            'std_val_auc': float(np.std(all_val_auc)),
            'mean_test_acc': float(np.mean(all_test_acc)),
            'std_test_acc': float(np.std(all_test_acc)),
            'mean_val_acc': float(np.mean(all_val_acc)),
            'std_val_acc': float(np.std(all_val_acc))
        }
    }
    
    # ä¿å­˜è¯¦ç»†ç»“æœç”¨äºç»˜å›¾
    detailed_save_name = 'detailed_results_for_plotting.json'
    with open(os.path.join(args.results_dir, detailed_save_name), 'w') as f:
        json.dump(detailed_results, f, indent=2)
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print(f'\n{"="*60}')
    print('FINAL RESULTS SUMMARY')
    print(f'{"="*60}')
    print(f'Mean Test AUC: {np.mean(all_test_auc):.4f} Â± {np.std(all_test_auc):.4f}')
    print(f'Mean Val AUC: {np.mean(all_val_auc):.4f} Â± {np.std(all_val_auc):.4f}')
    print(f'Mean Test Acc: {np.mean(all_test_acc):.4f} Â± {np.std(all_test_acc):.4f}')
    print(f'Mean Val Acc: {np.mean(all_val_acc):.4f} Â± {np.std(all_val_acc):.4f}')
    print(f'Results saved to: {os.path.join(args.results_dir, save_name)}')
    print(f'Detailed results for plotting: {os.path.join(args.results_dir, detailed_save_name)}')

# å‚æ•°è§£æ
parser = argparse.ArgumentParser(description='å¤šæ¨¡æ€ç”Ÿå­˜çŠ¶æ€é¢„æµ‹é…ç½®')

# æ•°æ®ç›¸å…³å‚æ•°
parser.add_argument('--data_root_dir', type=str, default=None, 
                    help='æ•°æ®æ ¹ç›®å½•')
parser.add_argument('--results_dir', default='./results', 
                    help='ç»“æœä¿å­˜ç›®å½• (default: ./results)')
parser.add_argument('--csv_path', type=str, default='dataset_csv/survival_status_labels.csv', 
                    help='CSVæ–‡ä»¶è·¯å¾„')
# å¯¹é½æ¨¡å‹ç›¸å…³å‚æ•°
parser.add_argument('--alignment_model_path', type=str, default=None, 
                    help='é¢„è®­ç»ƒå¯¹é½æ¨¡å‹è·¯å¾„ï¼ˆæä¾›æ­¤å‚æ•°å°†è‡ªåŠ¨å¯ç”¨å¯¹é½åŠŸèƒ½ï¼‰')
# å¤šæ¨¡æ€ç›¸å…³å‚æ•°
parser.add_argument('--target_channels', type=str, nargs='+', 
                    default=['CD3', 'CD8', 'CD56', 'CD68', 'CD163', 'HE', 'MHC1', 'PDL1'], 
                    help='ç›®æ ‡é€šé“')
parser.add_argument('--aligned_channels', type=str, nargs='*', 
                    default=None,
                    help='å¯¹é½ç›®æ ‡ï¼Œæ ¼å¼: channel_to_align1=align_channel_name1 channel_to_align2=align_channel_name2 ...')
# å®éªŒç›¸å…³å‚æ•°
parser.add_argument('--exp_code', type=str, 
                    help='å®éªŒä»£ç ï¼Œç”¨äºä¿å­˜ç»“æœ')
parser.add_argument('--seed', type=int, default=1, 
                    help='éšæœºç§å­ (default: 1)')
parser.add_argument('--k', type=int, default=10, 
                    help='foldæ•°é‡ (default: 10)')
parser.add_argument('--split_mode', type=str, choices=['random', 'fixed'], default='random',
                    help='æ•°æ®é›†åˆ†å‰²æ¨¡å¼: random=éšæœºåˆ†å‰², fixed=å›ºå®šæµ‹è¯•é›†åˆ†å‰² (default: random)')
parser.add_argument('--dataset_split_path', type=str, default=None,
                    help='å›ºå®šæµ‹è¯•é›†åˆ†å‰²JSONæ–‡ä»¶è·¯å¾„ (ä»…åœ¨split_mode=fixedæ—¶ä½¿ç”¨)')
parser.add_argument('--max_epochs', type=int, default=200,
                    help='æœ€å¤§è®­ç»ƒè½®æ•° (default: 200)')
parser.add_argument('--lr', type=float, default=1e-4,
                    help='å­¦ä¹ ç‡ (default: 0.0001)')
parser.add_argument('--reg', type=float, default=1e-5,
                    help='æƒé‡è¡°å‡ (default: 1e-5)')
parser.add_argument('--opt', type=str, choices=['adam', 'sgd'], default='adam',
                    help='ä¼˜åŒ–å™¨ç±»å‹')
parser.add_argument('--early_stopping', action='store_true', default=False, 
                    help='å¯ç”¨æ—©åœ')
parser.add_argument('--batch_size', type=int, default=64,
                    help='æ‰¹æ¬¡å¤§å° (default: 64)')
parser.add_argument('--lr_scheduler', type=str, 
                    choices=['none', 'cosine', 'cosine_warm_restart', 'step', 'plateau', 'exponential'], 
                    default='none',
                    help='å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ (default: none)')
parser.add_argument('--lr_scheduler_params', type=str, default='{}',
                    help='å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•° (JSONå­—ç¬¦ä¸²ï¼Œé»˜è®¤: {})')

# æ¨¡å‹ç›¸å…³å‚æ•°
parser.add_argument('--model_type', type=str, choices=[
    'mil', 'clam', 'auc_clam', 'clam_mlp', 'clam_mlp_detach', 'svd_gate_random_clam', 'svd_gate_random_clam_detach', 
    'gate_shared_mil', 'gate_mil_detach', 'gate_mil', 'gate_auc_mil'
    ], 
                    default='clam', help='æ¨¡å‹ç±»å‹ (default: clam)')
parser.add_argument('--input_dim', type=int, default=1024,
                    help='è¾“å…¥ç»´åº¦')
parser.add_argument('--dropout', type=float, default=0.25, 
                    help='dropoutç‡')
parser.add_argument('--n_classes', type=int, default=2,
                    help='ç±»åˆ«æ•° (default: 2)')
parser.add_argument('--base_loss_fn', type=str, choices=['svm', 'ce'], default='ce',
                    help='slideçº§åˆ«åˆ†ç±»æŸå¤±å‡½æ•° (default: ce)')

# CLAM ç›¸å…³å‚æ•°
parser.add_argument('--gate', action='store_true', default=True, 
                    help='CLAM: ä½¿ç”¨é—¨æ§æ³¨æ„åŠ›æœºåˆ¶')
parser.add_argument('--base_weight', type=float, default=0.7,
                    help='CLAM: bagçº§åˆ«æŸå¤±æƒé‡ç³»æ•° (default: 0.7)')
parser.add_argument('--inst_loss_fn', type=str, choices=['svm', 'ce', None], default=None,
                    help='CLAM: å®ä¾‹çº§åˆ«èšç±»æŸå¤±å‡½æ•° (default: None)')
parser.add_argument('--model_size', type=str, 
                    choices=['small', 'big', '128*64', '64*32', '32*16', '16*8', '8*4', '4*2', '2*1'], 
                    default='small', help='æ¨¡å‹å¤§å°')
parser.add_argument('--subtyping', action='store_true', default=False, 
                    help='å­ç±»å‹é—®é¢˜')
parser.add_argument('--inst_number', type=int, default=8, 
                    help='CLAM: æ­£è´Ÿæ ·æœ¬é‡‡æ ·æ•°é‡')
parser.add_argument('--channels_used_in_model', type=str, nargs='+', 
                    default=['features', 'CD3', 'CD8', 'CD56', 'CD68', 'CD163', 'HE', 'MHC1', 'PDL1'],
                    help='æ¨¡å‹ä¸­éœ€è¦ä½¿ç”¨çš„é€šé“')
parser.add_argument('--return_features', action='store_true', default=False, 
                    help='MIL & CLAM: è¿”å›ç‰¹å¾')
parser.add_argument('--attention_only', action='store_true', default=False, 
                    help='CLAM: ä»…è¿”å›æ³¨æ„åŠ›')

# Transfer layer
parser.add_argument('--output_dim', type=int, default=128, 
                    help='Transfer layer: æ¨¡æ€ç»Ÿä¸€çš„è¾“å‡ºç»´åº¦')

# SVDç›¸å…³å‚æ•°
parser.add_argument('--enable_svd', action='store_true', default=False, 
                    help='SVD: å¯ç”¨SVD')
parser.add_argument('--alignment_layer_num', type=int, default=2,
                    help='SVD: å¯¹é½å±‚æ•°')
parser.add_argument('--lambda1', type=float, default=1.0,
                    help='SVD: å¯¹é½æŸå¤±æƒé‡')
parser.add_argument('--lambda2', type=float, default=0.0,
                    help='SVD: å¯¹é½æŸå¤±æƒé‡')
parser.add_argument('--tau1', type=float, default=0.1,
                    help='SVD: å¯¹é½æŸå¤±æƒé‡')
parser.add_argument('--tau2', type=float, default=0.05,
                    help='SVD: å¯¹é½æŸå¤±æƒé‡')

# Dynamic Gateç›¸å…³å‚æ•°
parser.add_argument('--enable_dynamic_gate', action='store_true', default=False, 
                    help='Dynamic Gate: å¯ç”¨åŠ¨æ€é—¨æ§')
parser.add_argument('--confidence_weight', type=float, default=1.0,
                    help='Dynamic Gate: ç½®ä¿¡åº¦æƒé‡')
parser.add_argument('--feature_weight_weight', type=float, default=1.0,
                    help='Dynamic Gate: ç‰¹å¾æƒé‡æƒé‡')

# AUCç›¸å…³å‚æ•°
parser.add_argument('--auc_loss_weight', type=float, default=1.0,
                    help='AUC: AUCæŸå¤±æƒé‡')

# Random Lossç›¸å…³å‚æ•°
parser.add_argument('--enable_random_loss', action='store_true', default=False, 
                    help='Random Loss: å¯ç”¨éšæœºæŸå¤±')
parser.add_argument('--weight_random_loss', type=float, default=0.1, 
                    help='Random Loss: éšæœºæŸå¤±æƒé‡')
# è§£æå‚æ•°
args = parser.parse_args()
args.target_channels = parse_channels(args.target_channels)
args.aligned_channels = parse_channels(args.aligned_channels)
args.channels_used_in_model = parse_channels(args.channels_used_in_model)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è®¾ç½®éšæœºç§å­
seed_torch(args.seed)

# åˆ›å»ºç»“æœç›®å½•
if not os.path.isdir(args.results_dir):
    os.mkdir(args.results_dir)

# åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ç»“æœç›®å½•
args.results_dir = os.path.join(
    args.results_dir, 
    datetime.now().strftime("%Y%m%d-%H%M%S") + '_' + str(args.exp_code) + '_s{}'.format(args.seed)
)
if not os.path.isdir(args.results_dir):
    os.mkdir(args.results_dir)

# åˆ›å»ºç²¾ç®€çš„åˆ†ç±»é…ç½®å­—å…¸
configs = {
    'experiment_config': {
        'data_root_dir': args.data_root_dir,
        'results_dir': args.results_dir,
        'csv_path': args.csv_path,
        'alignment_model_path': args.alignment_model_path,
        'target_channels': args.target_channels,
        'aligned_channels': args.aligned_channels,
        'exp_code': args.exp_code,
        'seed': args.seed,
        'num_splits': args.k,
        'split_mode': args.split_mode,
        'dataset_split_path': args.dataset_split_path,
        'max_epochs': args.max_epochs,
        'lr': args.lr,
        'reg': args.reg,
        'opt': args.opt,
        'early_stopping': args.early_stopping,
        'batch_size': args.batch_size,
        'scheduler_config': {
            'type': args.lr_scheduler if args.lr_scheduler != 'none' else None,
            **(json.loads(args.lr_scheduler_params) if args.lr_scheduler_params else {})
        }
    },
    
    'model_config': {
        'model_type': args.model_type,
        'input_dim': args.input_dim,
        'dropout': args.dropout,
        'n_classes': args.n_classes,
        'base_loss_fn': args.base_loss_fn,
        'channels_used_in_model': args.channels_used_in_model,
        **_get_model_specific_config(args)
    }
}

# ä¿å­˜åˆ†ç±»é…ç½®
with open(args.results_dir + '/configs_{}.json'.format(args.exp_code), 'w') as f:
    json.dump(configs, f, indent=2)

# æ‰“å°ç²¾ç®€é…ç½®
print("################# Configuration ###################")
print(f"\nğŸ“‹ EXPERIMENT CONFIG:")
for key, val in configs['experiment_config'].items():
    print(f"  {key}: {val}")

print(f"\nğŸ“‹ MODEL CONFIG:")
for key, val in configs['model_config'].items():
    print(f"  {key}: {val}")

if __name__ == "__main__":
    results = main(args, configs)
    print("finished!")
    print("end script")
