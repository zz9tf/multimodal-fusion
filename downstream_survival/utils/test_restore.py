#!/usr/bin/env python3
"""
æ¢å¤è„šæœ¬ï¼šåŠ è½½ä¿å­˜çš„æ¨¡å‹å’Œsplitï¼Œè¿›è¡ŒéªŒè¯
"""

import argparse
import os
import sys
import json
import pandas as pd
import torch
import numpy as np
from torch.utils.data import Subset

# æ·»åŠ é¡¹ç›®è·¯å¾„
root_dir = '/home/zheng/zheng/multimodal-fusion/downstream_survival'
sys.path.append(root_dir)

from trainer import Trainer
from datasets.multimodal_dataset import MultimodalDataset
from main import parse_channels, create_k_fold_splits, _parse_aligned_channels, seed_torch

def _load_configs_from_results_dir(results_dir: str) -> dict:
    """ä»resultsç›®å½•åŠ è½½é…ç½®"""
    configs_path = os.path.join(results_dir, 'configs_test.json')
    if not os.path.exists(configs_path):
        raise FileNotFoundError(f'æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {configs_path}')
    
    with open(configs_path, 'r') as f:
        configs = json.load(f)
    
    return configs

def _load_split_from_csv(results_dir: str, fold_idx: int, dataset: MultimodalDataset) -> tuple:
    """
    ä» results_dir/splits_{fold_idx}.csv åŠ è½½è¯¥æŠ˜çš„ train/val/test åˆ’åˆ†ã€‚
    
    æœŸæœ›CSVåˆ—å: train,val,testï¼›
    å•å…ƒæ ¼å€¼ï¼šä¿å­˜çš„æ˜¯case_idï¼ˆå¦‚ 'patient_008'ï¼‰ï¼Œè€Œä¸æ˜¯ç´¢å¼•
    é€šè¿‡case_idæ˜ å°„åˆ°å½“å‰æ•°æ®é›†çš„ç´¢å¼•
    """
    path = os.path.join(results_dir, f'splits_{fold_idx}.csv')
    if not os.path.exists(path):
        raise FileNotFoundError(f'æœªæ‰¾åˆ°åˆ†å‰²æ–‡ä»¶: {path}')
    
    # åˆ›å»ºcase_idåˆ°ç´¢å¼•çš„æ˜ å°„
    if not hasattr(dataset, 'case_ids'):
        raise ValueError('æ•°æ®é›†å¿…é¡»æœ‰case_idså±æ€§')
    
    case_id_to_idx = {case_id: idx for idx, case_id in enumerate(dataset.case_ids)}
    
    # è¯»å–CSVæ–‡ä»¶
    df = pd.read_csv(path)
    
    train_indices = []
    val_indices = []
    test_indices = []
    
    # å¤„ç†trainåˆ—
    if 'train' in df.columns:
        train_case_ids = df['train'].dropna().tolist()
        for case_id in train_case_ids:
            if case_id in case_id_to_idx:
                train_indices.append(case_id_to_idx[case_id])
            else:
                print(f"âš ï¸ è­¦å‘Šï¼šcase_id {case_id} ä¸åœ¨å½“å‰æ•°æ®é›†ä¸­ï¼Œè·³è¿‡")
    
    # å¤„ç†valåˆ—
    if 'val' in df.columns:
        val_case_ids = df['val'].dropna().tolist()
        for case_id in val_case_ids:
            if case_id in case_id_to_idx:
                val_indices.append(case_id_to_idx[case_id])
            else:
                print(f"âš ï¸ è­¦å‘Šï¼šcase_id {case_id} ä¸åœ¨å½“å‰æ•°æ®é›†ä¸­ï¼Œè·³è¿‡")
    
    # å¤„ç†teståˆ—
    if 'test' in df.columns:
        test_case_ids = df['test'].dropna().tolist()
        for case_id in test_case_ids:
            if case_id in case_id_to_idx:
                test_indices.append(case_id_to_idx[case_id])
            else:
                print(f"âš ï¸ è­¦å‘Šï¼šcase_id {case_id} ä¸åœ¨å½“å‰æ•°æ®é›†ä¸­ï¼Œè·³è¿‡")
    
    return np.array(train_indices, dtype=int), np.array(val_indices, dtype=int), np.array(test_indices, dtype=int)

def _list_checkpoints(results_dir: str):
    """åˆ—å‡ºæ‰€æœ‰checkpointæ–‡ä»¶"""
    checkpoints = []
    for filename in os.listdir(results_dir):
        if filename.startswith('s_') and filename.endswith('_checkpoint.pt'):
            try:
                fold_idx = int(filename.split('_')[1])
                checkpoints.append((fold_idx, os.path.join(results_dir, filename)))
            except Exception:
                continue
    checkpoints.sort(key=lambda x: x[0])
    return checkpoints

def main():
    parser = argparse.ArgumentParser(description='æ¢å¤å¹¶éªŒè¯ä¿å­˜çš„æ¨¡å‹å’Œsplit')
    
    parser.add_argument('--results_dir', type=str, required=True, 
                       help='è®­ç»ƒç»“æœç›®å½•ï¼ˆåŒ…å«configs_test.jsonå’Œcheckpointsï¼‰')
    parser.add_argument('--data_root_dir', type=str, default=None,
                       help='æ•°æ®æ ¹ç›®å½•ï¼ˆå¦‚æœä¸æä¾›ï¼Œå°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰')
    parser.add_argument('--csv_path', type=str, default=None,
                       help='CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸æä¾›ï¼Œå°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰')
    
    args = parser.parse_args()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print("="*60)
    print("æ¢å¤å¹¶éªŒè¯ä¿å­˜çš„æ¨¡å‹å’Œsplit")
    print("="*60)
    
    # 1. åŠ è½½é…ç½®
    print(f"\nğŸ“¦ åŠ è½½é…ç½®...")
    configs = _load_configs_from_results_dir(args.results_dir)
    exp_cfg = configs.get('experiment_config', {})
    model_cfg = configs.get('model_config', {})
    
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
    print(f"   seed: {exp_cfg.get('seed')}")
    print(f"   k: {exp_cfg.get('num_splits')}")
    print(f"   model_type: {model_cfg.get('model_type')}")
    
    # 2. æ„å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°ï¼‰
    print(f"\nğŸ“¦ æ„å»ºæ•°æ®é›†...")
    data_root_dir = args.data_root_dir or exp_cfg.get('data_root_dir')
    csv_path = args.csv_path or exp_cfg.get('csv_path')
    target_channels = exp_cfg.get('target_channels', [])
    align_channels = exp_cfg.get('aligned_channels', {})
    
    if not data_root_dir:
        raise ValueError('data_root_dir is required')
    if not csv_path:
        raise ValueError('csv_path is required')
    
    # å¤„ç†è·¯å¾„ï¼šå¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºåŸºäºé¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
    if not os.path.isabs(csv_path):
        csv_path = os.path.join(root_dir, csv_path)
    
    if not os.path.isabs(data_root_dir):
        data_root_dir = os.path.abspath(data_root_dir)
    
    print(f"   data_root_dir: {data_root_dir}")
    print(f"   csv_path: {csv_path}")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(csv_path):
        # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„CSVæ–‡ä»¶
        possible_names = ['survival_dataset.csv', 'survival_status_labels.csv']
        csv_dir = os.path.dirname(csv_path) if os.path.dirname(csv_path) else root_dir
        csv_dir = os.path.join(root_dir, 'dataset_csv') if 'dataset_csv' in str(csv_path) else csv_dir
        
        suggestions = []
        if os.path.exists(csv_dir):
            for name in possible_names:
                possible_path = os.path.join(csv_dir, name)
                if os.path.exists(possible_path):
                    suggestions.append(possible_path)
        
        error_msg = f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}\n"
        if suggestions:
            error_msg += f"ğŸ’¡ æ‰¾åˆ°å¯èƒ½çš„CSVæ–‡ä»¶:\n"
            for sug in suggestions:
                error_msg += f"   - {sug}\n"
            error_msg += f"ğŸ’¡ è¯·ä½¿ç”¨: --csv_path {suggestions[0]}"
        else:
            error_msg += f"ğŸ’¡ è¯·ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„ç›¸å¯¹è·¯å¾„\n"
            error_msg += f"ğŸ’¡ å¸¸è§çš„CSVæ–‡ä»¶ä½ç½®: {os.path.join(root_dir, 'dataset_csv')}"
        
        raise FileNotFoundError(error_msg)
    if not os.path.exists(data_root_dir):
        raise FileNotFoundError(f"âŒ æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {data_root_dir}")
    print(f"   target_channels: {target_channels[:5]}..." if len(target_channels) > 5 else f"   target_channels: {target_channels}")
    
    dataset = MultimodalDataset(
        csv_path=csv_path,
        data_root_dir=data_root_dir,
        channels=target_channels,
        align_channels=align_channels,
        alignment_model_path=None,
        device=device,
        print_info=True
    )
    
    print(f"âœ… æ•°æ®é›†æ„å»ºå®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")
    if hasattr(dataset, 'case_ids') and len(dataset.case_ids) > 0:
        print(f"ğŸ” å‰5ä¸ªcase_id: {dataset.case_ids[:5]}")
        print(f"ğŸ” æ•°æ®é›†å¤§å°: {len(dataset.case_ids)}")
    
    # 3. åˆ—å‡ºæ‰€æœ‰checkpoints
    print(f"\nğŸ“‹ æŸ¥æ‰¾checkpoints...")
    checkpoints = _list_checkpoints(args.results_dir)
    if not checkpoints:
        raise FileNotFoundError('æœªæ‰¾åˆ°ä»»ä½• checkpointï¼ˆå½¢å¦‚ s_0_checkpoint.ptï¼‰ã€‚')
    
    print(f"âœ… æ‰¾åˆ° {len(checkpoints)} ä¸ªcheckpoints")
    for fold_idx, ckpt_path in checkpoints:
        print(f"   Fold {fold_idx}: {ckpt_path}")
    
    # 4. åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = Trainer(
        configs=configs,
        log_dir=os.path.join(args.results_dir, 'restore_logs')
    )
    
    # 5. æ¢å¤å¹¶éªŒè¯æ¯ä¸ªfold
    print(f"\nğŸ” å¼€å§‹æ¢å¤å¹¶éªŒè¯æ¯ä¸ªfold...")
    
    restored_results = []
    
    for fold_idx, ckpt_path in checkpoints:
        print(f"\n{'='*60}")
        print(f'æ¢å¤ Fold {fold_idx}')
        print(f"{'='*60}")
        
        # 5.1 åŠ è½½split
        print(f"\nğŸ“Š åŠ è½½split...")
        try:
            train_indices, val_indices, test_indices = _load_split_from_csv(
                args.results_dir, fold_idx, dataset
            )
            print(f"âœ… SplitåŠ è½½æˆåŠŸ")
            print(f"   train: {len(train_indices)} ä¸ª")
            print(f"   val: {len(val_indices)} ä¸ª")
            print(f"   test: {len(test_indices)} ä¸ª")
            
            if hasattr(dataset, 'case_ids') and len(test_indices) > 0:
                test_case_ids = [dataset.case_ids[i] for i in test_indices[:5]]
                print(f"   testé›†å‰5ä¸ªcase_id: {test_case_ids}")
        except Exception as e:
            print(f"âŒ SplitåŠ è½½å¤±è´¥: {e}")
            continue
        
        # 5.2 åˆ›å»ºå­æ•°æ®é›†
        train_ds = Subset(dataset, train_indices)
        val_ds = Subset(dataset, val_indices) if len(val_indices) > 0 else None
        test_ds = Subset(dataset, test_indices)
        datasets_tuple = (train_ds, val_ds, test_ds)
        
        # 5.3 åŠ è½½æ¨¡å‹å¹¶è¯„ä¼°
        print(f"\nğŸ”§ åŠ è½½æ¨¡å‹å¹¶è¯„ä¼°...")
        try:
            results_dict, test_auc, val_auc, test_acc, val_acc = trainer.evaluate_fold(
                datasets=datasets_tuple,
                fold_idx=fold_idx,
                checkpoint_path=ckpt_path
            )
            
            print(f"âœ… è¯„ä¼°å®Œæˆ")
            print(f"   Test AUC: {test_auc:.4f}")
            print(f"   Val AUC: {val_auc:.4f}" if val_auc is not None else "   Val AUC: None")
            print(f"   Test Acc: {test_acc:.4f}")
            print(f"   Val Acc: {val_acc:.4f}" if val_acc is not None else "   Val Acc: None")
            
            restored_results.append({
                'fold': fold_idx,
                'test_auc': float(test_auc) if test_auc is not None else None,
                'val_auc': float(val_auc) if val_auc is not None else None,
                'test_acc': float(test_acc) if test_acc is not None else None,
                'val_acc': float(val_acc) if val_acc is not None else None,
            })
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # 6. æ¯”è¾ƒç»“æœ
    print(f"\n{'='*60}")
    print("æ¢å¤ç»“æœæ‘˜è¦")
    print(f"{'='*60}")
    
    if restored_results:
        restored_df = pd.DataFrame(restored_results)
        print("\næ¢å¤åçš„ç»“æœ:")
        print(restored_df.to_string(index=False))
        
        # è®¡ç®—ç»Ÿè®¡é‡
        test_aucs = [r['test_auc'] for r in restored_results if r['test_auc'] is not None]
        test_accs = [r['test_acc'] for r in restored_results if r['test_acc'] is not None]
        
        if test_aucs:
            print(f"\nMean Test AUC: {np.mean(test_aucs):.4f} Â± {np.std(test_aucs):.4f}")
        if test_accs:
            print(f"Mean Test Acc: {np.mean(test_accs):.4f} Â± {np.std(test_accs):.4f}")
        
        # ä¿å­˜æ¢å¤ç»“æœ
        restore_results_path = os.path.join(args.results_dir, 'restored_results.csv')
        restored_df.to_csv(restore_results_path, index=False)
        print(f"\nâœ… æ¢å¤ç»“æœå·²ä¿å­˜åˆ°: {restore_results_path}")
        
        # åŠ è½½åŸå§‹ç»“æœè¿›è¡Œæ¯”è¾ƒ
        original_results_path = os.path.join(args.results_dir, 'detailed_results_for_plotting.json')
        if os.path.exists(original_results_path):
            print(f"\nğŸ“Š åŠ è½½åŸå§‹ç»“æœè¿›è¡Œæ¯”è¾ƒ...")
            with open(original_results_path, 'r') as f:
                original_results = json.load(f)
            
            original_fold_results = original_results.get('fold_results', {})
            original_test_aucs = original_fold_results.get('test_auc', [])
            original_test_accs = original_fold_results.get('test_acc', [])
            
            print(f"\nåŸå§‹ç»“æœï¼ˆè®­ç»ƒæ—¶ï¼‰:")
            print(f"  Test AUCs: {original_test_aucs}")
            print(f"  Test Accs: {original_test_accs}")
            
            print(f"\næ¢å¤ç»“æœï¼ˆé‡æ–°åŠ è½½åï¼‰:")
            print(f"  Test AUCs: {[r['test_auc'] for r in restored_results]}")
            print(f"  Test Accs: {[r['test_acc'] for r in restored_results]}")
            
            # æ¯”è¾ƒå·®å¼‚
            if len(original_test_aucs) == len(test_aucs):
                auc_diffs = [abs(o - r) for o, r in zip(original_test_aucs, test_aucs)]
                print(f"\nå·®å¼‚åˆ†æ:")
                print(f"  Test AUCå·®å¼‚: {auc_diffs}")
                print(f"  æœ€å¤§å·®å¼‚: {max(auc_diffs):.6f}")
                print(f"  å¹³å‡å·®å¼‚: {np.mean(auc_diffs):.6f}")
                
                if max(auc_diffs) < 1e-4:
                    print(f"âœ… å®Œç¾åŒ¹é…ï¼æ‰€æœ‰ç»“æœéƒ½ä¸€è‡´")
                elif max(auc_diffs) < 1e-3:
                    print(f"âš ï¸ æœ‰å¾®å°å·®å¼‚ï¼ˆå¯èƒ½æ˜¯æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜ï¼‰")
                else:
                    print(f"âŒ å­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œéœ€è¦æ£€æŸ¥")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸæ¢å¤çš„ç»“æœ")
    
    print(f"\nâœ… æ¢å¤éªŒè¯å®Œæˆ")

if __name__ == "__main__":
    main()

