#!/usr/bin/env python3
"""
ç¼ºæ¨¡æ€é²æ£’æ€§è¯„æµ‹è„šæœ¬ï¼ˆä»…æµ‹è¯•é›†è¯„æµ‹ï¼‰

åŠŸèƒ½æ¦‚è¿°ï¼š
- ç»™å®šç»“æœç›®å½•ï¼ˆåŒ…å«10ä¸ªfoldçš„checkpointä¸é…ç½®ï¼‰ã€æ•°æ®é›†ï¼›
- åœ¨æ¨ç†é˜¶æ®µé€šè¿‡ drop_prob æ§åˆ¶æ¨¡æ€ä¸¢å¼ƒï¼Œå¹¶åœ¨å„foldçš„æµ‹è¯•é›†ä¸Šè¯„æµ‹æ¨¡å‹è¡¨ç°ï¼›
- ä»…è¾“å‡ºæµ‹è¯•é›†æŒ‡æ ‡ï¼ˆå¦‚ Test AUC/ACCï¼‰ï¼Œä¿å­˜CSVä¸JSONã€‚
"""

import argparse
import os
import json
import sys
from typing import Dict, List, Optional, Tuple, Any
import numpy as np
import torch
from torch.utils.data import Subset

ROOT_DIR = '/home/zheng/zheng/multimodal-fusion/downstream_survival'
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)
from datasets.multimodal_dataset import MultimodalDataset
from trainer import Trainer
from main import parse_channels, create_k_fold_splits


def _load_configs_from_results_dir(results_dir: str) -> Dict[str, Any]:
    """
    ä»ç»“æœç›®å½•åŠ è½½é…ç½®JSONï¼ˆåŒ¹é…ç¬¬ä¸€ä¸ª configs_*.json æˆ– configs_*.JSONï¼‰ã€‚

    Returns:
        dict: é…ç½®å­—å…¸ï¼ŒåŒ…å« experiment_config ä¸ model_configã€‚
    """
    candidates = []
    for name in os.listdir(results_dir):
        if name.startswith('configs_') and name.lower().endswith('.json'):
            candidates.append(os.path.join(results_dir, name))
    if not candidates:
        raise FileNotFoundError(f'æœªåœ¨ç›®å½•æ‰¾åˆ°é…ç½®æ–‡ä»¶: {results_dir}')

    cfg_path = sorted(candidates)[0]
    with open(cfg_path, 'r') as f:
        return json.load(f)

def _list_checkpoints(results_dir: str) -> List[Tuple[int, str]]:
    """
    æšä¸¾å„foldçš„checkpointã€‚

    æœŸæœ›å‘½åï¼šs_0_checkpoint.pt ... s_9_checkpoint.pt
    Returns: List[(fold_idx, ckpt_path)]
    """
    items: List[Tuple[int, str]] = []
    for name in os.listdir(results_dir):
        if name.startswith('s_') and name.endswith('_checkpoint.pt'):
            try:
                fold = int(name.split('_')[1])
                items.append((fold, os.path.join(results_dir, name)))
            except Exception:
                continue
    items.sort(key=lambda x: x[0])
    return items

def _call_trainer_eval(
    trainer: Trainer,
    datasets: Tuple[Subset, Optional[Subset], Subset],
    fold_idx: int,
    checkpoint_path: str,
    drop_prob: Optional[float] = None,
) -> Tuple[Optional[float], Optional[float]]:
    """
    è°ƒç”¨è®­ç»ƒå™¨çš„è¯„æµ‹æ¥å£ï¼ˆè‡ªé€‚é…ä¸åŒå¯èƒ½çš„æ–¹æ³•åï¼‰ã€‚

    Args:
        trainer: è®­ç»ƒå™¨å®ä¾‹
        datasets: æ•°æ®é›†å…ƒç»„
        fold_idx: foldç´¢å¼•
        checkpoint_path: checkpointè·¯å¾„
        drop_prob: æ¨¡æ€ä¸¢å¼ƒæ¦‚ç‡ï¼ˆç”¨äºforwardæ—¶ä¼ å…¥ï¼‰

    Returns:
        (test_auc, test_acc)
    """
    # ä¼˜å…ˆæ‰¾æ˜¾å¼çš„è¯„æµ‹æ¥å£
    if hasattr(trainer, 'evaluate_with_checkpoint'):
        res = trainer.evaluate_with_checkpoint(
            datasets=datasets, 
            fold_idx=fold_idx, 
            checkpoint_path=checkpoint_path,
            drop_prob=drop_prob
        )
    else:
        raise RuntimeError('Trainer æœªæä¾›å…¼å®¹çš„è¯„æµ‹æ¥å£ï¼Œè¯·åœ¨ Trainer ä¸­å®ç° test/evaluate æ¥å£ã€‚')

    # ç»“æœå…¼å®¹ï¼šå¸¸è§è¿”å› (results_dict, test_auc, val_auc, test_acc, val_acc)
    test_auc = None
    test_acc = None
    if isinstance(res, tuple):
        # è§£æ test_accï¼ˆé€šå¸¸ä½äºå€’æ•°ç¬¬äºŒä¸ªï¼‰
        if len(res) >= 2:
            try:
                test_acc = float(res[-2])
            except Exception:
                test_acc = None
        # è§£æ test_aucï¼ˆé€šå¸¸ä½äºç´¢å¼•1ï¼‰
        if len(res) >= 2:
            try:
                test_auc = float(res[1])
            except Exception:
                test_auc = None
    elif isinstance(res, dict):
        test_auc = float(res.get('test_auc')) if res.get('test_auc') is not None else None
        test_acc = float(res.get('test_acc')) if res.get('test_acc') is not None else None
    return test_auc, test_acc

def run(args: argparse.Namespace) -> None:
    """
    ä¸»æµç¨‹ï¼šåŠ è½½é…ç½®ä¸checkpoint -> æ„é€ æ•°æ®ä¸åˆ†å‰² -> æ©è”½æŒ‡å®šæ¨¡æ€ -> è¯„æµ‹ -> ä¿å­˜æŠ¥å‘Šã€‚
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 1) ç»“æœç›®å½•é…ç½®ä¸checkpoint
    configs = _load_configs_from_results_dir(args.results_dir)
    exp_cfg = configs.get('experiment_config', {})
    model_cfg = configs.get('model_config', {})

    # åŸºç¡€é€šé“æŒ‰ä¸»ç¨‹åºè§„åˆ™è§£æï¼ˆæ”¯æŒç®€å†™ï¼‰
    base_target_channels = exp_cfg.get('target_channels') or model_cfg.get('channels_used_in_model')
    if not base_target_channels:
        base_target_channels = parse_channels(args.target_channels or [])

    # 2) æ•°æ®é›†
    dataset = MultimodalDataset(
        csv_path=args.csv_path or exp_cfg.get('csv_path'),
        data_root_dir=args.data_root_dir or exp_cfg.get('data_root_dir'),
        channels=base_target_channels,
        align_channels=exp_cfg.get('aligned_channels', None),
        alignment_model_path=exp_cfg.get('alignment_model_path', None),
        device=device,
    )
    
    print(f"ğŸ“Š æ•°æ®é›†æ„å»ºå®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“ data_root_dir: {args.data_root_dir or exp_cfg.get('data_root_dir')}")
    print(f"ğŸ“‹ channels: {base_target_channels[:5]}..." if len(base_target_channels) > 5 else f"ğŸ“‹ channels: {base_target_channels}")

    # 3) é‡æ–°ç”ŸæˆKæŠ˜åˆ’åˆ†ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    seed = exp_cfg.get('seed', 5678)
    k = exp_cfg.get('num_splits', 10)
    splits = create_k_fold_splits(dataset, k=k, seed=seed, fixed_test_split=None)
    
    # 4) è·å–checkpointåˆ—è¡¨
    checkpoints = _list_checkpoints(args.results_dir)
    if not checkpoints:
        raise FileNotFoundError('æœªæ‰¾åˆ°ä»»ä½• checkpointï¼ˆå½¢å¦‚ s_0_checkpoint.ptï¼‰ã€‚')

    # 5) è®­ç»ƒå™¨
    trainer = Trainer(configs=configs, log_dir=os.path.join(args.results_dir, 'training_logs'))

    # 6) éå†foldå¹¶è¯„æµ‹
    per_fold_metrics = []
    drop_prob = args.drop_prob

    for fold_idx, ckpt_path in checkpoints:
        if fold_idx >= len(splits):
            print(f"âš ï¸ Fold {fold_idx} è¶…å‡ºåˆ’åˆ†èŒƒå›´ï¼Œè·³è¿‡")
            continue
        
        split = splits[fold_idx]
        train_ds = Subset(dataset, split['train'])
        val_ds = Subset(dataset, split['val']) if len(split['val']) > 0 else None
        test_ds = Subset(dataset, split['test'])
        
        print(f"ğŸ“Š Fold {fold_idx} åˆ’åˆ†: train={len(split['train'])}, val={len(split['val'])}, test={len(split['test'])}")
        
        if drop_prob is not None:
            print(f"ğŸ” ä½¿ç”¨ drop_prob={drop_prob} åœ¨ forward æ—¶æ§åˆ¶æ¨¡æ€ä¸¢å¼ƒ")
        
        if not os.path.exists(ckpt_path):
            print(f"âŒ é”™è¯¯ï¼šcheckpointæ–‡ä»¶ä¸å­˜åœ¨: {ckpt_path}")
            continue
        
        test_auc, test_acc = _call_trainer_eval(
            trainer=trainer,
            datasets=(train_ds, val_ds, test_ds),
            fold_idx=fold_idx,
            checkpoint_path=ckpt_path,
            drop_prob=drop_prob,
        )

        per_fold_metrics.append({
            'fold': fold_idx,
            'test_auc': float(test_auc) if test_auc is not None else None,
            'test_acc': float(test_acc) if test_acc is not None else None,
        })

        print(f"Fold {fold_idx}: test_auc={test_auc} test_acc={test_acc}")

    # 7) æ±‡æ€»å¹¶ä¿å­˜
    def _safe_mean(values: List[Optional[float]]) -> Optional[float]:
        xs = [v for v in values if v is not None]
        return float(np.mean(xs)) if xs else None
    def _safe_std(values: List[Optional[float]]) -> Optional[float]:
        xs = [v for v in values if v is not None]
        return float(np.std(xs)) if xs else None

    summary = {
        'drop_prob': drop_prob,
        'per_fold': per_fold_metrics,
        'mean_test_auc': _safe_mean([m['test_auc'] for m in per_fold_metrics]),
        'std_test_auc': _safe_std([m['test_auc'] for m in per_fold_metrics]),
        'mean_test_acc': _safe_mean([m['test_acc'] for m in per_fold_metrics]),
        'std_test_acc': _safe_std([m['test_acc'] for m in per_fold_metrics]),
    }

    out_json = os.path.join(args.results_dir, f'robust_missing_drop_prob_{drop_prob}.json')
    with open(out_json, 'w') as f:
        json.dump(summary, f, indent=2)
    print(f'ä¿å­˜è¯„æµ‹ï¼š{out_json}')


def build_argparser() -> argparse.ArgumentParser:
    """æ„å»ºå‘½ä»¤è¡Œå‚æ•°è§£æã€‚"""
    p = argparse.ArgumentParser(description='ç¼ºæ¨¡æ€é²æ£’æ€§è¯„æµ‹')
    p.add_argument('--results_dir', type=str, required=True, help='è®­ç»ƒç»“æœç›®å½•ï¼ˆåŒ…å« s_?_checkpoint.pt ä¸ configs_*.jsonï¼‰')
    p.add_argument('--data_root_dir', type=str, default=None, help='æ•°æ®æ ¹ç›®å½•ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤å‚æ•°ï¼Œå¦åˆ™å›é€€åˆ°configs')
    p.add_argument('--csv_path', type=str, default=None, help='CSVè·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤å‚æ•°ï¼Œå¦åˆ™å›é€€åˆ°configs')
    p.add_argument('--target_channels', type=str, nargs='*', default=None, help='ç›®æ ‡é€šé“ï¼ˆè‹¥configsç¼ºå¤±æ—¶ä½¿ç”¨ï¼Œæ”¯æŒç®€å†™ï¼‰')
    p.add_argument('--drop_prob', type=float, default=None, help='æ¨¡æ€ä¸¢å¼ƒæ¦‚ç‡ï¼ˆ0.0-1.0ï¼‰ï¼Œåœ¨ forward æ—¶ä¼ å…¥æ¨¡å‹')
    return p


if __name__ == '__main__':
    parser = build_argparser()
    args_ = parser.parse_args()
    run(args_)


