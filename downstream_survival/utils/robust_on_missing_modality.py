#!/usr/bin/env python3
"""
ç¼ºæ¨¡æ€é²æ£’æ€§è¯„æµ‹è„šæœ¬ï¼ˆä»…æµ‹è¯•é›†è¯„æµ‹ï¼‰

åŠŸèƒ½æ¦‚è¿°ï¼š
- ç»™å®šç»“æœç›®å½•ï¼ˆåŒ…å«10ä¸ªfoldçš„checkpointä¸é…ç½®ï¼‰ã€æ•°æ®é›†ã€ç¼ºå¤±æ¨¡æ€æè¿°ä¸åˆ†å‰²æ–¹å¼ï¼›
- åœ¨æ¨ç†é˜¶æ®µå¯¹æŒ‡å®šæ¨¡æ€åšæ©è”½ï¼ˆç»Ÿä¸€ç½®é›¶ï¼‰ï¼Œå¹¶åœ¨å„foldçš„æµ‹è¯•é›†ä¸Šè¯„æµ‹æ¨¡å‹è¡¨ç°ï¼›
- ä»…è¾“å‡ºæµ‹è¯•é›†æŒ‡æ ‡ï¼ˆå¦‚ Test AUC/ACCï¼‰ï¼Œä¿å­˜CSVä¸JSONã€‚

ä¾èµ–ï¼š
- å¤ç”¨ `datasets.multimodal_dataset.MultimodalDataset` åŠ è½½æ•°æ®ï¼›
- å¤ç”¨ `Trainer` å®Œæˆæ¨¡å‹æ„å»ºä¸è¯„æµ‹ï¼ˆé€šè¿‡å¤šç§å¯èƒ½çš„æ¥å£åè‡ªé€‚é…ï¼‰ã€‚
"""

import argparse
import os
import json
import sys
from typing import Dict, List, Optional, Tuple, Any
import random
import numpy as np
import torch
from torch.utils.data import Subset

ROOT_DIR = '/home/zheng/zheng/multimodal-fusion/downstream_survival'
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

from datasets.multimodal_dataset import MultimodalDataset  # noqa: E402
from trainer import Trainer  # noqa: E402
from main import parse_channels, create_k_fold_splits  # noqa: E402


# ä½¿ç”¨ main.parse_channels ä½œä¸ºå•ä¸€çœŸæº

class MaskingDataset(Subset):
    """
    å­é›†æ•°æ®é›†åŒ…è£…å™¨ï¼šåœ¨ __getitem__ æ—¶å¯¹æŒ‡å®šé€šé“è¿›è¡ŒMaskã€‚

    Maskç­–ç•¥ï¼š
    - zero: å°†ç›®æ ‡é€šé“å¼ é‡ç½®é›¶ï¼ˆä¿æŒå½¢çŠ¶ä¸å˜ï¼‰
    - drop: ç­‰åŒäº zeroï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚ï¼Œä»…ç½®é›¶ï¼Œä¸åˆ é™¤é”®ï¼‰
    """

    def __init__(
        self,
        dataset: Subset,
        channels_to_mask: List[str],
    ) -> None:
        super().__init__(dataset.dataset, dataset.indices)
        self.base_subset = dataset
        self.channels_to_mask = set(channels_to_mask or [])
        # å›ºå®šç½®é›¶ç­–ç•¥
        self.mask_strategy = 'zero'

    def __getitem__(self, idx: int):
        base_item = self.base_subset[idx]
        # è®­ç»ƒç®¡çº¿ä¸­ dataset.__getitem__ è¿”å› (dict, label)
        if isinstance(base_item, tuple) and len(base_item) == 2:
            feature_dict, label = base_item
        else:
            # å…¼å®¹ä»…è¿”å›dictçš„æƒ…å†µ
            feature_dict, label = base_item, None

        masked = dict(feature_dict)

        # åŒæ—¶è€ƒè™‘å¯¹é½åçš„é”®ï¼šaligned_<channel>
        aligned_keys = []
        for ch in list(masked.keys()):
            if ch.startswith('aligned_'):
                aligned_keys.append(ch)

        keys_to_process = set()
        for ch in self.channels_to_mask:
            keys_to_process.add(ch)
            keys_to_process.add(f'aligned_{ch}')

        for key in list(masked.keys()):
            if key in keys_to_process:
                tensor = masked.get(key, None)
                if not isinstance(tensor, torch.Tensor):
                    # ä¸å¯ç”¨åˆ™è·³è¿‡
                    continue
                # ç»Ÿä¸€æŒ‰ç½®é›¶å¤„ç†
                masked[key] = torch.zeros_like(tensor)

        if label is None:
            return masked
        return masked, label


def _load_configs_from_results_dir(results_dir: str) -> Dict[str, Any]:
    """
    ä»ç»“æœç›®å½•åŠ è½½é…ç½®JSONï¼ˆåŒ¹é…ç¬¬ä¸€ä¸ª configs_*.json æˆ– configs_*.JSONï¼‰ã€‚

    Returns:
        dict: é…ç½®å­—å…¸ï¼ŒåŒ…å« experiment_config ä¸ model_configã€‚
    """
    candidates = []
    for name in os.listdir(results_dir):
        if name.startswith('configs_') and name.lower().endswith('.json'):
            candidates.append(os.path.join(results_dir, name))
    if not candidates:
        # å…¼å®¹ç”¨æˆ·ç¤ºä¾‹é‡Œçš„å‘½å
        alt = os.path.join(results_dir, 'configs_all_modality_clam_detach.json')
        if os.path.exists(alt):
            candidates.append(alt)
    if not candidates:
        raise FileNotFoundError(f'æœªåœ¨ç›®å½•æ‰¾åˆ°é…ç½®æ–‡ä»¶: {results_dir}')

    cfg_path = sorted(candidates)[0]
    with open(cfg_path, 'r') as f:
        return json.load(f)


def _list_checkpoints(results_dir: str) -> List[Tuple[int, str]]:
    """
    æšä¸¾å„foldçš„checkpointã€‚

    æœŸæœ›å‘½åï¼šs_0_checkpoint.pt ... s_9_checkpoint.pt
    Returns: List[(fold_idx, ckpt_path)]
    """
    items: List[Tuple[int, str]] = []
    for name in os.listdir(results_dir):
        if name.startswith('s_') and name.endswith('_checkpoint.pt'):
            try:
                fold = int(name.split('_')[1])
                items.append((fold, os.path.join(results_dir, name)))
            except Exception:
                continue
    items.sort(key=lambda x: x[0])
    return items

def _load_split_from_csv(results_dir: str, fold_idx: int, dataset: MultimodalDataset) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ä» results_dir/splits_{fold_idx}.csv åŠ è½½è¯¥æŠ˜çš„ train/val/test åˆ’åˆ†ã€‚
    
    æœŸæœ›CSVåˆ—å: train,val,testï¼›
    å•å…ƒæ ¼å€¼ï¼šä¿å­˜çš„æ˜¯case_idï¼ˆå¦‚ 'patient_008'ï¼‰ï¼Œè€Œä¸æ˜¯ç´¢å¼•
    é€šè¿‡case_idæ˜ å°„åˆ°å½“å‰æ•°æ®é›†çš„ç´¢å¼•
    """
    import csv
    
    path = os.path.join(results_dir, f'splits_{fold_idx}.csv')
    if not os.path.exists(path):
        raise FileNotFoundError(f'æœªæ‰¾åˆ°åˆ†å‰²æ–‡ä»¶: {path}')
    
    # åˆ›å»ºcase_idåˆ°ç´¢å¼•çš„æ˜ å°„
    if not hasattr(dataset, 'case_ids'):
        raise ValueError('æ•°æ®é›†å¿…é¡»æœ‰case_idså±æ€§')
    
    case_id_to_idx = {case_id: idx for idx, case_id in enumerate(dataset.case_ids)}
    
    train_indices: List[int] = []
    val_indices: List[int] = []
    test_indices: List[int] = []
    
    with open(path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            for col, case_id in (('train', row.get('train')), ('val', row.get('val')), ('test', row.get('test'))):
                if case_id is None or case_id == '' or case_id.lower() == 'nan':
                    continue
                
                # é€šè¿‡case_idæŸ¥æ‰¾å¯¹åº”çš„ç´¢å¼•
                if case_id in case_id_to_idx:
                    idx = case_id_to_idx[case_id]
                    if col == 'train':
                        train_indices.append(idx)
                    elif col == 'val':
                        val_indices.append(idx)
                    else:
                        test_indices.append(idx)
                else:
                    # case_idä¸åœ¨å½“å‰æ•°æ®é›†ä¸­ï¼ˆå¯èƒ½æ˜¯æ—§æ•°æ®é›†çš„åˆ’åˆ†ï¼‰
                    print(f"âš ï¸ è­¦å‘Šï¼šcase_id {case_id} ä¸åœ¨å½“å‰æ•°æ®é›†ä¸­ï¼Œè·³è¿‡")
    
    return np.array(train_indices, dtype=int), np.array(val_indices, dtype=int), np.array(test_indices, dtype=int)


def _build_dataset(
    csv_path: str,
    data_root_dir: str,
    channels: List[str],
    align_channels: Optional[Dict[str, str]],
    device: torch.device,
) -> MultimodalDataset:
    """
    æ„å»ºå¤šæ¨¡æ€æ•°æ®é›†ã€‚
    """
    return MultimodalDataset(
        csv_path=csv_path,
        data_root_dir=data_root_dir,
        channels=channels,
        align_channels=align_channels,
        alignment_model_path=None,  # ä¸ä½¿ç”¨é¢„è®­ç»ƒå¯¹é½
        device=device,
        print_info=True,
    )


def _call_trainer_eval(
    trainer: Trainer,
    datasets: Tuple[Subset, Optional[Subset], Subset],
    fold_idx: int,
    checkpoint_path: str,
) -> Tuple[Optional[float], Optional[float]]:
    """
    è°ƒç”¨è®­ç»ƒå™¨çš„è¯„æµ‹æ¥å£ï¼ˆè‡ªé€‚é…ä¸åŒå¯èƒ½çš„æ–¹æ³•åï¼‰ã€‚

    Returns:
        (test_auc, test_acc)
    """
    # ä¼˜å…ˆæ‰¾æ˜¾å¼çš„è¯„æµ‹æ¥å£
    if hasattr(trainer, 'evaluate_with_checkpoint'):
        res = trainer.evaluate_with_checkpoint(datasets=datasets, fold_idx=fold_idx, checkpoint_path=checkpoint_path)
    elif hasattr(trainer, 'evaluate_fold'):
        res = trainer.evaluate_fold(datasets=datasets, fold_idx=fold_idx, checkpoint_path=checkpoint_path)
    elif hasattr(trainer, 'test_fold'):
        res = trainer.test_fold(datasets=datasets, fold_idx=fold_idx, checkpoint_path=checkpoint_path)
    elif hasattr(trainer, 'evaluate'):
        res = trainer.evaluate(datasets=datasets, fold_idx=fold_idx, checkpoint_path=checkpoint_path)
    else:
        raise RuntimeError('Trainer æœªæä¾›å…¼å®¹çš„è¯„æµ‹æ¥å£ï¼Œè¯·åœ¨ Trainer ä¸­å®ç° test/evaluate æ¥å£ã€‚')

    # ç»“æœå…¼å®¹ï¼šå¸¸è§è¿”å› (results_dict, test_auc, val_auc, test_acc, val_acc)
    test_auc = None
    test_acc = None
    if isinstance(res, tuple):
        # è§£æ test_accï¼ˆé€šå¸¸ä½äºå€’æ•°ç¬¬äºŒä¸ªï¼‰
        if len(res) >= 2:
            try:
                test_acc = float(res[-2])
            except Exception:
                test_acc = None
        # è§£æ test_aucï¼ˆé€šå¸¸ä½äºç´¢å¼•1ï¼‰
        if len(res) >= 2:
            try:
                test_auc = float(res[1])
            except Exception:
                test_auc = None
    elif isinstance(res, dict):
        test_auc = float(res.get('test_auc')) if res.get('test_auc') is not None else None
        test_acc = float(res.get('test_acc')) if res.get('test_acc') is not None else None
    return test_auc, test_acc


def run(args: argparse.Namespace) -> None:
    """
    ä¸»æµç¨‹ï¼šåŠ è½½é…ç½®ä¸checkpoint -> æ„é€ æ•°æ®ä¸åˆ†å‰² -> æ©è”½æŒ‡å®šæ¨¡æ€ -> è¯„æµ‹ -> ä¿å­˜æŠ¥å‘Šã€‚
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 1) ç»“æœç›®å½•é…ç½®ä¸checkpoint
    configs = _load_configs_from_results_dir(args.results_dir)
    exp_cfg = configs.get('experiment_config', {})
    model_cfg = configs.get('model_config', {})

    # åŸºç¡€é€šé“æŒ‰ä¸»ç¨‹åºè§„åˆ™è§£æï¼ˆæ”¯æŒç®€å†™ï¼‰
    base_target_channels = exp_cfg.get('target_channels') or model_cfg.get('channels_used_in_model')
    if not base_target_channels:
        # è‹¥é…ç½®é‡Œæ²¡æœ‰ï¼Œä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®š
        base_target_channels = parse_channels(args.target_channels or [])

    # è§£æè¦maskçš„é€šé“ï¼ˆæ”¯æŒç®€å†™ä¸å®Œæ•´è·¯å¾„æ··åˆï¼‰
    channels_to_mask = parse_channels(args.missing_modalities or [])

    # 2) æ•°æ®é›†
    dataset = _build_dataset(
        csv_path=args.csv_path or exp_cfg.get('csv_path'),
        data_root_dir=args.data_root_dir or exp_cfg.get('data_root_dir'),
        channels=base_target_channels,
        align_channels=exp_cfg.get('aligned_channels'),
        device=device,
    )
    
    print(f"ğŸ“Š æ•°æ®é›†æ„å»ºå®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“ data_root_dir: {args.data_root_dir or exp_cfg.get('data_root_dir')}")
    print(f"ğŸ“‹ channels: {base_target_channels[:5]}..." if len(base_target_channels) > 5 else f"ğŸ“‹ channels: {base_target_channels}")
    
    # éªŒè¯æ•°æ®é›†é¡ºåºï¼ˆæ‰“å°å‰5ä¸ªcase_idï¼‰
    if hasattr(dataset, 'case_ids') and len(dataset.case_ids) > 0:
        print(f"ğŸ” å‰5ä¸ªcase_id: {dataset.case_ids[:5]}")
        print(f"ğŸ” å5ä¸ªcase_id: {dataset.case_ids[-5:]}")
        print(f"ğŸ” æ•°æ®é›†å¤§å°: {len(dataset.case_ids)}")

    # 3) é‡æ–°ç”ŸæˆKæŠ˜åˆ’åˆ†ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    seed = exp_cfg.get('seed', 5678)
    k = exp_cfg.get('num_splits', 10)
    print(f"ğŸ”§ ä½¿ç”¨é…ç½®ä¸­çš„ seed={seed}, k={k} é‡æ–°ç”Ÿæˆåˆ’åˆ†ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰")
    
    # éªŒè¯æ•°æ®é›†æ˜¯å¦ä¸è®­ç»ƒæ—¶ä¸€è‡´
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯éªŒè¯:")
    print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
    if hasattr(dataset, 'case_ids'):
        print(f"   case_idsæ•°é‡: {len(dataset.case_ids)}")
        print(f"   å‰10ä¸ªcase_id: {dataset.case_ids[:10]}")
    
    splits = create_k_fold_splits(dataset, k=k, seed=seed, fixed_test_split=None)
    print(f"âœ… ç”Ÿæˆäº† {len(splits)} ä¸ª folds")
    
    # éªŒè¯Fold 0çš„åˆ’åˆ†ï¼ˆä¸è®­ç»ƒæ—¶å¯¹æ¯”ï¼‰
    if len(splits) > 0:
        fold0_split = splits[0]
        print(f"\nğŸ“Š Fold 0 åˆ’åˆ†éªŒè¯:")
        print(f"   train: {len(fold0_split['train'])} ä¸ª")
        print(f"   val: {len(fold0_split['val'])} ä¸ª")
        print(f"   test: {len(fold0_split['test'])} ä¸ª")
        if hasattr(dataset, 'case_ids'):
            fold0_test_case_ids = [dataset.case_ids[i] for i in fold0_split['test'][:10]]
            print(f"   Fold 0 testé›†å‰10ä¸ªcase_id: {fold0_test_case_ids}")
    
    # 4) è·å–checkpointåˆ—è¡¨
    checkpoints = _list_checkpoints(args.results_dir)
    if not checkpoints:
        raise FileNotFoundError('æœªæ‰¾åˆ°ä»»ä½• checkpointï¼ˆå½¢å¦‚ s_0_checkpoint.ptï¼‰ã€‚')

    # 5) è®­ç»ƒå™¨
    trainer = Trainer(configs=configs, log_dir=os.path.join(args.results_dir, 'training_logs'))

    # 6) éå†foldå¹¶è¯„æµ‹ï¼ˆä½¿ç”¨é‡æ–°ç”Ÿæˆçš„åˆ’åˆ†ï¼‰ã€‚ä»…åœ¨æµ‹è¯•é›†ä¸Šåšmaskä¸è¯„æµ‹ã€‚
    per_fold_metrics = []

    for fold_idx, ckpt_path in checkpoints:
        if fold_idx >= len(splits):
            print(f"âš ï¸ Fold {fold_idx} è¶…å‡ºåˆ’åˆ†èŒƒå›´ï¼Œè·³è¿‡")
            continue
        
        split = splits[fold_idx]
        train_idx = split['train']
        val_idx = split['val']
        test_idx = split['test']
        
        print(f"ğŸ“Š Fold {fold_idx} åˆ’åˆ†: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}")
        
        # æ‰“å°æµ‹è¯•é›†çš„å‰5ä¸ªcase_idç”¨äºéªŒè¯
        if hasattr(dataset, 'case_ids') and len(test_idx) > 0:
            test_case_ids = [dataset.case_ids[i] for i in test_idx[:5]]
            print(f"ğŸ” Fold {fold_idx} testé›†å‰5ä¸ªcase_id: {test_case_ids}")

        train_ds = Subset(dataset, train_idx)
        val_ds = Subset(dataset, val_idx) if len(val_idx) > 0 else None
        test_ds = Subset(dataset, test_idx)

        # ä»…å¯¹æµ‹è¯•é›†åšmaskï¼›è®­ç»ƒ/éªŒè¯ä¿æŒåŸæ ·
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¦maskçš„é€šé“ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æµ‹è¯•é›†ï¼ˆé¿å…ä¸å¿…è¦çš„åŒ…è£…ï¼‰
        if channels_to_mask:
            masked_test = MaskingDataset(test_ds, channels_to_mask=channels_to_mask)
        else:
            masked_test = test_ds  # ä¸åšmaskï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æµ‹è¯•é›†

        datasets_tuple = (train_ds, val_ds, masked_test)

        print(f"ğŸ”§ è¯„æµ‹ Fold {fold_idx} checkpoint: {ckpt_path}")
        
        # éªŒè¯checkpointæ˜¯å¦å­˜åœ¨ä¸”å¯è¯»
        if not os.path.exists(ckpt_path):
            print(f"âŒ é”™è¯¯ï¼šcheckpointæ–‡ä»¶ä¸å­˜åœ¨: {ckpt_path}")
            continue
        
        # æ£€æŸ¥checkpointæ–‡ä»¶å¤§å°å’Œæ—¶é—´æˆ³
        ckpt_stat = os.stat(ckpt_path)
        print(f"ğŸ“¦ checkpointæ–‡ä»¶ä¿¡æ¯: å¤§å°={ckpt_stat.st_size} bytes, ä¿®æ”¹æ—¶é—´={ckpt_stat.st_mtime}")
        
        test_auc, test_acc = _call_trainer_eval(
            trainer=trainer,
            datasets=datasets_tuple,
            fold_idx=fold_idx,
            checkpoint_path=ckpt_path,
        )

        per_fold_metrics.append({
            'fold': fold_idx,
            'test_auc': float(test_auc) if test_auc is not None else None,
            'test_acc': float(test_acc) if test_acc is not None else None,
        })

        print(f"Fold {fold_idx}: test_auc={test_auc} test_acc={test_acc}")

    # 7) æ±‡æ€»å¹¶ä¿å­˜
    def _safe_mean(values: List[Optional[float]]) -> Optional[float]:
        xs = [v for v in values if v is not None]
        return float(np.mean(xs)) if xs else None
    def _safe_std(values: List[Optional[float]]) -> Optional[float]:
        xs = [v for v in values if v is not None]
        return float(np.std(xs)) if xs else None

    summary = {
        'missing_modalities': args.missing_modalities,
        'per_fold': per_fold_metrics,
        'mean_test_auc': _safe_mean([m['test_auc'] for m in per_fold_metrics]),
        'std_test_auc': _safe_std([m['test_auc'] for m in per_fold_metrics]),
        'mean_test_acc': _safe_mean([m['test_acc'] for m in per_fold_metrics]),
        'std_test_acc': _safe_std([m['test_acc'] for m in per_fold_metrics]),
    }

    out_json = os.path.join(args.results_dir, 'robust_missing_eval.json')
    with open(out_json, 'w') as f:
        json.dump(summary, f, indent=2)

    # å¯é€‰CSVï¼ˆæŒ‰foldï¼‰
    try:
        import pandas as pd  # ä»…ç”¨äºä¿å­˜CSV
        pd.DataFrame(per_fold_metrics).to_csv(os.path.join(args.results_dir, 'robust_missing_eval_per_fold.csv'), index=False)
    except Exception:
        pass

    print('ä¿å­˜è¯„æµ‹ï¼š', out_json)


def build_argparser() -> argparse.ArgumentParser:
    """
    æ„å»ºå‘½ä»¤è¡Œå‚æ•°è§£æã€‚
    """
    p = argparse.ArgumentParser(description='ç¼ºæ¨¡æ€é²æ£’æ€§è¯„æµ‹')
    p.add_argument('--results_dir', type=str, required=True, help='è®­ç»ƒç»“æœç›®å½•ï¼ˆåŒ…å« s_?_checkpoint.pt ä¸ configs_*.jsonï¼‰')
    p.add_argument('--data_root_dir', type=str, default=None, help='æ•°æ®æ ¹ç›®å½•ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤å‚æ•°ï¼Œå¦åˆ™å›é€€åˆ°configs')
    p.add_argument('--csv_path', type=str, default=None, help='CSVè·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤å‚æ•°ï¼Œå¦åˆ™å›é€€åˆ°configs')
    p.add_argument('--target_channels', type=str, nargs='*', default=None, help='ç›®æ ‡é€šé“ï¼ˆè‹¥configsç¼ºå¤±æ—¶ä½¿ç”¨ï¼Œæ”¯æŒç®€å†™ï¼‰')
    p.add_argument('--missing_modalities', type=str, nargs='*', default=None, help='éœ€Maskçš„æ¨¡æ€ï¼ˆæ”¯æŒç®€å†™ï¼šå¦‚ wsi, cd3, clinical ç­‰ï¼‰ï¼›ä¸ä¼ åˆ™ä¸åšmask')
    return p


if __name__ == '__main__':
    parser = build_argparser()
    args_ = parser.parse_args()
    run(args_)


