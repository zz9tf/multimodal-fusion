#!/usr/bin/env python3
"""
æ¨¡æ‹Ÿè®­ç»ƒè„šæœ¬ï¼šåªè®­ç»ƒå‡ æ­¥ï¼Œä¿å­˜æ¨¡å‹å’Œsplitç”¨äºæµ‹è¯•æ¢å¤åŠŸèƒ½
"""

import argparse
import os
import sys
import json
import torch
import numpy as np
from datetime import datetime
from torch.utils.data import Subset

# æ·»åŠ é¡¹ç›®è·¯å¾„
root_dir = '/home/zheng/zheng/multimodal-fusion/downstream_survival'
sys.path.append(root_dir)

from trainer import Trainer
from datasets.multimodal_dataset import MultimodalDataset
from main import parse_channels, create_k_fold_splits, _parse_aligned_channels, seed_torch

def main():
    parser = argparse.ArgumentParser(description='æ¨¡æ‹Ÿè®­ç»ƒï¼šåªè®­ç»ƒå‡ æ­¥ï¼Œä¿å­˜æ¨¡å‹å’Œsplit')
    
    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument('--data_root_dir', type=str, required=True, help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--csv_path', type=str, default='dataset_csv/survival_dataset.csv', help='CSVæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: dataset_csv/survival_dataset.csvï¼‰')
    parser.add_argument('--results_dir', type=str, default='./test_results', help='ç»“æœä¿å­˜ç›®å½•')
    
    # æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument('--target_channels', type=str, nargs='+', 
                       default=['CD3', 'CD8'], help='ç›®æ ‡é€šé“')
    parser.add_argument('--model_type', type=str, default='clam_mlp_detach', 
                       choices=['clam_mlp_detach', 'clam_mlp', 'clam'], help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--input_dim', type=int, default=1024, help='è¾“å…¥ç»´åº¦')
    parser.add_argument('--output_dim', type=int, default=128, help='è¾“å‡ºç»´åº¦')
    parser.add_argument('--n_classes', type=int, default=2, help='ç±»åˆ«æ•°')
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--k', type=int, default=3, help='foldæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼Œåªè®­ç»ƒ3ä¸ªfoldï¼‰')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--max_epochs', type=int, default=5, help='æœ€å¤§è®­ç»ƒè½®æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼Œåªè®­ç»ƒ5ä¸ªepochï¼‰')
    parser.add_argument('--batch_size', type=int, default=4, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--reg', type=float, default=1e-5, help='æƒé‡è¡°å‡')
    parser.add_argument('--opt', type=str, default='adam', choices=['adam', 'sgd'], help='ä¼˜åŒ–å™¨ç±»å‹')
    parser.add_argument('--early_stopping', action='store_true', default=False, help='å¯ç”¨æ—©åœ')
    
    # å¯¹é½ç›¸å…³å‚æ•°
    parser.add_argument('--aligned_channels', type=str, nargs='*', default=None, 
                       help='å¯¹é½ç›®æ ‡ï¼Œæ ¼å¼: channel_to_align1=align_channel_name1 ...')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    seed_torch(args.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºç»“æœç›®å½•
    if not os.path.isdir(args.results_dir):
        os.makedirs(args.results_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    args.results_dir = os.path.join(args.results_dir, f"test_train_{timestamp}_s{args.seed}")
    os.makedirs(args.results_dir)
    
    print("="*60)
    print("æ¨¡æ‹Ÿè®­ç»ƒï¼šåªè®­ç»ƒå‡ æ­¥ï¼Œä¿å­˜æ¨¡å‹å’Œsplit")
    print("="*60)
    
    # 1. è§£æchannels
    try:
        parsed_channels = parse_channels(args.target_channels)
        print(f"âœ… æˆåŠŸè§£æé€šé“: {len(parsed_channels)} ä¸ª")
        print(f"ğŸ“‹ åŸå§‹é€šé“: {args.target_channels}")
        print(f"ğŸ”— è§£æåé€šé“: {parsed_channels[:5]}..." if len(parsed_channels) > 5 else f"ğŸ”— è§£æåé€šé“: {parsed_channels}")
    except ValueError as e:
        print(f"âŒ é€šé“è§£æé”™è¯¯: {e}")
        return
    
    # 2. æ„å»ºalign_channelsæ˜ å°„
    align_channels = _parse_aligned_channels(args.aligned_channels)
    
    # 3. åˆ›å»ºæ•°æ®é›†
    print(f"\nğŸ“¦ åŠ è½½æ•°æ®é›†...")
    
    # å¤„ç†è·¯å¾„ï¼šå¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºåŸºäºé¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
    if args.csv_path and not os.path.isabs(args.csv_path):
        csv_path = os.path.join(root_dir, args.csv_path)
    else:
        csv_path = args.csv_path
    
    if args.data_root_dir and not os.path.isabs(args.data_root_dir):
        # data_root_dir é€šå¸¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œä½†å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ä¹Ÿå¤„ç†
        data_root_dir = os.path.abspath(args.data_root_dir)
    else:
        data_root_dir = args.data_root_dir
    
    print(f"   data_root_dir: {data_root_dir}")
    print(f"   csv_path: {csv_path}")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(csv_path):
        # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„CSVæ–‡ä»¶
        possible_names = ['survival_dataset.csv', 'survival_status_labels.csv']
        csv_dir = os.path.dirname(csv_path) if os.path.dirname(csv_path) else root_dir
        csv_dir = os.path.join(root_dir, 'dataset_csv') if 'dataset_csv' in csv_path else csv_dir
        
        suggestions = []
        if os.path.exists(csv_dir):
            for name in possible_names:
                possible_path = os.path.join(csv_dir, name)
                if os.path.exists(possible_path):
                    suggestions.append(possible_path)
        
        error_msg = f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}\n"
        if suggestions:
            error_msg += f"ğŸ’¡ æ‰¾åˆ°å¯èƒ½çš„CSVæ–‡ä»¶:\n"
            for sug in suggestions:
                error_msg += f"   - {sug}\n"
            error_msg += f"ğŸ’¡ è¯·ä½¿ç”¨: --csv_path {suggestions[0]}"
        else:
            error_msg += f"ğŸ’¡ è¯·ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„ç›¸å¯¹è·¯å¾„\n"
            error_msg += f"ğŸ’¡ å¸¸è§çš„CSVæ–‡ä»¶ä½ç½®: {os.path.join(root_dir, 'dataset_csv')}"
        
        raise FileNotFoundError(error_msg)
    if not os.path.exists(data_root_dir):
        raise FileNotFoundError(f"âŒ æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {data_root_dir}")
    
    dataset = MultimodalDataset(
        csv_path=csv_path,
        data_root_dir=data_root_dir,
        channels=parsed_channels,
        align_channels=align_channels,
        alignment_model_path=None,
        device=device,
        print_info=True
    )
    
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")
    if hasattr(dataset, 'case_ids') and len(dataset.case_ids) > 0:
        print(f"ğŸ” å‰5ä¸ªcase_id: {dataset.case_ids[:5]}")
        print(f"ğŸ” æ•°æ®é›†å¤§å°: {len(dataset.case_ids)}")
    
    # 4. åˆ›å»ºk-foldåˆ†å‰²
    print(f"\nğŸ“Š åˆ›å»º {args.k}-fold äº¤å‰éªŒè¯åˆ†å‰²...")
    splits = create_k_fold_splits(dataset, k=args.k, seed=args.seed, fixed_test_split=None)
    print(f"âœ… åˆ›å»ºäº† {len(splits)} ä¸ª folds")
    
    if len(splits) > 0:
        fold0_split = splits[0]
        print(f"\nğŸ“Š Fold 0 åˆ’åˆ†:")
        print(f"   train: {len(fold0_split['train'])} ä¸ª")
        print(f"   val: {len(fold0_split['val'])} ä¸ª")
        print(f"   test: {len(fold0_split['test'])} ä¸ª")
        if hasattr(dataset, 'case_ids'):
            fold0_test_case_ids = [dataset.case_ids[i] for i in fold0_split['test'][:10]]
            print(f"   Fold 0 testé›†å‰10ä¸ªcase_id: {fold0_test_case_ids}")
    
    # 5. æ„å»ºé…ç½®
    configs = {
        'experiment_config': {
            'data_root_dir': data_root_dir,
            'results_dir': args.results_dir,
            'csv_path': csv_path,
            'alignment_model_path': None,
            'target_channels': parsed_channels,
            'aligned_channels': align_channels,
            'exp_code': 'test_train',
            'seed': args.seed,
            'num_splits': args.k,
            'split_mode': 'random',
            'dataset_split_path': None,
            'max_epochs': args.max_epochs,
            'lr': args.lr,
            'reg': args.reg,
            'opt': args.opt,
            'early_stopping': args.early_stopping,
            'batch_size': args.batch_size,
            'scheduler_config': {'type': None}
        },
        'model_config': {
            'model_type': args.model_type,
            'input_dim': args.input_dim,
            'dropout': 0.25,
            'n_classes': args.n_classes,
            'base_loss_fn': 'ce',
            'channels_used_in_model': parsed_channels,
            'gate': True,
            'base_weight': 0.7,
            'inst_loss_fn': None,
            'model_size': 'small',
            'subtyping': False,
            'inst_number': 8,
            'return_features': False,
            'attention_only': False,
            'output_dim': args.output_dim,
        }
    }
    
    # ä¿å­˜é…ç½®
    configs_path = os.path.join(args.results_dir, 'configs_test.json')
    with open(configs_path, 'w') as f:
        json.dump(configs, f, indent=2, default=str)
    print(f"\nâœ… é…ç½®å·²ä¿å­˜åˆ°: {configs_path}")
    
    # 6. åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = Trainer(
        configs=configs,
        log_dir=os.path.join(args.results_dir, 'training_logs')
    )
    
    # 7. è®­ç»ƒæ¯ä¸ªfoldï¼ˆåªè®­ç»ƒå‡ ä¸ªfoldï¼Œç”¨äºæµ‹è¯•ï¼‰
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼ˆåªè®­ç»ƒ {args.k} ä¸ª foldsï¼Œæ¯ä¸ªfold {args.max_epochs} ä¸ªepochï¼‰...")
    
    all_test_auc = []
    all_val_auc = []
    all_test_acc = []
    all_val_acc = []
    
    for fold_idx in range(args.k):
        print(f"\n{'='*60}")
        print(f'è®­ç»ƒ Fold {fold_idx+1}/{args.k}')
        print(f"{'='*60}")
        
        seed_torch(args.seed)
        
        # è·å–å½“å‰foldçš„åˆ†å‰²
        split = splits[fold_idx]
        train_idx = split['train']
        val_idx = split['val']
        test_idx = split['test']
        
        print(f'Train samples: {len(train_idx)}')
        print(f'Val samples: {len(val_idx)}')
        print(f'Test samples: {len(test_idx)}')
        
        # åˆ›å»ºå­æ•°æ®é›†
        train_dataset = Subset(dataset, train_idx)
        val_dataset = Subset(dataset, val_idx)
        test_dataset = Subset(dataset, test_idx)
        
        datasets = (train_dataset, val_dataset, test_dataset)
        
        # ä½¿ç”¨è®­ç»ƒå™¨è¿›è¡Œè®­ç»ƒ
        print(f"\nğŸ“ å¼€å§‹è®­ç»ƒ Fold {fold_idx}...")
        results, test_auc, val_auc, test_acc, val_acc = trainer.train_fold(
            datasets=datasets,
            fold_idx=fold_idx
        )
        
        all_test_auc.append(test_auc)
        all_val_auc.append(val_auc)
        all_test_acc.append(test_acc)
        all_val_acc.append(val_acc)
        
        print(f'Fold {fold_idx+1} å®Œæˆ - Test AUC: {test_auc:.4f}, Val AUC: {val_auc:.4f}')
        print(f'                Test Acc: {test_acc:.4f}, Val Acc: {val_acc:.4f}')
        
        # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
        checkpoint_path = os.path.join(args.results_dir, f's_{fold_idx}_checkpoint.pt')
        split_path = os.path.join(args.results_dir, f'splits_{fold_idx}.csv')
        
        if os.path.exists(checkpoint_path):
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {checkpoint_path} ({os.path.getsize(checkpoint_path)} bytes)")
        else:
            print(f"âŒ æ¨¡å‹æœªä¿å­˜: {checkpoint_path}")
        
        if os.path.exists(split_path):
            print(f"âœ… Splitå·²ä¿å­˜: {split_path} ({os.path.getsize(split_path)} bytes)")
        else:
            print(f"âŒ Splitæœªä¿å­˜: {split_path}")
    
    # 8. ä¿å­˜æœ€ç»ˆç»“æœæ‘˜è¦
    print(f"\n{'='*60}")
    print('è®­ç»ƒç»“æœæ‘˜è¦')
    print(f"{'='*60}")
    print(f'Mean Test AUC: {np.mean(all_test_auc):.4f} Â± {np.std(all_test_auc):.4f}')
    print(f'Mean Val AUC: {np.mean(all_val_auc):.4f} Â± {np.std(all_val_auc):.4f}')
    print(f'Mean Test Acc: {np.mean(all_test_acc):.4f} Â± {np.std(all_test_acc):.4f}')
    print(f'Mean Val Acc: {np.mean(all_val_acc):.4f} Â± {np.std(all_val_acc):.4f}')
    
    detailed_results = {
        'configurations': configs,
        'fold_results': {
            'folds': list(range(args.k)),
            'test_auc': all_test_auc,
            'val_auc': all_val_auc,
            'test_acc': all_test_acc,
            'val_acc': all_val_acc
        },
        'summary_stats': {
            'mean_test_auc': float(np.mean(all_test_auc)),
            'std_test_auc': float(np.std(all_test_auc)),
            'mean_val_auc': float(np.mean(all_val_auc)),
            'std_val_auc': float(np.std(all_val_auc)),
            'mean_test_acc': float(np.mean(all_test_acc)),
            'std_test_acc': float(np.std(all_test_acc)),
            'mean_val_acc': float(np.mean(all_val_acc)),
            'std_val_acc': float(np.std(all_val_acc))
        }
    }
    
    results_path = os.path.join(args.results_dir, 'detailed_results_for_plotting.json')
    with open(results_path, 'w') as f:
        json.dump(detailed_results, f, indent=2, default=str)
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {args.results_dir}")
    print(f"\nğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨ test_restore.py æ¥æ¢å¤å¹¶éªŒè¯è¿™äº›ç»“æœ")

if __name__ == "__main__":
    main()

