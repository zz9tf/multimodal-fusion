#!/usr/bin/env python3
"""
æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–ä¸€è‡´æ€§
éªŒè¯åœ¨ seed_torch çš„æƒ…å†µä¸‹ï¼Œä¸¤æ¬¡åˆå§‹åŒ–æ¨¡å‹æ˜¯å¦ä¼šäº§ç”Ÿç›¸åŒçš„æƒé‡
åŒ…æ‹¬ï¼šåˆå§‹åŒ– -> ä¿å­˜ -> ç­‰å¾…10ç§’ -> åŠ è½½ -> å†æ¬¡åˆå§‹åŒ–çš„å®Œæ•´æµç¨‹
"""

import os
import sys
import torch
import numpy as np
import json
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
root_dir = '/home/zheng/zheng/multimodal-fusion/downstream_survival'
sys.path.append(root_dir)

from trainer import Trainer
from main import seed_torch


def compare_model_weights(model1, model2, tolerance=1e-6):
    """
    æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æƒé‡æ˜¯å¦ä¸€è‡´
    é€ä¸€æ¯”è¾ƒæ‰€æœ‰å‚æ•°å…ƒç´ ï¼ˆ445k+ä¸ªå‚æ•°ï¼‰
    
    Args:
        model1: ç¬¬ä¸€ä¸ªæ¨¡å‹
        model2: ç¬¬äºŒä¸ªæ¨¡å‹
        tolerance: æ•°å€¼å®¹å·®
        
    Returns:
        dict: æ¯”è¾ƒç»“æœ
    """
    state_dict1 = model1.state_dict()
    state_dict2 = model2.state_dict()
    
    results = {
        'total_param_tensors': 0,          # å‚æ•°å¼ é‡ä¸ªæ•°ï¼ˆstate_dicté”®æ•°é‡ï¼‰
        'total_param_elements': 0,         # å‚æ•°å…ƒç´ æ€»æ•°ï¼ˆæ ‡é‡ä¸ªæ•°ï¼Œåº”è¯¥æ˜¯445k+ï¼‰
        'matching_elements': 0,            # åœ¨å®¹å·®å†…ç›¸ç­‰çš„å…ƒç´ ä¸ªæ•°
        'different_elements': 0,           # è¶…è¿‡å®¹å·®çš„å…ƒç´ ä¸ªæ•°
        'matching_param_tensors': 0,       # åœ¨å®¹å·®å†…å®Œå…¨ç›¸ç­‰çš„å‚æ•°å¼ é‡ä¸ªæ•°
        'different_param_tensors': 0,      # è¶…è¿‡å®¹å·®çš„å‚æ•°å¼ é‡ä¸ªæ•°
        'global_max_abs_diff': 0.0,        # å…¨å±€å…ƒç´ çº§æœ€å¤§ç»å¯¹å·®
        'global_mean_abs_diff': 0.0,       # å…¨å±€å…ƒç´ çº§å¹³å‡ç»å¯¹å·®ï¼ˆæŒ‰å…ƒç´ åŠ æƒï¼‰
        'different_keys': [],              # è¶…å‡ºå®¹å·®çš„é”®åˆ—è¡¨ï¼ˆå¸¦ç»Ÿè®¡ï¼‰
        'missing_keys_1': [],
        'missing_keys_2': []
    }
    
    all_keys = set(state_dict1.keys()) | set(state_dict2.keys())
    results['total_param_tensors'] = len(all_keys)
    
    # å…¨å±€å…ƒç´ çº§ç»Ÿè®¡
    global_max = 0.0
    sum_abs_diff = 0.0
    total_elems = 0
    matching_elems = 0
    different_elems = 0
    
    print(f"   ğŸ” å¼€å§‹é€ä¸€æ¯”è¾ƒæ‰€æœ‰å‚æ•°å…ƒç´ ...")
    
    for key in sorted(all_keys):  # æŒ‰å­—æ¯é¡ºåºæ’åºï¼Œä¾¿äºæŸ¥çœ‹
        if key not in state_dict1:
            results['missing_keys_1'].append(key)
            continue
        if key not in state_dict2:
            results['missing_keys_2'].append(key)
            continue
        
        param1 = state_dict1[key]
        param2 = state_dict2[key]
        
        if param1.shape != param2.shape:
            results['different_keys'].append(f"{key}: shape mismatch ({param1.shape} vs {param2.shape})")
            results['different_param_tensors'] += 1
            continue
        
        # è®¡ç®—å·®å¼‚
        diff = torch.abs(param1 - param2)
        max_diff = torch.max(diff).item()
        mean_diff = torch.mean(diff).item()
        
        # å…ƒç´ çº§ç´¯ç§¯
        elem_count = diff.numel()
        total_elems += elem_count
        
        # ç»Ÿè®¡åŒ¹é…å’Œä¸åŒ¹é…çš„å…ƒç´ æ•°é‡
        matching_mask = diff <= tolerance
        matching_count = int(matching_mask.sum().item())
        different_count = elem_count - matching_count
        
        matching_elems += matching_count
        different_elems += different_count
        
        sum_abs_diff += float(diff.sum().item())
        if max_diff > global_max:
            global_max = max_diff
        
        # å¦‚æœè¯¥å¼ é‡æœ‰ä»»ä½•å…ƒç´ è¶…è¿‡å®¹å·®ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
        if max_diff > tolerance:
            results['different_param_tensors'] += 1
            results['different_keys'].append({
                'key': key,
                'shape': list(param1.shape),
                'numel': elem_count,
                'matching_elements': matching_count,
                'different_elements': different_count,
                'max_diff': max_diff,
                'mean_diff': mean_diff,
                'max_diff_location': None  # å¯ä»¥æ·»åŠ æœ€å¤§å·®å¼‚çš„ä½ç½®
            })
        else:
            results['matching_param_tensors'] += 1
    
    results['total_param_elements'] = int(total_elems)
    results['matching_elements'] = int(matching_elems)
    results['different_elements'] = int(different_elems)
    results['global_max_abs_diff'] = float(global_max)
    results['global_mean_abs_diff'] = float(sum_abs_diff / total_elems) if total_elems > 0 else 0.0
    
    print(f"   âœ… æ¯”è¾ƒå®Œæˆ: å…±æ¯”è¾ƒ {total_elems:,} ä¸ªå‚æ•°å…ƒç´ ")
    
    return results


def test_model_init_consistency(results_dir: str, fold_idx: int = 0, wait_seconds: int = 10):
    """
    æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–ä¸€è‡´æ€§ï¼ˆåŒ…æ‹¬ä¿å­˜ã€åŠ è½½ã€å¤šæ¬¡åˆå§‹åŒ–ï¼‰
    
    Args:
        results_dir: ç»“æœç›®å½•è·¯å¾„
        fold_idx: foldç´¢å¼•
        wait_seconds: ä¿å­˜åç­‰å¾…çš„ç§’æ•°
    """
    results_dir = Path(results_dir)
    configs_file = results_dir / 'configs_svd_random_clam_detach.json'
    
    if not configs_file.exists():
        # å°è¯•æŸ¥æ‰¾å…¶ä»–é…ç½®æ–‡ä»¶
        config_files = list(results_dir.glob('configs_*.json'))
        if config_files:
            configs_file = config_files[0]
        else:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {results_dir}")
    
    # åŠ è½½é…ç½®
    with open(configs_file, 'r') as f:
        configs = json.load(f)
    
    print(f"ğŸ“‹ åŠ è½½é…ç½®æ–‡ä»¶: {configs_file}")
    print(f"ğŸ“‹ æ¨¡å‹ç±»å‹: {configs['model_config']['model_type']}")
    
    # éªŒè¯æ¨¡å‹ç±»å‹
    expected_model_type = 'svd_gate_random_clam_detach'
    actual_model_type = configs['model_config']['model_type']
    if actual_model_type != expected_model_type:
        print(f"âš ï¸ è­¦å‘Š: æ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼æœŸæœ›: {expected_model_type}, å®é™…: {actual_model_type}")
    else:
        print(f"âœ… æ¨¡å‹ç±»å‹éªŒè¯é€šè¿‡: {actual_model_type}")
    
    # æ‰“å°å…³é”®é…ç½®ä¿¡æ¯
    print(f"\nğŸ“‹ å…³é”®æ¨¡å‹é…ç½®:")
    model_config = configs['model_config']
    print(f"   input_dim: {model_config.get('input_dim', 'N/A')}")
    print(f"   output_dim: {model_config.get('output_dim', 'N/A')}")
    print(f"   n_classes: {model_config.get('n_classes', 'N/A')}")
    print(f"   dropout: {model_config.get('dropout', 'N/A')}")
    print(f"   enable_svd: {model_config.get('enable_svd', 'N/A')}")
    print(f"   enable_random_loss: {model_config.get('enable_random_loss', 'N/A')}")
    print(f"   channels_used_in_model: {len(model_config.get('channels_used_in_model', []))} ä¸ªé€šé“")
    
    # è·å–seed
    seed = configs['experiment_config'].get('seed', 5678)
    print(f"\nğŸŒ± ä½¿ç”¨éšæœºç§å­: {seed}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        configs=configs,
        log_dir=str(results_dir / 'training_logs')
    )
    
    # éªŒè¯è®­ç»ƒå™¨ä½¿ç”¨çš„é…ç½®
    print(f"âœ… è®­ç»ƒå™¨å·²åˆ›å»ºï¼Œä½¿ç”¨é…ç½®: {trainer.model_config['model_type']}")
    
    # ========== ç¬¬ä¸€æ¬¡åˆå§‹åŒ–æ¨¡å‹ ==========
    print(f"\n{'='*60}")
    print("ç¬¬ä¸€æ¬¡åˆå§‹åŒ–æ¨¡å‹")
    print(f"{'='*60}")
    
    seed_torch(seed)
    model1 = trainer._init_model()
    state_dict1 = model1.state_dict()
    
    print(f"âœ… æ¨¡å‹1åˆå§‹åŒ–å®Œæˆ")
    print(f"   æ¨¡å‹ç±»å‹: {type(model1).__name__}")
    
    # ç»Ÿè®¡å®é™…å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model1.parameters())
    trainable_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)
    state_dict_keys = len(state_dict1)
    
    print(f"   state_dict é”®æ•°é‡: {state_dict_keys}")
    print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    print(f"   ä¸å¯è®­ç»ƒå‚æ•°æ•°é‡: {total_params - trainable_params:,}")
    
    # éªŒè¯æ¨¡å‹æ˜¯å¦æ˜¯æ­£ç¡®çš„ç±»å‹
    from models.svd_gate_random_clam_detach import SVDGateRandomClamDetach
    if isinstance(model1, SVDGateRandomClamDetach):
        print(f"   âœ… æ¨¡å‹ç±»å‹éªŒè¯é€šè¿‡: SVDGateRandomClamDetach")
    else:
        print(f"   âš ï¸ è­¦å‘Š: æ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼æœŸæœ›: SVDGateRandomClamDetach, å®é™…: {type(model1).__name__}")
    
    # æ£€æŸ¥ transfer_layer æ˜¯å¦å·²åˆ›å»º
    if hasattr(model1, 'transfer_layer'):
        transfer_layer_count = len(model1.transfer_layer)
        print(f"   transfer_layer æ•°é‡: {transfer_layer_count}")
        if transfer_layer_count > 0:
            print(f"   transfer_layer é€šé“: {list(model1.transfer_layer.keys())}")
        else:
            print(f"   âš ï¸ transfer_layer å°šæœªåˆ›å»ºï¼ˆå°†åœ¨ forward æ—¶åŠ¨æ€åˆ›å»ºï¼‰")
    
    # æ£€æŸ¥ alignment_layers æ˜¯å¦å·²åˆ›å»º
    if hasattr(model1, 'alignment_layers'):
        alignment_layers_count = len(model1.alignment_layers)
        print(f"   alignment_layers æ•°é‡: {alignment_layers_count}")
        if alignment_layers_count > 0:
            print(f"   alignment_layers é€šé“: {list(model1.alignment_layers.keys())}")
    
    # æ£€æŸ¥ TCPClassifier å’Œ TCPConfidenceLayer æ˜¯å¦å·²åˆ›å»º
    if hasattr(model1, 'TCPClassifier'):
        tcp_classifier_count = len(model1.TCPClassifier)
        print(f"   TCPClassifier æ•°é‡: {tcp_classifier_count}")
    if hasattr(model1, 'TCPConfidenceLayer'):
        tcp_confidence_count = len(model1.TCPConfidenceLayer)
        print(f"   TCPConfidenceLayer æ•°é‡: {tcp_confidence_count}")
    
    # åˆ—å‡ºæ‰€æœ‰å‚æ•°é”®ï¼ˆæŒ‰ç±»åˆ«åˆ†ç»„ï¼‰
    print(f"\n   ğŸ“‹ å‚æ•°é”®åˆ†ç±»ç»Ÿè®¡:")
    param_keys_by_type = {
        'attention_net': [],  # CLAMæ³¨æ„åŠ›ç½‘ç»œ
        'classifiers': [],  # CLAMåˆ†ç±»å™¨
        'instance_classifiers': [],  # CLAMå®ä¾‹åˆ†ç±»å™¨
        'transfer_layer': [],  # Transferå±‚
        'alignment_layers': [],  # SVDå¯¹é½å±‚
        'TCPClassifier': [],  # åŠ¨æ€é—¨æ§åˆ†ç±»å™¨
        'TCPConfidenceLayer': [],  # åŠ¨æ€é—¨æ§ç½®ä¿¡åº¦å±‚
        'fusion_prediction': [],  # èåˆé¢„æµ‹å±‚
        'other': []
    }
    
    for key in state_dict1.keys():
        if 'attention_net' in key:
            param_keys_by_type['attention_net'].append(key)
        elif 'classifiers' in key and 'instance' not in key:
            param_keys_by_type['classifiers'].append(key)
        elif 'instance_classifiers' in key:
            param_keys_by_type['instance_classifiers'].append(key)
        elif 'transfer_layer' in key:
            param_keys_by_type['transfer_layer'].append(key)
        elif 'alignment_layers' in key:
            param_keys_by_type['alignment_layers'].append(key)
        elif 'TCPClassifier' in key:
            param_keys_by_type['TCPClassifier'].append(key)
        elif 'TCPConfidenceLayer' in key:
            param_keys_by_type['TCPConfidenceLayer'].append(key)
        elif 'fusion_prediction' in key or 'fusion' in key.lower():
            param_keys_by_type['fusion_prediction'].append(key)
        else:
            param_keys_by_type['other'].append(key)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‚æ•°é‡
    for param_type, keys in param_keys_by_type.items():
        if keys:
            # è®¡ç®—è¯¥ç±»åˆ«çš„æ€»å‚æ•°é‡
            type_params = sum(state_dict1[key].numel() for key in keys)
            print(f"     {param_type}: {len(keys)} ä¸ªå‚æ•°é”®, {type_params:,} ä¸ªå‚æ•°")
            if len(keys) <= 5:
                for key in keys:
                    param_size = state_dict1[key].numel()
                    print(f"       - {key} ({param_size:,} å‚æ•°)")
            else:
                for key in keys[:3]:
                    param_size = state_dict1[key].numel()
                    print(f"       - {key} ({param_size:,} å‚æ•°)")
                print(f"       ... è¿˜æœ‰ {len(keys) - 3} ä¸ªå‚æ•°é”®")
    
    # ç»Ÿè®¡å„å±‚çš„å‚æ•°é‡
    print(f"\n   ğŸ“Š å„å±‚å‚æ•°é‡ç»Ÿè®¡:")
    layer_stats = {}
    
    # CLAMå±‚å‚æ•°
    clam_params = sum(state_dict1[key].numel() for key in param_keys_by_type['attention_net'] + 
                     param_keys_by_type['classifiers'] + param_keys_by_type['instance_classifiers'])
    if clam_params > 0:
        layer_stats['CLAMå±‚'] = clam_params
        print(f"     CLAMå±‚: {clam_params:,} ä¸ªå‚æ•°")
        print(f"       - attention_net: {sum(state_dict1[key].numel() for key in param_keys_by_type['attention_net']):,}")
        print(f"       - classifiers: {sum(state_dict1[key].numel() for key in param_keys_by_type['classifiers']):,}")
        print(f"       - instance_classifiers: {sum(state_dict1[key].numel() for key in param_keys_by_type['instance_classifiers']):,}")
    
    # Transferå±‚å‚æ•°
    transfer_params = sum(state_dict1[key].numel() for key in param_keys_by_type['transfer_layer'])
    if transfer_params > 0:
        layer_stats['Transferå±‚'] = transfer_params
        print(f"     Transferå±‚: {transfer_params:,} ä¸ªå‚æ•°")
    
    # SVDå¯¹é½å±‚å‚æ•°
    svd_params = sum(state_dict1[key].numel() for key in param_keys_by_type['alignment_layers'])
    if svd_params > 0:
        layer_stats['SVDå¯¹é½å±‚'] = svd_params
        print(f"     SVDå¯¹é½å±‚: {svd_params:,} ä¸ªå‚æ•°")
    
    # åŠ¨æ€é—¨æ§å±‚å‚æ•°
    gate_params = sum(state_dict1[key].numel() for key in param_keys_by_type['TCPClassifier'] + 
                     param_keys_by_type['TCPConfidenceLayer'])
    if gate_params > 0:
        layer_stats['åŠ¨æ€é—¨æ§å±‚'] = gate_params
        print(f"     åŠ¨æ€é—¨æ§å±‚: {gate_params:,} ä¸ªå‚æ•°")
        print(f"       - TCPClassifier: {sum(state_dict1[key].numel() for key in param_keys_by_type['TCPClassifier']):,}")
        print(f"       - TCPConfidenceLayer: {sum(state_dict1[key].numel() for key in param_keys_by_type['TCPConfidenceLayer']):,}")
    
    # èåˆé¢„æµ‹å±‚å‚æ•°
    fusion_params = sum(state_dict1[key].numel() for key in param_keys_by_type['fusion_prediction'])
    if fusion_params > 0:
        layer_stats['èåˆé¢„æµ‹å±‚'] = fusion_params
        print(f"     èåˆé¢„æµ‹å±‚: {fusion_params:,} ä¸ªå‚æ•°")
    
    # å…¶ä»–å‚æ•°
    other_params = sum(state_dict1[key].numel() for key in param_keys_by_type['other'])
    if other_params > 0:
        layer_stats['å…¶ä»–'] = other_params
        print(f"     å…¶ä»–: {other_params:,} ä¸ªå‚æ•°")
        for key in param_keys_by_type['other'][:5]:
            print(f"       - {key}")
    
    # éªŒè¯æ€»å‚æ•°é‡
    calculated_total = sum(layer_stats.values())
    print(f"\n     âœ… è®¡ç®—çš„æ€»å‚æ•°é‡: {calculated_total:,}")
    print(f"     âœ… å®é™…çš„æ€»å‚æ•°é‡: {total_params:,}")
    if calculated_total == total_params:
        print(f"     âœ… å‚æ•°é‡ç»Ÿè®¡ä¸€è‡´")
    else:
        print(f"     âš ï¸ å‚æ•°é‡ç»Ÿè®¡ä¸ä¸€è‡´ï¼Œå·®å¼‚: {abs(calculated_total - total_params):,}")
    
    # è·å–ç¬¬ä¸€ä¸ªå‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºç¤ºä¾‹
    first_key = list(state_dict1.keys())[0]
    first_param = state_dict1[first_key]
    print(f"\n   ç¤ºä¾‹å‚æ•° ({first_key}):")
    print(f"     shape: {first_param.shape}")
    print(f"     mean: {first_param.mean().item():.6f}")
    print(f"     std: {first_param.std().item():.6f}")
    print(f"     min: {first_param.min().item():.6f}")
    print(f"     max: {first_param.max().item():.6f}")
    
    # ========== ä¿å­˜æ¨¡å‹ ==========
    print(f"\n{'='*60}")
    print("ä¿å­˜æ¨¡å‹")
    print(f"{'='*60}")
    
    checkpoint_path = results_dir / f"test_init_consistency_checkpoint.pt"
    torch.save(model1.state_dict(), checkpoint_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {checkpoint_path}")
    
    # ========== ç­‰å¾…æŒ‡å®šç§’æ•° ==========
    print(f"\n{'='*60}")
    print(f"ç­‰å¾… {wait_seconds} ç§’...")
    print(f"{'='*60}")
    
    for i in range(wait_seconds, 0, -1):
        print(f"   å€’è®¡æ—¶: {i} ç§’", end='\r')
        time.sleep(1)
    print(f"   âœ… ç­‰å¾…å®Œæˆ")
    
    # ========== åŠ è½½æ¨¡å‹ ==========
    print(f"\n{'='*60}")
    print("åŠ è½½æ¨¡å‹")
    print(f"{'='*60}")
    
    model2_loaded = trainer._init_model()
    state_dict_loaded = torch.load(checkpoint_path, map_location=torch.device('cpu'))
    
    # å¤„ç†åŠ¨æ€åˆ›å»ºçš„transfer_layerï¼ˆå¦‚æœéœ€è¦ï¼‰
    if hasattr(model2_loaded, 'transfer_layer') and hasattr(model2_loaded, 'create_transfer_layer'):
        transfer_layer_channels = {}
        for key in state_dict_loaded.keys():
            if 'transfer_layer.' in key:
                parts = key.split('.')
                if len(parts) >= 3:
                    channel_name = parts[1]
                    weight_type = parts[2]
                    
                    if channel_name not in transfer_layer_channels:
                        transfer_layer_channels[channel_name] = {}
                    transfer_layer_channels[channel_name][weight_type] = state_dict_loaded[key]
        
        if hasattr(model2_loaded, 'output_dim'):
            output_dim = model2_loaded.output_dim
            for channel_name, weights in transfer_layer_channels.items():
                if channel_name not in model2_loaded.transfer_layer:
                    if 'weight' in weights:
                        weight_tensor = weights['weight']
                        if len(weight_tensor.shape) == 2:
                            input_dim = weight_tensor.shape[1]
                            transfer_layer = model2_loaded.create_transfer_layer(input_dim)
                            model2_loaded.transfer_layer[channel_name] = transfer_layer
    
    model2_loaded.load_state_dict(state_dict_loaded, strict=False)
    state_dict2_loaded = model2_loaded.state_dict()
    
    # ç»Ÿè®¡å®é™…å‚æ•°æ•°é‡
    total_params_2 = sum(p.numel() for p in model2_loaded.parameters())
    trainable_params_2 = sum(p.numel() for p in model2_loaded.parameters() if p.requires_grad)
    state_dict_keys_2 = len(state_dict2_loaded)
    
    print(f"âœ… æ¨¡å‹2ï¼ˆåŠ è½½ï¼‰åˆå§‹åŒ–å®Œæˆ")
    print(f"   state_dict é”®æ•°é‡: {state_dict_keys_2}")
    print(f"   æ€»å‚æ•°æ•°é‡: {total_params_2:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params_2:,}")
    
    # è·å–ç¬¬ä¸€ä¸ªå‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºç¤ºä¾‹
    first_key2_loaded = list(state_dict2_loaded.keys())[0]
    first_param2_loaded = state_dict2_loaded[first_key2_loaded]
    print(f"   ç¤ºä¾‹å‚æ•° ({first_key2_loaded}):")
    print(f"     shape: {first_param2_loaded.shape}")
    print(f"     mean: {first_param2_loaded.mean().item():.6f}")
    print(f"     std: {first_param2_loaded.std().item():.6f}")
    print(f"     min: {first_param2_loaded.min().item():.6f}")
    print(f"     max: {first_param2_loaded.max().item():.6f}")
    
    # ========== ç¬¬ä¸‰æ¬¡åˆå§‹åŒ–æ¨¡å‹ï¼ˆç›¸åŒseedï¼‰ ==========
    print(f"\n{'='*60}")
    print("ç¬¬ä¸‰æ¬¡åˆå§‹åŒ–æ¨¡å‹ï¼ˆç›¸åŒseedï¼‰")
    print(f"{'='*60}")
    
    seed_torch(seed)
    model3 = trainer._init_model()
    state_dict3 = model3.state_dict()
    
    # ç»Ÿè®¡å®é™…å‚æ•°æ•°é‡
    total_params_3 = sum(p.numel() for p in model3.parameters())
    trainable_params_3 = sum(p.numel() for p in model3.parameters() if p.requires_grad)
    state_dict_keys_3 = len(state_dict3)
    
    print(f"âœ… æ¨¡å‹3åˆå§‹åŒ–å®Œæˆ")
    print(f"   state_dict é”®æ•°é‡: {state_dict_keys_3}")
    print(f"   æ€»å‚æ•°æ•°é‡: {total_params_3:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params_3:,}")
    
    # è·å–ç¬¬ä¸€ä¸ªå‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºç¤ºä¾‹
    first_key3 = list(state_dict3.keys())[0]
    first_param3 = state_dict3[first_key3]
    print(f"   ç¤ºä¾‹å‚æ•° ({first_key3}):")
    print(f"     shape: {first_param3.shape}")
    print(f"     mean: {first_param3.mean().item():.6f}")
    print(f"     std: {first_param3.std().item():.6f}")
    print(f"     min: {first_param3.min().item():.6f}")
    print(f"     max: {first_param3.max().item():.6f}")
    
    # ========== æ¯”è¾ƒä¸‰ä¸ªæ¨¡å‹çš„æƒé‡ ==========
    print(f"\n{'='*60}")
    print("æ¯”è¾ƒä¸‰ä¸ªæ¨¡å‹çš„æƒé‡")
    print(f"{'='*60}")
    
    # æ¯”è¾ƒ æ¨¡å‹1 vs æ¨¡å‹2ï¼ˆåŠ è½½ï¼‰
    print(f"\nğŸ“Š æ¯”è¾ƒ æ¨¡å‹1 vs æ¨¡å‹2ï¼ˆåŠ è½½ï¼‰:")
    comparison_1_2 = compare_model_weights(model1, model2_loaded, tolerance=1e-6)
    print(f"\n   ğŸ“ˆ æ¯”è¾ƒç»“æœç»Ÿè®¡:")
    print(f"   å‚æ•°å¼ é‡ä¸ªæ•°: {comparison_1_2['total_param_tensors']}")
    print(f"   å‚æ•°å…ƒç´ æ€»æ•°: {comparison_1_2['total_param_elements']:,}")
    print(f"   åŒ¹é…çš„å…ƒç´ : {comparison_1_2['matching_elements']:,} ({100*comparison_1_2['matching_elements']/comparison_1_2['total_param_elements']:.2f}%)")
    print(f"   ä¸åŒçš„å…ƒç´ : {comparison_1_2['different_elements']:,} ({100*comparison_1_2['different_elements']/comparison_1_2['total_param_elements']:.2f}%)")
    print(f"   å®Œå…¨åŒ¹é…çš„å¼ é‡: {comparison_1_2['matching_param_tensors']}")
    print(f"   æœ‰å·®å¼‚çš„å¼ é‡: {comparison_1_2['different_param_tensors']}")
    print(f"   å…¨å±€æœ€å¤§ç»å¯¹å·®: {comparison_1_2['global_max_abs_diff']:.6e}")
    print(f"   å…¨å±€å¹³å‡ç»å¯¹å·®: {comparison_1_2['global_mean_abs_diff']:.6e}")
    
    if comparison_1_2['different_keys']:
        print(f"\n   âš ï¸ æœ‰å·®å¼‚çš„å‚æ•°å¼ é‡ ({len(comparison_1_2['different_keys'])} ä¸ª):")
        for diff_info in comparison_1_2['different_keys'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            if isinstance(diff_info, dict):
                print(f"     - {diff_info['key']}:")
                print(f"       shape: {diff_info['shape']}, numel: {diff_info['numel']:,}")
                print(f"       åŒ¹é…å…ƒç´ : {diff_info['matching_elements']:,}, ä¸åŒå…ƒç´ : {diff_info['different_elements']:,}")
                print(f"       max_diff: {diff_info['max_diff']:.6e}, mean_diff: {diff_info['mean_diff']:.6e}")
            else:
                print(f"     - {diff_info}")
        if len(comparison_1_2['different_keys']) > 10:
            print(f"     ... è¿˜æœ‰ {len(comparison_1_2['different_keys']) - 10} ä¸ªæœ‰å·®å¼‚çš„å¼ é‡")
    
    # æ¯”è¾ƒ æ¨¡å‹1 vs æ¨¡å‹3ï¼ˆç¬¬ä¸‰æ¬¡åˆå§‹åŒ–ï¼‰
    print(f"\nğŸ“Š æ¯”è¾ƒ æ¨¡å‹1 vs æ¨¡å‹3ï¼ˆç¬¬ä¸‰æ¬¡åˆå§‹åŒ–ï¼‰:")
    comparison_1_3 = compare_model_weights(model1, model3, tolerance=1e-6)
    print(f"\n   ğŸ“ˆ æ¯”è¾ƒç»“æœç»Ÿè®¡:")
    print(f"   å‚æ•°å¼ é‡ä¸ªæ•°: {comparison_1_3['total_param_tensors']}")
    print(f"   å‚æ•°å…ƒç´ æ€»æ•°: {comparison_1_3['total_param_elements']:,}")
    print(f"   åŒ¹é…çš„å…ƒç´ : {comparison_1_3['matching_elements']:,} ({100*comparison_1_3['matching_elements']/comparison_1_3['total_param_elements']:.2f}%)")
    print(f"   ä¸åŒçš„å…ƒç´ : {comparison_1_3['different_elements']:,} ({100*comparison_1_3['different_elements']/comparison_1_3['total_param_elements']:.2f}%)")
    print(f"   å®Œå…¨åŒ¹é…çš„å¼ é‡: {comparison_1_3['matching_param_tensors']}")
    print(f"   æœ‰å·®å¼‚çš„å¼ é‡: {comparison_1_3['different_param_tensors']}")
    print(f"   å…¨å±€æœ€å¤§ç»å¯¹å·®: {comparison_1_3['global_max_abs_diff']:.6e}")
    print(f"   å…¨å±€å¹³å‡ç»å¯¹å·®: {comparison_1_3['global_mean_abs_diff']:.6e}")
    
    if comparison_1_3['different_keys']:
        print(f"\n   âš ï¸ æœ‰å·®å¼‚çš„å‚æ•°å¼ é‡ ({len(comparison_1_3['different_keys'])} ä¸ª):")
        for diff_info in comparison_1_3['different_keys'][:10]:
            if isinstance(diff_info, dict):
                print(f"     - {diff_info['key']}:")
                print(f"       shape: {diff_info['shape']}, numel: {diff_info['numel']:,}")
                print(f"       åŒ¹é…å…ƒç´ : {diff_info['matching_elements']:,}, ä¸åŒå…ƒç´ : {diff_info['different_elements']:,}")
                print(f"       max_diff: {diff_info['max_diff']:.6e}, mean_diff: {diff_info['mean_diff']:.6e}")
            else:
                print(f"     - {diff_info}")
        if len(comparison_1_3['different_keys']) > 10:
            print(f"     ... è¿˜æœ‰ {len(comparison_1_3['different_keys']) - 10} ä¸ªæœ‰å·®å¼‚çš„å¼ é‡")
    
    # æ¯”è¾ƒ æ¨¡å‹2ï¼ˆåŠ è½½ï¼‰vs æ¨¡å‹3ï¼ˆç¬¬ä¸‰æ¬¡åˆå§‹åŒ–ï¼‰
    print(f"\nğŸ“Š æ¯”è¾ƒ æ¨¡å‹2ï¼ˆåŠ è½½ï¼‰vs æ¨¡å‹3ï¼ˆç¬¬ä¸‰æ¬¡åˆå§‹åŒ–ï¼‰:")
    comparison_2_3 = compare_model_weights(model2_loaded, model3, tolerance=1e-6)
    print(f"\n   ğŸ“ˆ æ¯”è¾ƒç»“æœç»Ÿè®¡:")
    print(f"   å‚æ•°å¼ é‡ä¸ªæ•°: {comparison_2_3['total_param_tensors']}")
    print(f"   å‚æ•°å…ƒç´ æ€»æ•°: {comparison_2_3['total_param_elements']:,}")
    print(f"   åŒ¹é…çš„å…ƒç´ : {comparison_2_3['matching_elements']:,} ({100*comparison_2_3['matching_elements']/comparison_2_3['total_param_elements']:.2f}%)")
    print(f"   ä¸åŒçš„å…ƒç´ : {comparison_2_3['different_elements']:,} ({100*comparison_2_3['different_elements']/comparison_2_3['total_param_elements']:.2f}%)")
    print(f"   å®Œå…¨åŒ¹é…çš„å¼ é‡: {comparison_2_3['matching_param_tensors']}")
    print(f"   æœ‰å·®å¼‚çš„å¼ é‡: {comparison_2_3['different_param_tensors']}")
    print(f"   å…¨å±€æœ€å¤§ç»å¯¹å·®: {comparison_2_3['global_max_abs_diff']:.6e}")
    print(f"   å…¨å±€å¹³å‡ç»å¯¹å·®: {comparison_2_3['global_mean_abs_diff']:.6e}")
    
    if comparison_2_3['different_keys']:
        print(f"\n   âš ï¸ æœ‰å·®å¼‚çš„å‚æ•°å¼ é‡ ({len(comparison_2_3['different_keys'])} ä¸ª):")
        for diff_info in comparison_2_3['different_keys'][:10]:
            if isinstance(diff_info, dict):
                print(f"     - {diff_info['key']}:")
                print(f"       shape: {diff_info['shape']}, numel: {diff_info['numel']:,}")
                print(f"       åŒ¹é…å…ƒç´ : {diff_info['matching_elements']:,}, ä¸åŒå…ƒç´ : {diff_info['different_elements']:,}")
                print(f"       max_diff: {diff_info['max_diff']:.6e}, mean_diff: {diff_info['mean_diff']:.6e}")
            else:
                print(f"     - {diff_info}")
        if len(comparison_2_3['different_keys']) > 10:
            print(f"     ... è¿˜æœ‰ {len(comparison_2_3['different_keys']) - 10} ä¸ªæœ‰å·®å¼‚çš„å¼ é‡")
    
    # åˆ¤æ–­æ˜¯å¦ä¸€è‡´ï¼ˆåŸºäºå…ƒç´ çº§æ¯”è¾ƒï¼‰
    is_consistent_1_2 = (
        comparison_1_2['different_elements'] == 0 and
        len(comparison_1_2['missing_keys_1']) == 0 and
        len(comparison_1_2['missing_keys_2']) == 0 and
        comparison_1_2['global_max_abs_diff'] < 1e-6
    )
    
    is_consistent_1_3 = (
        comparison_1_3['different_elements'] == 0 and
        len(comparison_1_3['missing_keys_1']) == 0 and
        len(comparison_1_3['missing_keys_2']) == 0 and
        comparison_1_3['global_max_abs_diff'] < 1e-6
    )
    
    is_consistent_2_3 = (
        comparison_2_3['different_elements'] == 0 and
        len(comparison_2_3['missing_keys_1']) == 0 and
        len(comparison_2_3['missing_keys_2']) == 0 and
        comparison_2_3['global_max_abs_diff'] < 1e-6
    )
    
    all_consistent = is_consistent_1_2 and is_consistent_1_3 and is_consistent_2_3
    
    print(f"\n{'='*60}")
    print("æœ€ç»ˆç»“è®º")
    print(f"{'='*60}")
    print(f"æ¨¡å‹1 vs æ¨¡å‹2ï¼ˆåŠ è½½ï¼‰: {'âœ… ä¸€è‡´' if is_consistent_1_2 else 'âŒ ä¸ä¸€è‡´'}")
    print(f"æ¨¡å‹1 vs æ¨¡å‹3ï¼ˆç¬¬ä¸‰æ¬¡åˆå§‹åŒ–ï¼‰: {'âœ… ä¸€è‡´' if is_consistent_1_3 else 'âŒ ä¸ä¸€è‡´'}")
    print(f"æ¨¡å‹2ï¼ˆåŠ è½½ï¼‰vs æ¨¡å‹3ï¼ˆç¬¬ä¸‰æ¬¡åˆå§‹åŒ–ï¼‰: {'âœ… ä¸€è‡´' if is_consistent_2_3 else 'âŒ ä¸ä¸€è‡´'}")
    
    if all_consistent:
        print(f"\nâœ… ç»“è®º: ä¸‰æ¬¡æ¨¡å‹æƒé‡å®Œå…¨ä¸€è‡´ï¼")
        print(f"   - åˆå§‹åŒ– -> ä¿å­˜ -> åŠ è½½: ä¸€è‡´")
        print(f"   - åˆå§‹åŒ– -> å†æ¬¡åˆå§‹åŒ–: ä¸€è‡´")
        print(f"   - åŠ è½½ -> å†æ¬¡åˆå§‹åŒ–: ä¸€è‡´")
    else:
        print(f"\nâŒ ç»“è®º: ä¸‰æ¬¡æ¨¡å‹æƒé‡ä¸ä¸€è‡´ï¼")
        if not is_consistent_1_2:
            print(f"   âš ï¸ ä¿å­˜/åŠ è½½è¿‡ç¨‹å¯èƒ½å¼•å…¥äº†å·®å¼‚")
        if not is_consistent_1_3:
            print(f"   âš ï¸ å¤šæ¬¡åˆå§‹åŒ–å¯èƒ½äº§ç”Ÿäº†ä¸åŒçš„æƒé‡")
        if not is_consistent_2_3:
            print(f"   âš ï¸ åŠ è½½çš„æ¨¡å‹ä¸é‡æ–°åˆå§‹åŒ–çš„æ¨¡å‹ä¸ä¸€è‡´")
        print(f"\n   å¯èƒ½çš„åŸå› :")
        print(f"   1. æ¨¡å‹åˆå§‹åŒ–è¿‡ç¨‹ä¸­ä½¿ç”¨äº†éç¡®å®šæ€§æ“ä½œ")
        print(f"   2. æŸäº›å±‚ä½¿ç”¨äº†éšæœºåˆå§‹åŒ–ä½†æœªè®¾ç½®seed")
        print(f"   3. åŠ¨æ€åˆ›å»ºçš„å±‚ï¼ˆå¦‚transfer_layerï¼‰å¯èƒ½åœ¨ä¸åŒæ—¶æœºåˆ›å»º")
        print(f"   4. ä¿å­˜/åŠ è½½è¿‡ç¨‹ä¸­å¯èƒ½ä¸¢å¤±äº†æŸäº›çŠ¶æ€")
    print(f"{'='*60}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if checkpoint_path.exists():
        checkpoint_path.unlink()
        print(f"\nğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {checkpoint_path}")
    
    return {
        'comparison_1_2': comparison_1_2,
        'comparison_1_3': comparison_1_3,
        'comparison_2_3': comparison_2_3,
        'is_consistent_1_2': is_consistent_1_2,
        'is_consistent_1_3': is_consistent_1_3,
        'is_consistent_2_3': is_consistent_2_3,
        'all_consistent': all_consistent
    }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–ä¸€è‡´æ€§ï¼ˆåŒ…æ‹¬ä¿å­˜ã€åŠ è½½ã€å¤šæ¬¡åˆå§‹åŒ–ï¼‰')
    parser.add_argument('--results_dir', type=str, required=True,
                        help='ç»“æœç›®å½•è·¯å¾„')
    parser.add_argument('--fold_idx', type=int, default=0,
                        help='foldç´¢å¼• (default: 0)')
    parser.add_argument('--wait_seconds', type=int, default=10,
                        help='ä¿å­˜åç­‰å¾…çš„ç§’æ•° (default: 10)')
    
    args = parser.parse_args()
    
    try:
        results = test_model_init_consistency(
            args.results_dir,
            args.fold_idx,
            args.wait_seconds
        )
        
        if not results['all_consistent']:
            sys.exit(1)
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

