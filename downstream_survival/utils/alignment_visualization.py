#!/usr/bin/env python3
"""
SVD å¯¹é½å‰åç‰¹å¾ä¿å­˜å·¥å…·

ç”¨æ³•ç¤ºä¾‹ï¼š
python -m downstream_survival.utils.alignment_visualization \
  --results_dir /path/to/results \
  --fold_idx 0 \
  --save_dir /path/to/save/features

è¯´æ˜ï¼š
- è¯¥è„šæœ¬ä¼šæ ¹æ®é…ç½®æ„å»ºæ•°æ®é›†ä¸æ¨¡å‹ï¼ˆç±»ä¼¼ robust_on_missing_modality.pyï¼‰ï¼Œ
  åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œå‰å‘æ¨ç†ï¼Œæ”¶é›†å„æ¨¡æ€åœ¨ SVD å¯¹é½å‰åçš„ç‰¹å¾ï¼Œ
  ä¿å­˜ä¸º numpy æ–‡ä»¶ç”¨äºåç»­å¯¹æ¯”åˆ†æã€‚
"""

from __future__ import annotations

import os
import json
import argparse
import sys
from typing import Dict, List, Tuple, Any, Optional

import torch
import numpy as np
from torch.utils.data import Subset, DataLoader

# é¡¹ç›®æ ¹ç›®å½•
ROOT_DIR = '/home/zheng/zheng/multimodal-fusion/downstream_survival'
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

from datasets.multimodal_dataset import MultimodalDataset
from trainer import Trainer
from main import parse_channels, create_k_fold_splits


def _ensure_dir(path: str) -> None:
    """å¦‚æœç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»ºã€‚"""
    if path and not os.path.isdir(path):
        os.makedirs(path, exist_ok=True)


def _load_configs_from_results_dir(results_dir: str) -> Dict[str, Any]:
    """
    ä»ç»“æœç›®å½•åŠ è½½é…ç½®JSONï¼ˆåŒ¹é…ç¬¬ä¸€ä¸ª configs_*.json æˆ– configs_*.JSONï¼‰ã€‚
    
    Returns:
        dict: é…ç½®å­—å…¸ï¼ŒåŒ…å« experiment_config ä¸ model_configã€‚
    """
    candidates = []
    for name in os.listdir(results_dir):
        if name.startswith('configs_') and name.lower().endswith('.json'):
            candidates.append(os.path.join(results_dir, name))
    if not candidates:
        raise FileNotFoundError(f'æœªåœ¨ç›®å½•æ‰¾åˆ°é…ç½®æ–‡ä»¶: {results_dir}')

    cfg_path = sorted(candidates)[0]
    with open(cfg_path, 'r') as f:
        return json.load(f)


def _collect_features_from_testset(
    trainer: Trainer,
    test_dataset: Subset,
    fold_idx: int,
    checkpoint_path: str,
    device: torch.device,
) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œæ¨¡å‹ï¼Œæ”¶é›† SVD å¯¹é½å‰åçš„ç‰¹å¾ã€‚
    
    Args:
        trainer: è®­ç»ƒå™¨å®ä¾‹
        test_dataset: æµ‹è¯•é›†
        fold_idx: foldç´¢å¼•
        checkpoint_path: checkpointè·¯å¾„
        device: è®¾å¤‡
        
    Returns:
        (original_features, aligned_features)
        ä¸¤è€…å­—å…¸é”®å‡ä¸ºæ¨¡æ€åï¼Œå€¼ä¸ºè¯¥æ¨¡æ€ä¸‹æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾æ•°ç»„ [N, D]ã€‚
    """
    # åŠ è½½æ¨¡å‹
    model = trainer._init_model()
    model.eval()
    
    # åŠ è½½checkpoint
    state = torch.load(checkpoint_path, map_location=device)
    
    # å¤„ç† transfer_layerï¼ˆç±»ä¼¼ evaluate_foldï¼‰
    if hasattr(model, 'transfer_layer') and hasattr(model, 'create_transfer_layer'):
        transfer_layer_channels = {}
        for key in state.keys():
            if 'transfer_layer.' in key:
                parts = key.split('.')
                if len(parts) >= 3:
                    channel_name = parts[1]
                    weight_type = parts[2]
                    if channel_name not in transfer_layer_channels:
                        transfer_layer_channels[channel_name] = {}
                    transfer_layer_channels[channel_name][weight_type] = state[key]
        
        if hasattr(model, 'output_dim'):
            output_dim = model.output_dim
            for channel_name, weights in transfer_layer_channels.items():
                if channel_name not in model.transfer_layer:
                    if 'weight' in weights:
                        weight_tensor = weights['weight']
                        if len(weight_tensor.shape) == 2:
                            input_dim = weight_tensor.shape[1]
                            transfer_layer = model.create_transfer_layer(input_dim)
                            model.transfer_layer[channel_name] = transfer_layer
    
    # åŠ è½½æƒé‡
    try:
        model.load_state_dict(state, strict=True)
    except RuntimeError:
        model.load_state_dict(state, strict=False)

    
    # åˆ›å»º DataLoaderï¼ˆbatch_size=1ï¼Œå› ä¸ºéœ€è¦é€ä¸ªå¤„ç†æ ·æœ¬ï¼‰
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
    
    # æ”¶é›†ç‰¹å¾
    original_features: Dict[str, List[np.ndarray]] = {}
    aligned_features: Dict[str, List[np.ndarray]] = {}
    
    model.eval()
    with torch.no_grad():
        for batch_idx, (input_data, label) in enumerate(test_loader):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            if isinstance(input_data, dict):
                input_data = {k: (v.to(device) if torch.is_tensor(v) else v) for k, v in input_data.items()}
            label = label.to(device)
            
            # å‰å‘ä¼ æ’­
            out = model(input_data, label)
            
            # æå– original_features_dict å’Œ aligned_svd_features_dict
            if isinstance(out, dict):
                if 'original_features_dict' in out:
                    for key, tensor in out['original_features_dict'].items():
                        arr = tensor.detach().float().cpu().numpy()
                        # batch_size=1ï¼Œæ‰€ä»¥æ˜¯ [1, D]ï¼Œå»æ‰ batch ç»´åº¦å¾—åˆ° [D]
                        if arr.ndim == 2 and arr.shape[0] == 1:
                            arr = arr[0]
                        original_features.setdefault(key, []).append(arr)
                
                if 'aligned_svd_features_dict' in out:
                    for key, tensor in out['aligned_svd_features_dict'].items():
                        arr = tensor.detach().float().cpu().numpy()
                        # batch_size=1ï¼Œæ‰€ä»¥æ˜¯ [1, D]ï¼Œå»æ‰ batch ç»´åº¦å¾—åˆ° [D]
                        if arr.ndim == 2 and arr.shape[0] == 1:
                            arr = arr[0]
                        aligned_features.setdefault(key, []).append(arr)
    
    # å°†åˆ—è¡¨è½¬æ¢ä¸º numpy æ•°ç»„
    original_features_stacked = {}
    aligned_features_stacked = {}
    
    for key, arrays in original_features.items():
        # ç»Ÿä¸€å¤„ç†ï¼šå¦‚æœæ¯ä¸ªæ•°ç»„æ˜¯ [D]ï¼Œåˆ™å †å ä¸º [N, D]
        if all(arr.ndim == 1 for arr in arrays):
            original_features_stacked[key] = np.stack(arrays, axis=0)
        else:
            # å¦‚æœå½¢çŠ¶ä¸ä¸€è‡´ï¼Œå°è¯•æ‹¼æ¥
            original_features_stacked[key] = np.concatenate(arrays, axis=0)
    
    for key, arrays in aligned_features.items():
        if all(arr.ndim == 1 for arr in arrays):
            aligned_features_stacked[key] = np.stack(arrays, axis=0)
        else:
            aligned_features_stacked[key] = np.concatenate(arrays, axis=0)
    
    return original_features_stacked, aligned_features_stacked


def _save_features(
    original_features: Dict[str, np.ndarray],
    aligned_features: Dict[str, np.ndarray],
    save_dir: str,
    fold_idx: int,
) -> None:
    """
    ä¿å­˜ SVD å¯¹é½å‰åçš„ç‰¹å¾åˆ°æ–‡ä»¶ã€‚
    
    Args:
        original_features: SVD å¯¹é½å‰çš„ç‰¹å¾å­—å…¸
        aligned_features: SVD å¯¹é½åçš„ç‰¹å¾å­—å…¸
        save_dir: ä¿å­˜ç›®å½•
        fold_idx: foldç´¢å¼•
    """
    _ensure_dir(save_dir)
    
    # ä¿å­˜æ¯ä¸ªæ¨¡æ€çš„ç‰¹å¾
    for modality in sorted(set(list(original_features.keys()) + list(aligned_features.keys()))):
        safe_name = modality.replace('/', '_').replace('=', '_')
        
        if modality in original_features:
            original_path = os.path.join(save_dir, f'fold_{fold_idx}_{safe_name}_original.npy')
            np.save(original_path, original_features[modality])
            print(f'  âœ… ä¿å­˜åŸå§‹ç‰¹å¾: {original_path} (shape: {original_features[modality].shape})')
        
        if modality in aligned_features:
            aligned_path = os.path.join(save_dir, f'fold_{fold_idx}_{safe_name}_aligned.npy')
            np.save(aligned_path, aligned_features[modality])
            print(f'  âœ… ä¿å­˜å¯¹é½ç‰¹å¾: {aligned_path} (shape: {aligned_features[modality].shape})')
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        'fold_idx': fold_idx,
        'modalities': sorted(set(list(original_features.keys()) + list(aligned_features.keys()))),
        'original_features_shapes': {k: list(v.shape) for k, v in original_features.items()},
        'aligned_features_shapes': {k: list(v.shape) for k, v in aligned_features.items()},
    }
    metadata_path = os.path.join(save_dir, f'fold_{fold_idx}_metadata.json')
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f'  âœ… ä¿å­˜å…ƒæ•°æ®: {metadata_path}')


def main() -> None:
    parser = argparse.ArgumentParser(description='SVD å¯¹é½å‰åç‰¹å¾ä¿å­˜å·¥å…·')
    parser.add_argument('--results_dir', type=str, required=True, 
                       help='è®­ç»ƒç»“æœç›®å½•ï¼ˆåŒ…å« s_?_checkpoint.pt ä¸ configs_*.jsonï¼‰')
    parser.add_argument('--fold_idx', type=int, default=0, 
                       help='è¦å¤„ç†çš„ fold ç´¢å¼•ï¼ˆé»˜è®¤ 0ï¼‰')
    parser.add_argument('--save_dir', type=str, default=None,
                       help='ç‰¹å¾ä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨ results_dir/svd_features')
    parser.add_argument('--data_root_dir', type=str, default=None,
                       help='æ•°æ®æ ¹ç›®å½•ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤å‚æ•°ï¼Œå¦åˆ™å›é€€åˆ°configs')
    parser.add_argument('--csv_path', type=str, default=None,
                       help='CSVè·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤å‚æ•°ï¼Œå¦åˆ™å›é€€åˆ°configs')
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 1) åŠ è½½é…ç½®
    configs = _load_configs_from_results_dir(args.results_dir)
    exp_cfg = configs.get('experiment_config', {})
    model_cfg = configs.get('model_config', {})

    # 2) æ„å»ºæ•°æ®é›†
    base_target_channels = exp_cfg.get('target_channels') or model_cfg.get('channels_used_in_model')
    if not base_target_channels:
        base_target_channels = parse_channels([])
    
    dataset = MultimodalDataset(
        csv_path=args.csv_path or exp_cfg.get('csv_path'),
        data_root_dir=args.data_root_dir or exp_cfg.get('data_root_dir'),
        channels=base_target_channels,
        align_channels=exp_cfg.get('aligned_channels', None),
        alignment_model_path=exp_cfg.get('alignment_model_path', None),
        device=device,
    )
    
    print(f"ğŸ“Š æ•°æ®é›†æ„å»ºå®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")

    # 3) ç”Ÿæˆ K æŠ˜åˆ’åˆ†
    seed = exp_cfg.get('seed', 5678)
    k = exp_cfg.get('num_splits', 10)
    splits = create_k_fold_splits(dataset, k=k, seed=seed, fixed_test_split=None)
    
    if args.fold_idx >= len(splits):
        raise ValueError(f'Fold {args.fold_idx} è¶…å‡ºåˆ’åˆ†èŒƒå›´ï¼ˆå…± {len(splits)} ä¸ª foldï¼‰')
    
    split = splits[args.fold_idx]
    test_ds = Subset(dataset, split['test'])
    print(f"ğŸ“Š Fold {args.fold_idx} æµ‹è¯•é›†: {len(test_ds)} ä¸ªæ ·æœ¬")

    # 4) æ„å»ºè®­ç»ƒå™¨å¹¶åŠ è½½æ¨¡å‹
    trainer = Trainer(configs=configs, log_dir=os.path.join(args.results_dir, 'training_logs'))
    
    # 5) è·å– checkpoint
    checkpoint_path = os.path.join(args.results_dir, f's_{args.fold_idx}_checkpoint.pt')
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f'Checkpoint ä¸å­˜åœ¨: {checkpoint_path}')
    
    print(f"ğŸ“¦ åŠ è½½ checkpoint: {checkpoint_path}")

    # 6) æ”¶é›†ç‰¹å¾
    print(f"ğŸ”„ åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œæ¨¡å‹ï¼Œæ”¶é›†ç‰¹å¾...")
    original_features, aligned_features = _collect_features_from_testset(
        trainer=trainer,
        test_dataset=test_ds,
        fold_idx=args.fold_idx,
        checkpoint_path=checkpoint_path,
        device=device,
    )

    # 7) ä¿å­˜ç‰¹å¾
    save_dir = args.save_dir or os.path.join(args.results_dir, 'svd_features')
    _ensure_dir(save_dir)
    
    print(f"ğŸ’¾ ä¿å­˜ç‰¹å¾åˆ°: {save_dir}")
    _save_features(original_features, aligned_features, save_dir, args.fold_idx)

    print(f"âœ… å®Œæˆï¼ç‰¹å¾å·²ä¿å­˜åˆ°: {save_dir}")


if __name__ == '__main__':
    main()
