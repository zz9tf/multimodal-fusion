"""
æ¯”è¾ƒä¸¤ä¸ª DataLoader çš„ samples æ˜¯å¦ä¸€æ ·ï¼ˆé€šè¿‡ patient_id/case_idï¼‰
"""

import sys
import os
from typing import List
from torch.utils.data import DataLoader

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_dir = os.path.dirname(current_dir)
sys.path.append(project_dir)

from trainer import get_split_loader


def get_loader_case_ids(loader: DataLoader) -> List[str]:
    """
    ä» DataLoader ä¸­æå–æ‰€æœ‰ case_idï¼ˆpatient_idï¼‰
    
    Args:
        loader: æ•°æ®åŠ è½½å™¨
        
    Returns:
        case_ids: case_id åˆ—è¡¨ï¼ŒæŒ‰ç…§ DataLoader çš„è¿­ä»£é¡ºåº
    """
    dataset_ref = loader.dataset
    case_ids = []
    
    # éå† DataLoader è·å–å®é™…çš„ case_id é¡ºåº
    for batch_idx, (data, label) in enumerate(loader):
        # ä»æ•°æ®é›†ä¸­è·å– case_id
        if hasattr(dataset_ref, 'case_ids'):
            # ç›´æ¥æ•°æ®é›†ï¼ˆæ‹¥æœ‰ case_ids å±æ€§ï¼‰
            case_id = dataset_ref.case_ids[batch_idx]
        elif hasattr(dataset_ref, 'dataset') and hasattr(dataset_ref.dataset, 'case_ids') and hasattr(dataset_ref, 'indices'):
            # Subset æ•°æ®é›†ï¼ˆæ²¡æœ‰ case_ids å±æ€§ï¼Œéœ€ä»åŸæ•°æ®é›†æ˜ å°„ï¼‰
            # DataLoader ä¼šæŒ‰ç…§ dataset_ref.indices çš„é¡ºåºè¿­ä»£ï¼Œæ‰€ä»¥ batch_idx å¯¹åº” dataset_ref.indices[batch_idx]
            base = dataset_ref.dataset.case_ids
            base_list = list(base) if not isinstance(base, list) else base
            case_id = base_list[dataset_ref.indices[batch_idx]]
        else:
            # é™çº§ï¼šä½¿ç”¨ç´¢å¼•ä½œä¸º case_id
            case_id = f"sample_{batch_idx}"
        
        case_ids.append(case_id)
    
    return case_ids


def compare_loader_samples(loader1: DataLoader, loader2: DataLoader, name1: str = "Loader1", name2: str = "Loader2") -> bool:
    """
    æ¯”è¾ƒä¸¤ä¸ª DataLoader çš„ samples æ˜¯å¦ä¸€æ ·ï¼ˆé€šè¿‡ patient_id/case_idï¼‰
    
    Args:
        loader1: ç¬¬ä¸€ä¸ªæ•°æ®åŠ è½½å™¨
        loader2: ç¬¬äºŒä¸ªæ•°æ®åŠ è½½å™¨
        name1: ç¬¬ä¸€ä¸ªåŠ è½½å™¨çš„åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        name2: ç¬¬äºŒä¸ªåŠ è½½å™¨çš„åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
    Returns:
        is_same: å¦‚æœ samples ä¸€æ ·è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    case_ids1 = get_loader_case_ids(loader1)
    case_ids2 = get_loader_case_ids(loader2)
    
    # æ¯”è¾ƒé•¿åº¦
    if len(case_ids1) != len(case_ids2):
        print(f"âš ï¸ {name1} å’Œ {name2} çš„æ ·æœ¬æ•°é‡ä¸åŒ: {len(case_ids1)} vs {len(case_ids2)}")
        return False
    
    # æ¯”è¾ƒæ¯ä¸ªä½ç½®çš„ case_id
    differences = []
    for i, (cid1, cid2) in enumerate(zip(case_ids1, case_ids2)):
        if cid1 != cid2:
            differences.append((i, cid1, cid2))
    
    if differences:
        print(f"âš ï¸ {name1} å’Œ {name2} çš„æ ·æœ¬é¡ºåºä¸åŒ:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(case_ids1)}")
        print(f"   ä¸åŒä½ç½®æ•°: {len(differences)}")
        if len(differences) <= 10:
            print(f"   å‰ {len(differences)} ä¸ªä¸åŒä½ç½®:")
            for idx, cid1, cid2 in differences:
                print(f"     ä½ç½® {idx}: {name1}={cid1}, {name2}={cid2}")
        else:
            print(f"   å‰ 10 ä¸ªä¸åŒä½ç½®:")
            for idx, cid1, cid2 in differences[:10]:
                print(f"     ä½ç½® {idx}: {name1}={cid1}, {name2}={cid2}")
            print(f"   ... è¿˜æœ‰ {len(differences) - 10} ä¸ªä¸åŒä½ç½®")
        return False
    else:
        print(f"âœ… {name1} å’Œ {name2} çš„æ ·æœ¬é¡ºåºä¸€è‡´ (å…± {len(case_ids1)} ä¸ªæ ·æœ¬)")
        return True


if __name__ == "__main__":
    import argparse
    from datasets.multimodal_dataset import MultimodalDataset
    from main import create_k_fold_splits, parse_channels
    from torch.utils.data import Subset
    
    parser = argparse.ArgumentParser(description="æ¯”è¾ƒ DataLoader çš„ samples æ˜¯å¦ä¸€æ ·")
    parser.add_argument("--data_root_dir", type=str, required=True, help="æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--csv_path", type=str, required=True, help="CSV æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--channels", type=str, required=True, help="é€šé“åˆ—è¡¨ï¼Œç”¨ç©ºæ ¼åˆ†éš”")
    parser.add_argument("--fold_idx", type=int, default=0, help="Fold ç´¢å¼•")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--compare_splits", action="store_true", help="æ¯”è¾ƒ trainã€valã€test ä¹‹é—´çš„ samples æ˜¯å¦ä¸€è‡´")
    parser.add_argument("--compare_same_split", action="store_true", help="æ¯”è¾ƒåŒä¸€ä¸ª split çš„ä¸¤ä¸ª DataLoader æ˜¯å¦ä¸€è‡´")
    parser.add_argument("--split_type", type=str, choices=["train", "val", "test"], default="val", help="æ¯”è¾ƒå“ªä¸ª splitï¼ˆä»…åœ¨ --compare_same_split æ—¶ä½¿ç”¨ï¼‰")
    
    args = parser.parse_args()
    
    # è§£æé€šé“
    target_channels = args.channels.split()
    channels = parse_channels(target_channels)
    
    # åŠ è½½æ•°æ®é›†
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {args.data_root_dir}")
    dataset = MultimodalDataset(
        csv_path=args.csv_path,
        data_root_dir=args.data_root_dir,
        channels=channels,
        align_channels=None,
        alignment_model_path=None,
        device="cpu",
        print_info=True
    )
    
    # åˆ›å»º K æŠ˜åˆ†å‰²
    print(f"ğŸ”„ åˆ›å»º K æŠ˜åˆ†å‰² (fold={args.fold_idx}, seed={args.seed})")
    k_fold_splits = create_k_fold_splits(dataset, k=10, seed=args.seed)
    
    if args.fold_idx >= len(k_fold_splits):
        print(f"âŒ Fold ç´¢å¼• {args.fold_idx} è¶…å‡ºèŒƒå›´ (å…± {len(k_fold_splits)} ä¸ª folds)")
        sys.exit(1)
    
    split = k_fold_splits[args.fold_idx]
    
    # åˆ›å»º Subset æ•°æ®é›†
    train_subset = Subset(dataset, split['train'])
    val_subset = Subset(dataset, split['val'])
    test_subset = Subset(dataset, split['test'])
    
    print(f"\nğŸ“Š Split å¤§å°:")
    print(f"   Train: {len(train_subset)}")
    print(f"   Val: {len(val_subset)}")
    print(f"   Test: {len(test_subset)}")
    
    # æ¯”è¾ƒä¸åŒ split ä¹‹é—´çš„ samples
    if args.compare_splits:
        print(f"\nğŸ” æ¯”è¾ƒ trainã€valã€test ä¹‹é—´çš„ samples æ˜¯å¦ä¸€è‡´...")
        
        # åˆ›å»º DataLoader
        train_loader = get_split_loader(train_subset, training=False, weighted=False, batch_size=1, generator=None)
        val_loader = get_split_loader(val_subset, training=False, weighted=False, batch_size=1, generator=None)
        test_loader = get_split_loader(test_subset, training=False, weighted=False, batch_size=1, generator=None)
        
        # è·å–æ‰€æœ‰ case_ids
        train_case_ids = set(get_loader_case_ids(train_loader))
        val_case_ids = set(get_loader_case_ids(val_loader))
        test_case_ids = set(get_loader_case_ids(test_loader))
        
        # æ¯”è¾ƒæ˜¯å¦æœ‰é‡å 
        train_val_overlap = train_case_ids & val_case_ids
        train_test_overlap = train_case_ids & test_case_ids
        val_test_overlap = val_case_ids & test_case_ids
        
        all_same = True
        
        if train_val_overlap:
            print(f"âš ï¸ Train å’Œ Val æœ‰é‡å çš„ samples: {len(train_val_overlap)} ä¸ª")
            print(f"   é‡å çš„ case_ids: {sorted(list(train_val_overlap))[:10]}{'...' if len(train_val_overlap) > 10 else ''}")
            all_same = False
        else:
            print(f"âœ… Train å’Œ Val æ²¡æœ‰é‡å çš„ samples")
        
        if train_test_overlap:
            print(f"âš ï¸ Train å’Œ Test æœ‰é‡å çš„ samples: {len(train_test_overlap)} ä¸ª")
            print(f"   é‡å çš„ case_ids: {sorted(list(train_test_overlap))[:10]}{'...' if len(train_test_overlap) > 10 else ''}")
            all_same = False
        else:
            print(f"âœ… Train å’Œ Test æ²¡æœ‰é‡å çš„ samples")
        
        if val_test_overlap:
            print(f"âš ï¸ Val å’Œ Test æœ‰é‡å çš„ samples: {len(val_test_overlap)} ä¸ª")
            print(f"   é‡å çš„ case_ids: {sorted(list(val_test_overlap))[:10]}{'...' if len(val_test_overlap) > 10 else ''}")
            all_same = False
        else:
            print(f"âœ… Val å’Œ Test æ²¡æœ‰é‡å çš„ samples")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„ samples
        all_case_ids = train_case_ids | val_case_ids | test_case_ids
        dataset_case_ids = set(dataset.case_ids)
        missing_case_ids = dataset_case_ids - all_case_ids
        
        if missing_case_ids:
            print(f"âš ï¸ æœ‰ {len(missing_case_ids)} ä¸ª samples æ²¡æœ‰è¢«åŒ…å«åœ¨ä»»ä½• split ä¸­")
            print(f"   é—æ¼çš„ case_ids: {sorted(list(missing_case_ids))[:10]}{'...' if len(missing_case_ids) > 10 else ''}")
            all_same = False
        else:
            print(f"âœ… æ‰€æœ‰ samples éƒ½è¢«åŒ…å«åœ¨ split ä¸­")
        
        if all_same:
            print(f"\nâœ… æ‰€æœ‰ split ä¹‹é—´æ²¡æœ‰é‡å ï¼Œä¸”æ‰€æœ‰ samples éƒ½è¢«åŒ…å«")
            sys.exit(0)
        else:
            print(f"\nâŒ å‘ç° split ä¹‹é—´çš„é‡å æˆ–é—æ¼")
            sys.exit(1)
    
    # æ¯”è¾ƒåŒä¸€ä¸ª split çš„ä¸¤ä¸ª DataLoader
    elif args.compare_same_split:
        print(f"\nğŸ”§ åˆ›å»ºä¸¤ä¸ª DataLoader (split_type={args.split_type})")
        if args.split_type == "train":
            split_dataset = train_subset
            split_name = "Train"
        elif args.split_type == "val":
            split_dataset = val_subset
            split_name = "Val"
        else:
            split_dataset = test_subset
            split_name = "Test"
        
        # åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„ DataLoader
        loader1 = get_split_loader(split_dataset, training=False, weighted=False, batch_size=1, generator=None)
        loader2 = get_split_loader(split_dataset, training=False, weighted=False, batch_size=1, generator=None)
        
        # æ¯”è¾ƒ samples
        print(f"\nğŸ” æ¯”è¾ƒä¸¤ä¸ª {split_name} Loader çš„æ ·æœ¬é¡ºåº...")
        is_same = compare_loader_samples(loader1, loader2, name1=f"{split_name} Loader 1", name2=f"{split_name} Loader 2")
        
        if is_same:
            print(f"\nâœ… ä¸¤ä¸ª {split_name} Loader çš„æ ·æœ¬é¡ºåºä¸€è‡´")
            sys.exit(0)
        else:
            print(f"\nâŒ ä¸¤ä¸ª {split_name} Loader çš„æ ·æœ¬é¡ºåºä¸ä¸€è‡´")
            sys.exit(1)
    
    # é»˜è®¤ï¼šåŒæ—¶æ¯”è¾ƒ split ä¹‹é—´å’ŒåŒä¸€ä¸ª split çš„ä¸¤ä¸ª DataLoader
    else:
        print(f"\nğŸ” æ¯”è¾ƒ trainã€valã€test ä¹‹é—´çš„ samples æ˜¯å¦ä¸€è‡´...")
        
        # åˆ›å»º DataLoader
        train_loader = get_split_loader(train_subset, training=False, weighted=False, batch_size=1, generator=None)
        val_loader = get_split_loader(val_subset, training=False, weighted=False, batch_size=1, generator=None)
        test_loader = get_split_loader(test_subset, training=False, weighted=False, batch_size=1, generator=None)
        
        # è·å–æ‰€æœ‰ case_ids
        train_case_ids = set(get_loader_case_ids(train_loader))
        val_case_ids = set(get_loader_case_ids(val_loader))
        test_case_ids = set(get_loader_case_ids(test_loader))
        
        # æ¯”è¾ƒæ˜¯å¦æœ‰é‡å 
        train_val_overlap = train_case_ids & val_case_ids
        train_test_overlap = train_case_ids & test_case_ids
        val_test_overlap = val_case_ids & test_case_ids
        
        all_same = True
        
        if train_val_overlap:
            print(f"âš ï¸ Train å’Œ Val æœ‰é‡å çš„ samples: {len(train_val_overlap)} ä¸ª")
            print(f"   é‡å çš„ case_ids: {sorted(list(train_val_overlap))[:10]}{'...' if len(train_val_overlap) > 10 else ''}")
            all_same = False
        else:
            print(f"âœ… Train å’Œ Val æ²¡æœ‰é‡å çš„ samples")
        
        if train_test_overlap:
            print(f"âš ï¸ Train å’Œ Test æœ‰é‡å çš„ samples: {len(train_test_overlap)} ä¸ª")
            print(f"   é‡å çš„ case_ids: {sorted(list(train_test_overlap))[:10]}{'...' if len(train_test_overlap) > 10 else ''}")
            all_same = False
        else:
            print(f"âœ… Train å’Œ Test æ²¡æœ‰é‡å çš„ samples")
        
        if val_test_overlap:
            print(f"âš ï¸ Val å’Œ Test æœ‰é‡å çš„ samples: {len(val_test_overlap)} ä¸ª")
            print(f"   é‡å çš„ case_ids: {sorted(list(val_test_overlap))[:10]}{'...' if len(val_test_overlap) > 10 else ''}")
            all_same = False
        else:
            print(f"âœ… Val å’Œ Test æ²¡æœ‰é‡å çš„ samples")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„ samples
        all_case_ids = train_case_ids | val_case_ids | test_case_ids
        dataset_case_ids = set(dataset.case_ids)
        missing_case_ids = dataset_case_ids - all_case_ids
        
        if missing_case_ids:
            print(f"âš ï¸ æœ‰ {len(missing_case_ids)} ä¸ª samples æ²¡æœ‰è¢«åŒ…å«åœ¨ä»»ä½• split ä¸­")
            print(f"   é—æ¼çš„ case_ids: {sorted(list(missing_case_ids))[:10]}{'...' if len(missing_case_ids) > 10 else ''}")
            all_same = False
        else:
            print(f"âœ… æ‰€æœ‰ samples éƒ½è¢«åŒ…å«åœ¨ split ä¸­")
        
        # æ¯”è¾ƒåŒä¸€ä¸ª split çš„ä¸¤ä¸ª DataLoader
        print(f"\nğŸ” æ¯”è¾ƒåŒä¸€ä¸ª split çš„ä¸¤ä¸ª DataLoader æ˜¯å¦ä¸€è‡´...")
        
        # è¯´æ˜ weighted çš„ä½œç”¨
        print(f"\nğŸ“– Weighted å‚æ•°è¯´æ˜:")
        print(f"   - weighted=True: ä½¿ç”¨ WeightedRandomSamplerï¼Œæ ¹æ®ç±»åˆ«æƒé‡è¿›è¡Œé‡‡æ ·ï¼Œå¹³è¡¡ç±»åˆ«åˆ†å¸ƒ")
        print(f"   - weighted=False: ä½¿ç”¨ shuffle=Trueï¼Œåªæ˜¯éšæœºæ‰“ä¹±é¡ºåºï¼Œä¸è¿›è¡Œç±»åˆ«å¹³è¡¡")
        print(f"   - æ³¨æ„ï¼šå¦‚æœæ²¡æœ‰æä¾› generatorï¼Œæ¯æ¬¡åˆ›å»º DataLoader æ—¶éƒ½ä¼šä½¿ç”¨ä¸åŒçš„éšæœºçŠ¶æ€")
        
        # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
        print(f"\nğŸ“Š æ£€æŸ¥ Train æ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒ...")
        from trainer import make_weights_for_balanced_classes_split
        weights = make_weights_for_balanced_classes_split(train_subset)
        print(f"   WeightedRandomSampler çš„æƒé‡å·²è®¡ç®—ï¼Œæƒé‡èŒƒå›´: {weights.min():.4f} - {weights.max():.4f}")
        
        # è·å–æ ‡ç­¾åˆ†å¸ƒ
        train_labels = []
        for i in range(len(train_subset)):
            if hasattr(train_subset, 'dataset') and hasattr(train_subset.dataset, 'get_label'):
                original_idx = train_subset.indices[i]
                label = train_subset.dataset.get_label(original_idx)
            else:
                label = train_subset.get_label(i)
            train_labels.append(label)
        
        from collections import Counter
        label_counts = Counter(train_labels)
        print(f"   ç±»åˆ«åˆ†å¸ƒ: {dict(label_counts)}")
        
        # æ¯”è¾ƒ Train Loaderï¼ˆä½¿ç”¨ weighted=True å’Œ weighted=Falseï¼Œä½¿ç”¨ç›¸åŒçš„ generatorï¼‰
        print(f"\nğŸ“Š æ¯”è¾ƒ Train Loaderï¼ˆweighted=True vs weighted=Falseï¼Œä½¿ç”¨ç›¸åŒçš„ generatorï¼‰...")
        import torch
        generator = torch.Generator().manual_seed(args.seed)
        train_loader_weighted = get_split_loader(train_subset, training=True, weighted=True, batch_size=1, generator=generator)
        generator2 = torch.Generator().manual_seed(args.seed)
        train_loader_unweighted = get_split_loader(train_subset, training=True, weighted=False, batch_size=1, generator=generator2)
        train_same_weighted = compare_loader_samples(train_loader_weighted, train_loader_unweighted, name1="Train Loader (weighted=True)", name2="Train Loader (weighted=False)")
        
        # æ¯”è¾ƒ Train Loaderï¼ˆä¸¤ä¸ª weighted=True çš„ loaderï¼Œä½¿ç”¨ç›¸åŒçš„ generatorï¼‰
        print(f"\nğŸ“Š æ¯”è¾ƒ Train Loaderï¼ˆä¸¤ä¸ª weighted=True çš„ loaderï¼Œä½¿ç”¨ç›¸åŒçš„ generatorï¼‰...")
        generator3 = torch.Generator().manual_seed(args.seed)
        train_loader1_weighted = get_split_loader(train_subset, training=True, weighted=True, batch_size=1, generator=generator3)
        generator4 = torch.Generator().manual_seed(args.seed)
        train_loader2_weighted = get_split_loader(train_subset, training=True, weighted=True, batch_size=1, generator=generator4)
        train_same_weighted_self = compare_loader_samples(train_loader1_weighted, train_loader2_weighted, name1="Train Loader 1 (weighted=True)", name2="Train Loader 2 (weighted=True)")
        
        # æ¯”è¾ƒ Train Loaderï¼ˆä¸¤ä¸ª weighted=False çš„ loaderï¼Œä½¿ç”¨ç›¸åŒçš„ generatorï¼‰
        print(f"\nğŸ“Š æ¯”è¾ƒ Train Loaderï¼ˆä¸¤ä¸ª weighted=False çš„ loaderï¼Œä½¿ç”¨ç›¸åŒçš„ generatorï¼‰...")
        generator5 = torch.Generator().manual_seed(args.seed)
        train_loader1_unweighted = get_split_loader(train_subset, training=True, weighted=False, batch_size=1, generator=generator5)
        generator6 = torch.Generator().manual_seed(args.seed)
        train_loader2_unweighted = get_split_loader(train_subset, training=True, weighted=False, batch_size=1, generator=generator6)
        train_same_unweighted_self = compare_loader_samples(train_loader1_unweighted, train_loader2_unweighted, name1="Train Loader 1 (weighted=False)", name2="Train Loader 2 (weighted=False)")
        
        # æ£€æŸ¥ weighted å¯¹ç±»åˆ«åˆ†å¸ƒçš„å½±å“
        print(f"\nğŸ“Š æ£€æŸ¥ weighted å¯¹ç±»åˆ«åˆ†å¸ƒçš„å½±å“...")
        train_case_ids_weighted = get_loader_case_ids(train_loader1_weighted)
        train_case_ids_unweighted = get_loader_case_ids(train_loader1_unweighted)
        
        # è·å–æ¯ä¸ª case_id çš„æ ‡ç­¾
        case_id_to_label = {}
        for i in range(len(train_subset)):
            if hasattr(train_subset, 'dataset') and hasattr(train_subset.dataset, 'case_ids'):
                original_idx = train_subset.indices[i]
                case_id = train_subset.dataset.case_ids[original_idx]
                label = train_subset.dataset.get_label(original_idx)
            else:
                case_id = train_subset.case_ids[i]
                label = train_subset.get_label(i)
            case_id_to_label[case_id] = label
        
        # ç»Ÿè®¡ weighted å’Œ unweighted çš„ç±»åˆ«åˆ†å¸ƒ
        weighted_labels = [case_id_to_label[cid] for cid in train_case_ids_weighted]
        unweighted_labels = [case_id_to_label[cid] for cid in train_case_ids_unweighted]
        
        weighted_label_counts = Counter(weighted_labels)
        unweighted_label_counts = Counter(unweighted_labels)
        
        print(f"   Weighted=True çš„ç±»åˆ«åˆ†å¸ƒ: {dict(weighted_label_counts)}")
        print(f"   Weighted=False çš„ç±»åˆ«åˆ†å¸ƒ: {dict(unweighted_label_counts)}")
        
        # è®¡ç®—ç±»åˆ«æ¯”ä¾‹
        total_weighted = sum(weighted_label_counts.values())
        total_unweighted = sum(unweighted_label_counts.values())
        
        print(f"   Weighted=True çš„ç±»åˆ«æ¯”ä¾‹:")
        for label, count in sorted(weighted_label_counts.items()):
            print(f"     {label}: {count}/{total_weighted} ({count/total_weighted*100:.2f}%)")
        
        print(f"   Weighted=False çš„ç±»åˆ«æ¯”ä¾‹:")
        for label, count in sorted(unweighted_label_counts.items()):
            print(f"     {label}: {count}/{total_unweighted} ({count/total_unweighted*100:.2f}%)")
        
        # æ¯”è¾ƒ Val Loader
        print(f"\nğŸ“Š æ¯”è¾ƒ Val Loader...")
        val_loader1 = get_split_loader(val_subset, training=False, weighted=False, batch_size=1, generator=None)
        val_loader2 = get_split_loader(val_subset, training=False, weighted=False, batch_size=1, generator=None)
        val_same = compare_loader_samples(val_loader1, val_loader2, name1="Val Loader 1", name2="Val Loader 2")
        
        # æ¯”è¾ƒ Test Loader
        print(f"\nğŸ“Š æ¯”è¾ƒ Test Loader...")
        test_loader1 = get_split_loader(test_subset, training=False, weighted=False, batch_size=1, generator=None)
        test_loader2 = get_split_loader(test_subset, training=False, weighted=False, batch_size=1, generator=None)
        test_same = compare_loader_samples(test_loader1, test_loader2, name1="Test Loader 1", name2="Test Loader 2")
        
        # æ£€æŸ¥ Train Loader çš„å†…å®¹æ˜¯å¦ä¸ dataset ä¸€è‡´
        print(f"\nğŸ“Š æ£€æŸ¥ Train Loader çš„å†…å®¹æ˜¯å¦ä¸ dataset ä¸€è‡´...")
        train_case_ids_loader = set(get_loader_case_ids(train_loader1_weighted))
        train_case_ids_dataset = set([dataset.case_ids[i] for i in split['train']])
        
        if train_case_ids_loader == train_case_ids_dataset:
            print(f"âœ… Train Loader çš„å†…å®¹ä¸ dataset ä¸€è‡´ (å…± {len(train_case_ids_loader)} ä¸ª samples)")
        else:
            print(f"âš ï¸ Train Loader çš„å†…å®¹ä¸ dataset ä¸ä¸€è‡´")
            missing_in_loader = train_case_ids_dataset - train_case_ids_loader
            extra_in_loader = train_case_ids_loader - train_case_ids_dataset
            if missing_in_loader:
                print(f"   Loader ä¸­ç¼ºå°‘çš„ case_ids: {sorted(list(missing_in_loader))[:10]}{'...' if len(missing_in_loader) > 10 else ''}")
            if extra_in_loader:
                print(f"   Loader ä¸­å¤šä½™çš„ case_ids: {sorted(list(extra_in_loader))[:10]}{'...' if len(extra_in_loader) > 10 else ''}")
            train_same_weighted_self = False
        
        if all_same and train_same_weighted_self and train_same_unweighted_self and val_same and test_same:
            print(f"\nâœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡")
            sys.exit(0)
        else:
            print(f"\nâŒ å‘ç°ä¸€äº›é—®é¢˜")
            sys.exit(1)

