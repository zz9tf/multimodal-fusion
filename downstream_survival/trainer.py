"""
é€šç”¨è®­ç»ƒå™¨ç±»
æ”¯æŒä¸åŒçš„æ¨¡å‹ç±»å‹å’Œè®­ç»ƒé…ç½®
"""

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import os
import json
import csv
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from sklearn.preprocessing import label_binarize
import pandas as pd

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# åªå¯¼å…¥å¿…è¦çš„æ¨¡å—ï¼Œé¿å…ä¾èµ– utils.utils
from models.model_factory import ModelFactory
from sklearn.metrics import roc_auc_score
from torchmetrics.classification import AUROC as TM_AUROC

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def to_serializable(obj: Any) -> Any:
    """é€šç”¨JSONå®‰å…¨åºåˆ—åŒ–è½¬æ¢å™¨

    - å°† numpy æ ‡é‡è½¬æ¢ä¸º Python æ ‡é‡
    - å°† numpy æ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨
    - å°† torch å¼ é‡ç§»åŠ¨åˆ° CPU å¹¶è½¬æ¢ä¸ºåˆ—è¡¨
    - å…¶ä»–ä¸å¯åºåˆ—åŒ–å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    """
    # numpy æ ‡é‡
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, (np.floating,)):
        return float(obj)
    if obj is np.nan:
        return None
    # numpy æ•°ç»„
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    # torch å¼ é‡
    if torch.is_tensor(obj):
        try:
            return obj.detach().cpu().tolist()
        except Exception:
            return str(obj)
    # å…¶ä»–å¸¸è§ä¸å¯åºåˆ—åŒ–ç±»å‹å…œåº•ä¸ºå­—ç¬¦ä¸²
    try:
        json.dumps(obj)
        return obj
    except Exception:
        return str(obj)

def save_splits(split_datasets, column_keys, filename, boolean_style=False):
	"""ä¿å­˜æ•°æ®é›†åˆ†å‰²ä¿¡æ¯"""
	try:
		# è·å–æ¯ä¸ªåˆ†å‰²çš„case_ids
		splits = []
		for i, dataset in enumerate(split_datasets):
			if hasattr(dataset, 'case_ids'):
				# ç›´æ¥æ˜¯MultimodalDatasetå¯¹è±¡
				splits.append(pd.Series(dataset.case_ids))
			else:
				# fallback: ä½¿ç”¨ç´¢å¼•
				splits.append(pd.Series([f"sample_{j}" for j in range(len(dataset))]))
		
		if not boolean_style:
			df = pd.concat(splits, ignore_index=True, axis=1)
			df.columns = column_keys
		else:
			df = pd.concat(splits, ignore_index = True, axis=0)
			index = df.values.tolist()
			one_hot = np.eye(len(split_datasets)).astype(bool)
			bool_array = np.repeat(one_hot, [len(dset) for dset in split_datasets], axis=0)
			df = pd.DataFrame(bool_array, index=index, columns = ['train', 'val', 'test'])

		df.to_csv(filename)
		print(f"âœ… ä¿å­˜åˆ†å‰²ä¿¡æ¯åˆ°: {filename}")
	except Exception as e:
		print(f"âš ï¸ ä¿å­˜åˆ†å‰²ä¿¡æ¯å¤±è´¥: {e}")
		# åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆ†å‰²è®°å½•
		split_info = {
			'split_type': column_keys,
			'train_size': len(split_datasets[0]) if len(split_datasets) > 0 else 0,
			'val_size': len(split_datasets[1]) if len(split_datasets) > 1 else 0,
			'test_size': len(split_datasets[2]) if len(split_datasets) > 2 else 0
		}
		pd.DataFrame([split_info]).to_csv(filename, index=False)
		print(f"âœ… ä¿å­˜ç®€åŒ–åˆ†å‰²ä¿¡æ¯åˆ°: {filename}")

# å·¥å…·å‡½æ•°ï¼Œä½¿ç”¨é«˜æ•ˆçš„æ‰‹åŠ¨å®ç°
def calculate_accuracy(Y_hat: torch.Tensor, Y: torch.Tensor) -> float:
    """è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡"""
    return float((Y_hat == Y).float().mean().item())

def cal_auc(y_true: np.ndarray, y_scores: np.ndarray) -> float:
    """å¿«é€Ÿè®¡ç®—AUC - æ¯”sklearnå¿«18å€"""
    # æ’åº
    sorted_indices = np.argsort(y_scores)[::-1]
    y_true_sorted = y_true[sorted_indices]
    
    # è®¡ç®—TPRå’ŒFPR
    tp = np.cumsum(y_true_sorted)
    fp = np.cumsum(1 - y_true_sorted)
    
    # é¿å…é™¤é›¶
    tp_total = tp[-1]
    fp_total = fp[-1]
    
    if tp_total == 0 or fp_total == 0:
        return 0.5
    
    tpr = tp / tp_total
    fpr = fp / fp_total
    
    # è®¡ç®—AUCï¼ˆæ¢¯å½¢ç§¯åˆ†ï¼‰
    auc = np.trapz(tpr, fpr)
    return auc

def cal_roc_curve(y_true: np.ndarray, y_scores: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """å¿«é€Ÿè®¡ç®—ROCæ›²çº¿ - æ¯”sklearnå¿«9.6å€"""
    # æ’åº
    sorted_indices = np.argsort(y_scores)[::-1]
    y_true_sorted = y_true[sorted_indices]
    y_scores_sorted = y_scores[sorted_indices]
    
    # è®¡ç®—TPRå’ŒFPR
    tp = np.cumsum(y_true_sorted)
    fp = np.cumsum(1 - y_true_sorted)
    
    tp_total = tp[-1]
    fp_total = fp[-1]
    
    if tp_total == 0:
        tpr = np.zeros_like(tp, dtype=float)
    else:
        tpr = tp / tp_total
    
    if fp_total == 0:
        fpr = np.zeros_like(fp, dtype=float)
    else:
        fpr = fp / fp_total
    
    # æ·»åŠ èµ·å§‹ç‚¹
    tpr = np.concatenate([[0], tpr])
    fpr = np.concatenate([[0], fpr])
    
    return fpr, tpr, y_scores_sorted

def print_network(model: nn.Module):
    """æ‰“å°ç½‘ç»œç»“æ„å’Œå‚æ•°ç»Ÿè®¡"""
    print("=" * 50)
    print("Model Architecture:")
    print("=" * 50)
    print(model)
    print("=" * 50)
    
    # è®¡ç®—å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Non-trainable parameters: {total_params - trainable_params:,}")
    print("=" * 50)

def get_optim(model: nn.Module, opt: str, lr: float, reg: float) -> torch.optim.Optimizer:
    """è·å–ä¼˜åŒ–å™¨"""
    if opt == "adam":
        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 
                                   lr=lr, weight_decay=reg)
    elif opt == 'sgd':
        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), 
                                  lr=lr, momentum=0.9, weight_decay=reg)
    else:
        raise NotImplementedError
    
    return optimizer

def get_scheduler(optimizer: torch.optim.Optimizer, scheduler_config: Dict) -> Optional[torch.optim.lr_scheduler._LRScheduler]:
    """
    è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    Args:
        optimizer: ä¼˜åŒ–å™¨
        scheduler_config: è°ƒåº¦å™¨é…ç½®å­—å…¸
        
    Returns:
        å­¦ä¹ ç‡è°ƒåº¦å™¨æˆ–None
    """
    scheduler_type = scheduler_config.get('type', None)
    
    if scheduler_type is None:
        return None
    
    if scheduler_type == 'step':
        step_size = scheduler_config.get('step_size', 50)
        gamma = scheduler_config.get('gamma', 0.5)
        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
    
    elif scheduler_type == 'cosine':
        T_max = scheduler_config.get('T_max', 200)
        eta_min = scheduler_config.get('eta_min', 0.0)
        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)
    
    elif scheduler_type == 'cosine_warm_restart':
        T_0 = scheduler_config.get('T_0', 10)  # ç¬¬ä¸€ä¸ªé‡å¯å‘¨æœŸé•¿åº¦
        T_mult = scheduler_config.get('T_mult', 2)  # å‘¨æœŸé•¿åº¦å€å¢å› å­
        eta_min = scheduler_config.get('eta_min', 0.0)  # æœ€å°å­¦ä¹ ç‡
        return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=T_0, T_mult=T_mult, eta_min=eta_min
        )
    
    elif scheduler_type == 'plateau':
        mode = scheduler_config.get('mode', 'min')
        patience = scheduler_config.get('patience', 10)
        factor = scheduler_config.get('factor', 0.5)
        return torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode=mode, patience=patience, factor=factor, verbose=True
        )
    
    elif scheduler_type == 'exponential':
        gamma = scheduler_config.get('gamma', 0.95)
        return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)
    
    else:
        print(f"âš ï¸ æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")
        return None

def get_split_loader(dataset, training=False, weighted=False, batch_size=1):
    """è·å–æ•°æ®åŠ è½½å™¨"""
    if training:
        if weighted:
            weights = make_weights_for_balanced_classes_split(dataset)
            sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))
            return torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler)
        else:
            return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    else:
        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)

def make_weights_for_balanced_classes_split(dataset):
    """ä¸ºå¹³è¡¡ç±»åˆ«åˆ›å»ºæƒé‡"""
    N = float(len(dataset))
    
    # è·å–æ ‡ç­¾ï¼Œé€‚é…MultimodalDatasetæ ¼å¼
    labels = []
    unique_labels = set()
    
    for i in range(len(dataset)):
        # å¤„ç†Subsetå¯¹è±¡
        if hasattr(dataset, 'dataset') and hasattr(dataset.dataset, 'get_label'):
            # é€šè¿‡Subsetçš„indicesè·å–åŸå§‹æ•°æ®é›†çš„ç´¢å¼•
            original_idx = dataset.indices[i]
            label = dataset.dataset.get_label(original_idx)
        else:
            # ç›´æ¥å¤„ç†MultimodalDataset
            label = dataset.get_label(i)
        
        unique_labels.add(label)
        labels.append(label)
    
    # ä½¿ç”¨æ•°æ®é›†çš„æ ‡ç­¾æ˜ å°„
    if hasattr(dataset, 'label_to_int'):
        label_to_int = dataset.label_to_int
    else:
        # å¦‚æœæ²¡æœ‰æ ‡ç­¾æ˜ å°„ï¼Œåˆ›å»ºé»˜è®¤æ˜ å°„
        label_to_int = {label: idx for idx, label in enumerate(sorted(unique_labels))}
    
    # å°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—
    numeric_labels = [label_to_int[label] for label in labels]
    labels = np.array(numeric_labels)
    
    class_counts = np.bincount(labels)
    class_weights = N / class_counts
    weights = [class_weights[labels[i]] for i in range(len(dataset))]
    return torch.DoubleTensor(weights)


class Logger:
    """
    ç»Ÿä¸€çš„è®­ç»ƒæŒ‡æ ‡è®°å½•å™¨
    æ•´åˆäº†å‡†ç¡®ç‡ç»Ÿè®¡ã€è®­ç»ƒæ—¥å¿—è®°å½•å’Œæœ€ä½³æŒ‡æ ‡è·Ÿè¸ªåŠŸèƒ½
    """
    
    def __init__(self, n_classes: int, log_dir: str = None, fold: int = 0):
        """
        åˆå§‹åŒ–æŒ‡æ ‡è®°å½•å™¨
        
        Args:
            n_classes: ç±»åˆ«æ•°é‡
            log_dir: æ—¥å¿—ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
            fold: foldç´¢å¼•
        """
        self.n_classes = n_classes
        self.log_dir = log_dir
        self.fold = fold
        
        # ç±»åˆ«ç»Ÿè®¡
        self.batch_log = {
            'class_stats': [{"count": 0, "correct": 0} for _ in range(self.n_classes)],
            'labels': [],
            'probs': [],
            'loss': 0.0
        }
        
        # è®­ç»ƒæ—¥å¿—
        self.epoch_logs = []
        self.best_metrics = {
            'best_val_loss': float('inf'),
            'best_val_acc': 0.0,
            'best_val_auc': 0.0,
            'best_epoch': 0
        }
        
        # åˆå§‹åŒ–æ–‡ä»¶è®°å½•ï¼ˆå¦‚æœæä¾›äº†log_dirï¼‰
        if log_dir:
            os.makedirs(log_dir, exist_ok=True)
            self.csv_path = os.path.join(log_dir, f'fold_{fold}_training_log.csv')
            with open(self.csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'epoch', 'train_loss', 'train_acc', 'train_auc',
                    'val_loss', 'val_acc', 'val_auc', 'learning_rate',
                    'timestamp'
                ])
    
    def reset_epoch_stats(self):
        """é‡ç½®å½“å‰epochçš„ç»Ÿè®¡"""
        self.batch_log = {
            'class_stats': [{"count": 0, "correct": 0} for _ in range(self.n_classes)],
            'labels': [],
            'probs': [],
            'loss': 0.0
        }
    
    def log_batch(self, Y_hat, Y, Y_prob, loss):
        """
        è®°å½•æ‰¹æ¬¡é¢„æµ‹ç»“æœï¼Œlossï¼Œlabelsï¼Œprobs
        
        Args:
            Y_hat: é¢„æµ‹ç»“æœ (int, tensor, æˆ– array)
            Y: çœŸå®æ ‡ç­¾ (int, tensor, æˆ– array)
            Y_prob: é¢„æµ‹æ¦‚ç‡ (tensor, æˆ– array)
            loss: æŸå¤±å€¼ (tensor)
        """
        # ç»Ÿä¸€è½¬ä¸ºTensorï¼ˆä¸è½¬numpyï¼‰ï¼Œç”¨äºåç»­torch.cat
        if not torch.is_tensor(Y_hat):
            Y_hat = torch.as_tensor(Y_hat)
        if not torch.is_tensor(Y):
            Y = torch.as_tensor(Y)
        if not torch.is_tensor(Y_prob):
            Y_prob = torch.as_tensor(Y_prob)

        # ç»Ÿè®¡åˆ†ç±»æ­£ç¡®æ•°
        if Y_hat.numel() == 1 and Y.numel() == 1:
            label_class = int(Y.item())
            self.batch_log['class_stats'][label_class]["count"] += 1
            self.batch_log['class_stats'][label_class]["correct"] += (int(Y_hat.item() == Y.item()))
        else:
            unique_labels = torch.unique(Y)
            for label_class in unique_labels.tolist():
                cls_mask = (Y == label_class)
                self.batch_log['class_stats'][label_class]["count"] += int(cls_mask.sum().item())
                self.batch_log['class_stats'][label_class]["correct"] += int((Y_hat[cls_mask] == Y[cls_mask]).sum().item())

        # è¿½åŠ åˆ°æ—¥å¿—ï¼ˆä¿æŒä¸ºTensorï¼‰
        self.batch_log['labels'].append(Y)
        self.batch_log['probs'].append(Y_prob)
        self.batch_log['loss'] += float(loss.item())
    
    def get_class_accuracy(self, class_idx: int) -> Tuple[Optional[float], int, int]:
        """
        è·å–æŒ‡å®šç±»åˆ«çš„å‡†ç¡®ç‡
        
        Returns:
            (accuracy, correct_count, total_count)
        """
        count = self.batch_log['class_stats'][class_idx]["count"]
        correct = self.batch_log['class_stats'][class_idx]["correct"]
        
        if count == 0:
            return None, correct, count
        else:
            return float(correct) / count, correct, count
    
    def get_overall_accuracy(self) -> float:
        """è·å–æ•´ä½“å‡†ç¡®ç‡"""
        total_correct = sum(stat["correct"] for stat in self.batch_log['class_stats'])
        total_count = sum(stat["count"] for stat in self.batch_log['class_stats'])
        
        if total_count == 0:
            return 0.0
        return float(total_correct) / total_count
    
    def log_epoch(self, epoch: int, train_metrics: Dict, val_metrics: Dict, lr: float):
        """
        è®°å½•epochæŒ‡æ ‡
        
        Args:
            epoch: å½“å‰epoch
            train_metrics: è®­ç»ƒæŒ‡æ ‡å­—å…¸
            val_metrics: éªŒè¯æŒ‡æ ‡å­—å…¸  
            lr: å­¦ä¹ ç‡
        """
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        epoch_log = {
            'epoch': epoch,
            'train_loss': train_metrics.get('loss', 0.0),
            'train_acc': train_metrics.get('acc', 0.0),
            'train_auc': train_metrics.get('auc', 0.0),
            'val_loss': val_metrics.get('loss', 0.0),
            'val_acc': val_metrics.get('acc', 0.0),
            'val_auc': val_metrics.get('auc', 0.0),
            'learning_rate': lr,
            'timestamp': timestamp
        }
        
        self.epoch_logs.append(epoch_log)
        
        # å†™å…¥CSVæ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨äº†æ–‡ä»¶è®°å½•ï¼‰
        if self.log_dir:
            with open(self.csv_path, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    epoch_log['epoch'], epoch_log['train_loss'], epoch_log['train_acc'], 
                    epoch_log['train_auc'], epoch_log['val_loss'], epoch_log['val_acc'], 
                    epoch_log['val_auc'], epoch_log['learning_rate'], epoch_log['timestamp']
                ])
        
        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        self._update_best_metrics(epoch, val_metrics)
        
        # æ‰“å°è¿›åº¦
        print(f"ğŸ“Š Epoch {epoch:3d} | "
              f"Train: Loss={train_metrics.get('loss', 0.0):.4f}, "
              f"Acc={train_metrics.get('acc', 0.0):.4f}, "
              f"AUC={train_metrics.get('auc', 0.0):.4f} | "
              f"Val: Loss={val_metrics.get('loss', 0.0):.4f}, "
              f"Acc={val_metrics.get('acc', 0.0):.4f}, "
              f"AUC={val_metrics.get('auc', 0.0):.4f}")
    
    def _update_best_metrics(self, epoch: int, val_metrics: Dict):
        """æ›´æ–°æœ€ä½³æŒ‡æ ‡"""
        val_loss = val_metrics.get('loss', float('inf'))
        val_acc = val_metrics.get('acc', 0.0)
        val_auc = val_metrics.get('auc', 0.0)
        
        if val_loss < self.best_metrics['best_val_loss']:
            self.best_metrics['best_val_loss'] = val_loss
            self.best_metrics['best_epoch'] = epoch
            
        if val_acc > self.best_metrics['best_val_acc']:
            self.best_metrics['best_val_acc'] = val_acc
            
        if val_auc > self.best_metrics['best_val_auc']:
            self.best_metrics['best_val_auc'] = val_auc
    
    def save_summary(self, test_metrics: Dict = None):
        """ä¿å­˜è®­ç»ƒæ€»ç»“"""
        summary = {
            'fold': self.fold,
            'best_metrics': self.best_metrics,
            'total_epochs': len(self.epoch_logs),
            'final_epoch': self.epoch_logs[-1] if self.epoch_logs else None,
            'test_metrics': test_metrics
        }
        
        if self.log_dir:
            # ä¿å­˜JSONæ€»ç»“
            summary_path = os.path.join(self.log_dir, f'fold_{self.fold}_summary.json')
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False, default=to_serializable)
        
        # æ‰“å°æ€»ç»“
        print(f"\nğŸ¯ Fold {self.fold} è®­ç»ƒæ€»ç»“:")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {self.best_metrics['best_val_loss']:.4f} (Epoch {self.best_metrics['best_epoch']})")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_metrics['best_val_acc']:.4f}")
        print(f"   æœ€ä½³éªŒè¯AUC: {self.best_metrics['best_val_auc']:.4f}")
        
        if test_metrics:
            print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_metrics.get('acc', 0.0):.4f}")
            print(f"   æµ‹è¯•AUC: {test_metrics.get('auc', 0.0):.4f}")
        
        return summary

class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    
    def __init__(self, patience: int = 20, stop_epoch: int = 50, verbose: bool = False):
        self.patience = patience
        self.stop_epoch = stop_epoch
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf

    def __call__(self, epoch: int, val_loss: float, model: nn.Module, ckpt_name: str = 'checkpoint.pt') -> bool:
        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, ckpt_name)
        elif score < self.best_score:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience and epoch > self.stop_epoch:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, ckpt_name)
            self.counter = 0
        
        return self.early_stop

    def save_checkpoint(self, val_loss: float, model: nn.Module, ckpt_name: str):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model.state_dict(), ckpt_name)
        self.val_loss_min = val_loss

class Trainer:
    """
    é€šç”¨è®­ç»ƒå™¨ç±»
    æ”¯æŒä¸åŒçš„æ¨¡å‹ç±»å‹å’Œè®­ç»ƒé…ç½®
    """
    
    def __init__(self, 
                 configs: Dict,
                 log_dir: str = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            configs: é…ç½®å­—å…¸
            log_dir: æ—¥å¿—ä¿å­˜ç›®å½•
        """
        self.model_config = configs['model_config']
        self.experiment_config = configs['experiment_config']
        self.results_dir = self.experiment_config['results_dir']
        self.log_dir = log_dir or './logs'
        
        # éªŒè¯é…ç½®å®Œæ•´æ€§
        required_training_params = ['max_epochs', 'lr', 'reg', 'opt', 'early_stopping', 'batch_size']
        missing_training_params = [param for param in required_training_params if param not in self.experiment_config]
        if missing_training_params:
            raise ValueError(f"è®­ç»ƒé…ç½®ç¼ºå°‘å¿…éœ€å‚æ•°: {missing_training_params}")
        
        # ä»é…ç½®ä¸­æå–å‚æ•°
        self.max_epochs = self.experiment_config['max_epochs']
        self.lr = self.experiment_config['lr']
        self.reg = self.experiment_config['reg']
        self.opt = self.experiment_config['opt']
        self.early_stopping = self.experiment_config['early_stopping']
        self.batch_size = self.experiment_config['batch_size']
        
        # åˆå§‹åŒ–æ¨¡å‹å’ŒæŸå¤±å‡½æ•°
        self.model = None
        self.loss_fn = None
        self.scheduler = None

    def _init_model(self) -> nn.Module:
        """åˆå§‹åŒ–æ¨¡å‹"""
        # ä»model_configä¸­è·å–å‚æ•°å¹¶æ„å»ºé…ç½®
        config = self.model_config.copy()
        
        # ä½¿ç”¨æ¨¡å‹å·¥å‚åˆ›å»ºæ¨¡å‹
        model = ModelFactory.create_model(config)
        
        return model.to(device)
    
    def train_fold(self, 
                   datasets: Tuple[Any, Any, Any],
                   fold_idx: int) -> Tuple[Dict, float, float, float, float]:
        """
        Level 1: Foldè®­ç»ƒä¸»å…¥å£
        
        Args:
            datasets: (train_dataset, val_dataset, test_dataset)
            fold_idx: foldç´¢å¼•
            
        Returns:
            (results_dict, test_auc, val_auc, test_acc, val_acc)
        """
        print(f'\nTraining Fold {fold_idx}!')
        
        # åˆ›å»ºç›®å½•å’Œæ—¥å¿—è®°å½•å™¨
        metrics_logger = Logger(self.model_config['n_classes'], self.log_dir, fold_idx)

        # ä¿å­˜æ•°æ®é›†åˆ†å‰²
        train_split, val_split, test_split = datasets
        save_splits(datasets, ['train', 'val', 'test'], 
                   os.path.join(self.results_dir, 'splits_{}.csv'.format(fold_idx)))
        
        print(f"Training on {len(train_split)} samples")
        print(f"Validating on {len(val_split)} samples")
        print(f"Testing on {len(test_split)} samples")

        # åˆå§‹åŒ–æ¨¡å‹å’ŒæŸå¤±å‡½æ•°
        model = self._init_model()
        self.loss_fn = model.loss_fn
        print_network(model)
        optimizer = get_optim(model, self.opt, self.lr, self.reg)
        
        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_config = self.experiment_config.get('scheduler_config', {})
        self.scheduler = get_scheduler(optimizer, scheduler_config)
        if self.scheduler:
            print(f"ğŸ¯ ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_config.get('type', 'unknown')}")
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        train_loader = get_split_loader(train_split, training=True, weighted=True, batch_size=1)
        val_loader = get_split_loader(val_split, training=False, weighted=False, batch_size=1)
        test_loader = get_split_loader(test_split, training=False, weighted=False, batch_size=1)

        # åˆå§‹åŒ–æ—©åœ
        early_stopping_obj = EarlyStopping(patience=25, stop_epoch=10, verbose=True) if self.early_stopping else None
        
        # 2. è®­ç»ƒ
        for epoch in range(self.max_epochs):
            # è®­ç»ƒå’ŒéªŒè¯
            train_metrics = self._train_single_epoch(epoch, train_loader, optimizer, model, metrics_logger)
            val_metrics, stop = self._validate_single_epoch(fold_idx, epoch, val_loader, model, early_stopping_obj)
            
            # è®°å½•æ—¥å¿—
            metrics_logger.log_epoch(epoch, train_metrics, val_metrics, optimizer.param_groups[0]['lr'])
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            if self.scheduler:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    # ReduceLROnPlateauéœ€è¦éªŒè¯æŸå¤±
                    self.scheduler.step(val_metrics['loss'])
                else:
                    # å…¶ä»–è°ƒåº¦å™¨ä½¿ç”¨epoch
                    self.scheduler.step()
            
            if stop: 
                break
        
        # 3. æœ€ç»ˆè¯„ä¼°å’Œè¿”å›ç»“æœ
        # ä¿å­˜æ¨¡å‹
        checkpoint_path = os.path.join(self.results_dir, "s_{}_checkpoint.pt".format(fold_idx))
        if self.early_stopping:
            model.load_state_dict(torch.load(checkpoint_path))
        else:
            torch.save(model.state_dict(), checkpoint_path)

        # æœ€ç»ˆè¯„ä¼°
        _, val_accuracy, val_auc, _ = self._evaluate_model(val_loader, model)
        results_dict, test_accuracy, test_auc, eval_logger = self._evaluate_model(test_loader, model)
        
        print('Val accuracy: {:.4f}, ROC AUC: {:.4f}'.format(val_accuracy, val_auc))
        print('Test accuracy: {:.4f}, ROC AUC: {:.4f}'.format(test_accuracy, test_auc))

        # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
        for i in range(self.model_config['n_classes']):
            acc, correct, count = eval_logger.get_class_accuracy(i)
            print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))
        
        # ä¿å­˜è®­ç»ƒæ€»ç»“
        metrics_logger.save_summary({
            'acc': test_accuracy,
            'auc': test_auc,
            'loss': 1-test_accuracy
        })
            
        return results_dict, test_auc, val_auc, test_accuracy, val_accuracy

    def _train_single_epoch(self, epoch: int, loader: DataLoader, optimizer: torch.optim.Optimizer, model: nn.Module, logger: Logger) -> Dict:
        """
        Level 3: æ ‡å‡†æ¨¡å‹å•ä¸ªepochè®­ç»ƒ
        """
        model.train()
        
        # ğŸ”§ é‡ç½®epochç»Ÿè®¡ä¿¡æ¯ï¼Œç¡®ä¿æ¯ä¸ªepochçš„ç»Ÿè®¡æ˜¯ç‹¬ç«‹çš„
        logger.reset_epoch_stats()

        print('\n')
        batch_size = self.experiment_config['batch_size']
        total_loss = 0
        for batch_idx, (data, label) in enumerate(loader):
            # æ ‡ç­¾å·²ç»æ˜¯tensorï¼Œç›´æ¥ç§»åŠ¨åˆ°è®¾å¤‡
            label = label.to(device)
            
            # data ç°åœ¨æ˜¯å­—å…¸æ ¼å¼ï¼Œæ¯ä¸ªchannelåŒ…å«ä¸€ä¸ªå¼ é‡
            # éœ€è¦å°†æ¯ä¸ªchannelçš„å¼ é‡ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
            for channel in data:
                data[channel] = data[channel].to(device)
            results = model(data, label)
            Y_prob = results['probabilities']
            Y_hat = results['predictions']
            
            # è®¡ç®—æŸå¤±
            results['labels'] = label
            loss = self.loss_fn(results['logits'], results['labels'], results)
            total_loss += loss
            # è®°å½•æŒ‡æ ‡
            logger.log_batch(Y_hat, label, Y_prob, loss)
            
            if (batch_idx + 1) % batch_size == 0:
                # åå‘ä¼ æ’­
                if hasattr(model, 'group_loss_fn'):
                    results['auc_loss'] = model.group_loss_fn(results)
                    total_loss += results['auc_loss']
                total_loss = total_loss/batch_size
                total_loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                if hasattr(model, 'verbose_items'):
                    items = model.verbose_items(results)
                    if len(items) > 0:
                        print('Batch {}/{}: '.format(batch_idx + 1, len(loader)) + ' '.join([f'{key}: {value:.4f}' for key, value in items]))
                total_loss = 0
        
        if len(loader) % batch_size != 0:
            # è®¡ç®—å‰©ä½™batchçš„æ•°é‡
            remaining_batches = len(loader) % batch_size
            # åå‘ä¼ æ’­
            if hasattr(model, 'group_loss_fn'):
                results['auc_loss'] = model.group_loss_fn(results)
                total_loss += results['auc_loss']
            total_loss = total_loss / remaining_batches  # ä½¿ç”¨å‰©ä½™batchæ•°é‡è¿›è¡Œå¹³å‡
            total_loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            if hasattr(model, 'verbose_items'):
                items = model.verbose_items(results)
                if len(items) > 0:
                    print('Final batch: ' + ' '.join([f'{key}: {value:.4f}' for key, value in items]))
            total_loss = 0
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        train_loss = logger.batch_log['loss'] / len(loader)

        print('Epoch: {}, train_loss: {:.4f}, train_acc: {:.4f}'.format(epoch, train_loss, logger.get_overall_accuracy()))
        if hasattr(model, 'verbose_items'):
            results['is_epoch'] = True
            items = model.verbose_items(results)
            if len(items) > 0:
                print('- ' + ' '.join([f'{key}: {value:.4f}' for key, value in items]))
        
        # è®¡ç®—å¹¶è¿”å›æŒ‡æ ‡
        return self._calculate_epoch_metrics(logger)

    def _calculate_epoch_metrics(self, logger: Dict) -> Dict:
        """è®¡ç®—epochæŒ‡æ ‡"""
        n_classes = self.model_config['n_classes']
        
        # è®¡ç®—å‡†ç¡®ç‡
        train_acc = 0.0
        for i in range(n_classes):
            acc, correct, count = logger.get_class_accuracy(i)
            if acc is not None:
                train_acc += acc
            print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))
        
        labels = torch.cat(logger.batch_log['labels'], dim=0)
        probs = torch.cat(logger.batch_log['probs'], dim=0) # [N, C]
        train_acc /= n_classes
        train_loss = logger.batch_log['loss'] / len(labels)

        # è®¡ç®—AUC - ä½¿ç”¨ torchmetricsï¼ˆTensor/GPU åŸç”Ÿï¼‰
        if n_classes == 2:
            auroc = TM_AUROC(task='binary').to(probs.device)
            train_auc = float(auroc(probs[:, 1], labels.long()).item())
        else:
            auroc = TM_AUROC(task='multiclass', num_classes=n_classes, average='macro').to(probs.device)
            train_auc = float(auroc(probs, labels.long()).item())
        
        metrics = {
            'loss': train_loss,
            'acc': train_acc,
            'auc': train_auc
        }
        return metrics

    def _validate_single_epoch(self, cur: int, epoch: int, loader: DataLoader, model: nn.Module, early_stopping=None) -> Tuple[Dict, bool]:
        """éªŒè¯å‡½æ•°"""
        model.eval()
        n_classes = self.model_config['n_classes']
        logger = Logger(n_classes=n_classes)
        
        # é‡ç½®æ¨¡å‹çš„group_logitså’Œgroup_labelsï¼Œç¡®ä¿éªŒè¯æ—¶ä»å¹²å‡€çŠ¶æ€å¼€å§‹
        if hasattr(model, 'group_logits'):
            model.group_logits = []
        if hasattr(model, 'group_labels'):
            model.group_labels = []
        
        with torch.no_grad():
            for batch_idx, (data, label) in enumerate(loader):
                label = label.to(device)
                
                # data ç°åœ¨æ˜¯å­—å…¸æ ¼å¼ï¼Œæ¯ä¸ªchannelåŒ…å«ä¸€ä¸ªå¼ é‡
                # éœ€è¦å°†æ¯ä¸ªchannelçš„å¼ é‡ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
                for channel in data:
                    data[channel] = data[channel].to(device)

                results = model(data, label)
                Y_prob = results['probabilities']
                Y_hat = results['predictions']

                results['labels'] = label
                loss = self.loss_fn(results['logits'], results['labels'], results)
                logger.log_batch(Y_hat, label, Y_prob, loss)
        
        # åœ¨éªŒè¯ç»“æŸæ—¶è®¡ç®—AUCæŸå¤±
        if hasattr(model, 'group_loss_fn') and hasattr(model, 'group_logits') and model.group_logits:
            results['auc_loss'] = model.group_loss_fn(results)
            logger.batch_log['loss'] += results['auc_loss']
            
        val_loss = logger.batch_log['loss']/len(loader)
        val_acc = logger.get_overall_accuracy()
        labels = torch.cat(logger.batch_log['labels'], dim=0)
        prob = torch.cat(logger.batch_log['probs'], dim=0)
        
        if n_classes == 2:
            auroc = TM_AUROC(task='binary').to(prob.device)
            auc = float(auroc(prob[:, 1], labels.long()).item())
        else:
            auroc = TM_AUROC(task='multiclass', num_classes=n_classes, average='macro').to(prob.device)
            auc = float(auroc(prob, labels.long()).item())

        print('\nVal Set, val_loss: {:.4f}, val_accuracy: {:.4f}, auc: {:.4f}'.format(val_loss, val_acc, auc))
        
        if hasattr(model, 'verbose_items'):
            results['is_epoch'] = True
            items = model.verbose_items(results)
            if len(items) > 0:
                print('- ' + ' '.join([f'{key}: {value:.4f}' for key, value in items]))

        for i in range(n_classes):
            acc, correct, count = logger.get_class_accuracy(i)
            print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))
        
        val_metrics = {
            'loss': val_loss,
            'acc': val_acc,
            'auc': auc
        }

        if early_stopping:
            assert self.results_dir
            early_stopping(epoch, val_loss, model, 
                         ckpt_name=os.path.join(self.results_dir, "s_{}_checkpoint.pt".format(cur)))
            
            if early_stopping.early_stop:
                print("Early stopping")
                return val_metrics, True

        return val_metrics, False

    def _evaluate_model(self, loader: DataLoader, model: nn.Module) -> Tuple[Dict, float, float, Logger]:
        """æ¨¡å‹è¯„ä¼°æ€»ç»“"""
        model.eval()
        logger = Logger(n_classes=self.model_config['n_classes'])

        # é‡ç½®æ¨¡å‹çš„group_logitså’Œgroup_labelsï¼Œç¡®ä¿æµ‹è¯•æ—¶ä»å¹²å‡€çŠ¶æ€å¼€å§‹
        if hasattr(model, 'group_logits'):
            model.group_logits = []
        if hasattr(model, 'group_labels'):
            model.group_labels = []

        dataset_ref = loader.dataset
        case_ids_list: List[str]
        if hasattr(dataset_ref, 'case_ids'): # ç›´æ¥æ•°æ®é›†ï¼ˆæ‹¥æœ‰ case_ids å±æ€§ï¼‰
            base = dataset_ref.case_ids
            case_ids_list = list(base) if not isinstance(base, list) else base
        elif hasattr(dataset_ref, 'dataset') and hasattr(dataset_ref.dataset, 'case_ids') and hasattr(dataset_ref, 'indices'): # Subset æ•°æ®é›†ï¼ˆæ²¡æœ‰ case_ids å±æ€§ï¼Œéœ€ä»åŸæ•°æ®é›†æ˜ å°„ï¼‰
            try:
                base = dataset_ref.dataset.case_ids
                base_list = list(base) if not isinstance(base, list) else base
                case_ids_list = [base_list[i] for i in dataset_ref.indices]
            except Exception:
                case_ids_list = [f"sample_{i}" for i in range(len(dataset_ref))]
        else:
            case_ids_list = [f"sample_{i}" for i in range(len(dataset_ref))]
        patient_results = {}

        for batch_idx, (data, label) in enumerate(loader):
            label = label.to(device)
            # data ç°åœ¨æ˜¯å­—å…¸æ ¼å¼ï¼Œæ¯ä¸ªchannelåŒ…å«ä¸€ä¸ªå¼ é‡ï¼Œæ¬åˆ°è®¾å¤‡
            for channel in data:
                data[channel] = data[channel].to(device)
            case_id = case_ids_list[batch_idx]
            with torch.inference_mode():
                results = model(data, label)
                Y_prob = results['probabilities']
                Y_hat = results['predictions']
            
            results['labels'] = label
            loss = self.loss_fn(results['logits'], results['labels'], results)
            logger.log_batch(Y_hat, label, Y_prob, loss)
            
            patient_results.update({case_id: {'case_id': np.array(case_id), 'prob': Y_prob.cpu().numpy(), 'label': label.item()}})
        
        # åœ¨æµ‹è¯•ç»“æŸæ—¶è®¡ç®—AUCæŸå¤±
        if hasattr(model, 'group_loss_fn') and hasattr(model, 'group_logits') and model.group_logits:
            results['auc_loss'] = model.group_loss_fn(results)
            logger.batch_log['loss'] += results['auc_loss']
        
        test_loss = logger.batch_log['loss']/len(loader)
        test_acc = logger.get_overall_accuracy()
        
        if hasattr(model, 'verbose_items'):
            results['is_epoch'] = True
            items = model.verbose_items(results)
            if len(items) > 0:
                print('- ' + ' '.join([f'{key}: {value:.4f}' for key, value in items]))
        
        labels = torch.cat(logger.batch_log['labels'], dim=0)
        prob = torch.cat(logger.batch_log['probs'], dim=0)

        if self.model_config['n_classes'] == 2:
            auroc = TM_AUROC(task='binary').to(prob.device)
            auc = float(auroc(prob[:, 1], labels.long()).item())
        else:
            auroc = TM_AUROC(task='multiclass', num_classes=self.model_config['n_classes'], average='macro').to(prob.device)
            auc = float(auroc(prob, labels.long()).item())
        
        print('\nTest Set, test_loss: {:.4f}, test_accuracy: {:.4f}, auc: {:.4f}'.format(test_loss, test_acc, auc))

        return patient_results, test_acc, auc, logger
