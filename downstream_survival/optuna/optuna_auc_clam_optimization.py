#!/usr/bin/env python3
"""
Optuna è¶…å‚æ•°ä¼˜åŒ–è„šæœ¬ - é’ˆå¯¹ AUC_CLAM æ¨¡å‹
åŸºäº main.py å’Œ trainer.py çš„æ¶æ„è¿›è¡Œä¼˜åŒ–
"""

import os
import sys
import json
import argparse
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, Any, List
import optuna
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner
import optuna.visualization as vis
import torch
from torch.utils.data import Subset
import threading
import random
from optuna.storages.journal import JournalFileBackend
from optuna.storages import JournalStorage
import time
from contextlib import contextmanager
from collections import defaultdict
# å¯¼å…¥é¡¹ç›®æ¨¡å—
from trainer import Trainer
from datasets.multimodal_dataset import MultimodalDataset
from optuna_config import OptunaConfig
from main import create_k_fold_splits, parse_channels

class PerformanceProfiler:
    """
    æ€§èƒ½åˆ†æå™¨ - ç”¨äºè®°å½•å’Œåˆ†æä»£ç æ‰§è¡Œæ—¶é—´
    """
    
    def __init__(self, enable: bool = True):
        """
        åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨
        
        Args:
            enable: æ˜¯å¦å¯ç”¨æ€§èƒ½åˆ†æ
        """
        self.enable = enable
        self.timings = defaultdict(list)  # å­˜å‚¨æ¯ä¸ªæ­¥éª¤çš„æ—¶é—´åˆ—è¡¨
        self.current_trial = None
        self.start_times = {}  # å­˜å‚¨å½“å‰æ­£åœ¨è®¡æ—¶çš„æ­¥éª¤å¼€å§‹æ—¶é—´
        
    @contextmanager
    def time_block(self, step_name: str, trial_number: int = None):
        """
        ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºè®°å½•ä»£ç å—çš„æ‰§è¡Œæ—¶é—´
        
        Args:
            step_name: æ­¥éª¤åç§°
            trial_number: è¯•éªŒç¼–å·ï¼ˆå¯é€‰ï¼‰
        """
        if not self.enable:
            yield
            return
            
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        try:
            yield
        finally:
            # è®°å½•ç»“æŸæ—¶é—´å¹¶è®¡ç®—è€—æ—¶
            elapsed_time = time.time() - start_time
            key = f"{step_name}" if trial_number is None else f"Trial_{trial_number}_{step_name}"
            self.timings[step_name].append(elapsed_time)
            
            # æ‰“å°å®æ—¶ä¿¡æ¯
            trial_info = f"Trial {trial_number}: " if trial_number is not None else ""
            print(f"â±ï¸  {trial_info}{step_name}: {elapsed_time:.2f}ç§’")
    
    def get_statistics(self) -> Dict[str, Dict[str, float]]:
        """
        è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«æ¯ä¸ªæ­¥éª¤çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ€»æ—¶é—´ã€å¹³å‡æ—¶é—´ã€æœ€å°æ—¶é—´ã€æœ€å¤§æ—¶é—´ã€è°ƒç”¨æ¬¡æ•°ï¼‰
        """
        stats = {}
        for step_name, times in self.timings.items():
            if times:
                stats[step_name] = {
                    'total': sum(times),
                    'mean': np.mean(times),
                    'min': min(times),
                    'max': max(times),
                    'count': len(times),
                    'std': np.std(times) if len(times) > 1 else 0.0
                }
        return stats
    
    def print_summary(self):
        """æ‰“å°æ€§èƒ½åˆ†ææ‘˜è¦"""
        if not self.enable or not self.timings:
            return
            
        print("\n" + "="*80)
        print("ğŸ“Š æ€§èƒ½åˆ†ææ‘˜è¦")
        print("="*80)
        
        stats = self.get_statistics()
        
        # æŒ‰æ€»æ—¶é—´æ’åº
        sorted_stats = sorted(stats.items(), key=lambda x: x[1]['total'], reverse=True)
        
        # æ‰“å°è¡¨å¤´
        print(f"{'æ­¥éª¤åç§°':<40} {'æ€»æ—¶é—´(ç§’)':<15} {'å¹³å‡(ç§’)':<15} {'æœ€å°(ç§’)':<15} {'æœ€å¤§(ç§’)':<15} {'è°ƒç”¨æ¬¡æ•°':<10}")
        print("-"*80)
        
        total_all_time = sum(s['total'] for s in stats.values())
        
        # æ‰“å°æ¯ä¸ªæ­¥éª¤çš„ç»Ÿè®¡ä¿¡æ¯
        for step_name, stat in sorted_stats:
            percentage = (stat['total'] / total_all_time * 100) if total_all_time > 0 else 0
            print(f"{step_name:<40} {stat['total']:<15.2f} {stat['mean']:<15.2f} "
                  f"{stat['min']:<15.2f} {stat['max']:<15.2f} {stat['count']:<10} "
                  f"({percentage:.1f}%)")
        
        print("-"*80)
        print(f"{'æ€»è®¡':<40} {total_all_time:<15.2f}")
        print("="*80)
        
        # æ‰“å°å‰5ä¸ªæœ€è€—æ—¶çš„æ­¥éª¤
        print("\nğŸ” å‰5ä¸ªæœ€è€—æ—¶çš„æ­¥éª¤:")
        for i, (step_name, stat) in enumerate(sorted_stats[:5], 1):
            percentage = (stat['total'] / total_all_time * 100) if total_all_time > 0 else 0
            print(f"  {i}. {step_name}: {stat['total']:.2f}ç§’ ({percentage:.1f}%)")
        
        print()
    
    def save_to_file(self, filepath: str):
        """
        ä¿å­˜æ€§èƒ½åˆ†æç»“æœåˆ°æ–‡ä»¶
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
        """
        if not self.enable:
            return
            
        stats = self.get_statistics()
        output = {
            'timings': {k: v for k, v in self.timings.items()},
            'statistics': stats,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ æ€§èƒ½åˆ†æç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    
    def reset(self):
        """é‡ç½®æ€§èƒ½åˆ†æå™¨"""
        self.timings.clear()
        self.start_times.clear()
        self.current_trial = None

class AUCCLAMOptimizer:
    """
    é€šç”¨æ¨¡å‹çš„ Optuna ä¼˜åŒ–å™¨
    æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹çš„è¶…å‚æ•°æœç´¢å’Œæ¨¡å‹æ€§èƒ½ä¼˜åŒ–
    """
    
    def __init__(self, 
                 data_root_dir: str,
                 csv_path: str,
                 model_type: str = 'svd_gate_random_clam_detach',
                 results_dir: str = './optuna_results',
                 n_trials: int = 100,
                 n_jobs: int = 1,
                 timeout: int = None,
                 pruner: bool = True,
                 sampler: str = 'tpe',
                 enable_realtime_viz: bool = False,
                 viz_port: int = 8080,
                 data_root_base: str = None,
                 num_data_copies: int = 5,
                 **kwargs):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            data_root_dir: æ•°æ®æ ¹ç›®å½•ï¼ˆå•ä¸ªæ•°æ®é›†ç›®å½•ï¼‰
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            results_dir: ç»“æœä¿å­˜ç›®å½•
            n_trials: ä¼˜åŒ–è¯•éªŒæ¬¡æ•°
            n_jobs: å¹¶è¡Œä½œä¸šæ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            pruner: æ˜¯å¦å¯ç”¨å‰ªæ
            sampler: é‡‡æ ·å™¨ç±»å‹ ('tpe', 'random', 'cmaes')
            enable_realtime_viz: æ˜¯å¦å¯ç”¨å®æ—¶å¯è§†åŒ–
            viz_port: å¯è§†åŒ–ç«¯å£
            data_root_base: æ•°æ®é›†ç›®å½•çš„åŸºç¡€è·¯å¾„ï¼Œå¦‚æœæœ‰å¤šä¸ªæ•°æ®é›†å‰¯æœ¬
            num_data_copies: æ•°æ®é›†å‰¯æœ¬æ•°é‡ï¼ˆå¦‚ 5 è¡¨ç¤ºæœ‰ 1, 2, 3, 4, 5 äº”ä¸ªå‰¯æœ¬ï¼‰
        """
        self.data_root_dir = data_root_dir
        self.data_root_base = data_root_base  # æ•°æ®é›†ç›®å½•çš„åŸºç¡€è·¯å¾„
        self.num_data_copies = num_data_copies  # æ•°æ®é›†å‰¯æœ¬æ•°é‡
        self.csv_path = csv_path
        self.model_type = model_type
        self.results_dir = results_dir
        self.n_trials = n_trials
        self.n_jobs = n_jobs
        self.timeout = timeout
        self.enable_realtime_viz = enable_realtime_viz
        self.viz_port = viz_port
        self.kwargs = kwargs  # å­˜å‚¨å…¶ä»–å‚æ•°ï¼ˆå¦‚ input_dim, n_classes ç­‰ï¼‰
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs(self.results_dir, exist_ok=True)
        
        # é…ç½® Optuna é‡‡æ ·å™¨å’Œå‰ªæå™¨
        if sampler == 'tpe':
            self.sampler = TPESampler(seed=42)
        elif sampler == 'random':
            self.sampler = optuna.samplers.RandomSampler(seed=42)
        elif sampler == 'cmaes':
            self.sampler = optuna.samplers.CmaEsSampler(seed=42)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡‡æ ·å™¨ç±»å‹: {sampler}")
            
        self.pruner = MedianPruner() if pruner else None
        
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = OptunaConfig()
        
        # å­˜å‚¨æœ€ä½³è¯•éªŒç»“æœ
        self.best_trial = None
        self.trial_results = []
        
        # å®æ—¶å¯è§†åŒ–ç›¸å…³
        self.viz_thread = None
        self.study = None
        
        # æ€§èƒ½åˆ†æå™¨
        self.profiler = PerformanceProfiler(enable=True)
        
    def _create_objective_function(self, 
                                 dataset: MultimodalDataset,
                                 k_fold_splits: List[Dict],
                                 target_channels: List[str],
                                 n_folds: int = 3) -> callable:
        """
        åˆ›å»ºç›®æ ‡å‡½æ•°ç”¨äº Optuna ä¼˜åŒ–
        
        Args:
            dataset: å¤šæ¨¡æ€æ•°æ®é›†
            k_fold_splits: KæŠ˜äº¤å‰éªŒè¯åˆ†å‰²
            target_channels: ç›®æ ‡é€šé“åˆ—è¡¨
            n_folds: ç”¨äºä¼˜åŒ–çš„æŠ˜æ•°ï¼ˆå‡å°‘è®¡ç®—æ—¶é—´ï¼‰
            
        Returns:
            ç›®æ ‡å‡½æ•°
        """
        def objective(trial: optuna.Trial) -> float:
            """
            Optuna ç›®æ ‡å‡½æ•°
            
            Returns:
                éªŒè¯é›†å¹³å‡AUCåˆ†æ•°
            """
            # æ˜¾ç¤ºå½“å‰ trial å’Œè¿›ç¨‹/çº¿ç¨‹ä¿¡æ¯ï¼ˆç”¨äºéªŒè¯ n_jobsï¼‰
            print(f"ğŸ”¬ Trial {trial.number} å¼€å§‹æ‰§è¡Œ (è¿›ç¨‹ID: {os.getpid()}, çº¿ç¨‹ID: {threading.current_thread().ident})")
            
            trial_start_time = time.time()
            try:
                # 0. é€‰æ‹©æ•°æ®é›†ç›®å½•ï¼ˆå¦‚æœæœ‰å¤šä¸ªå‰¯æœ¬ï¼‰
                if self.data_root_base is not None:
                    # æ ¹æ® trial.number é€‰æ‹©æ•°æ®é›†å‰¯æœ¬ï¼ˆå¾ªç¯ä½¿ç”¨ï¼‰
                    with self.profiler.time_block("æ•°æ®é›†ç›®å½•é€‰æ‹©", trial.number):
                        data_copy_idx = (trial.number % self.num_data_copies) + 1
                        trial_data_root_dir = os.path.join(self.data_root_base, str(data_copy_idx))
                        print(f"ğŸ“‚ Trial {trial.number} ä½¿ç”¨æ•°æ®é›†å‰¯æœ¬: {trial_data_root_dir}")
                    
                    # åŠ è½½æ•°æ®é›†ï¼ˆæ¯ä¸ª trial ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†å‰¯æœ¬ï¼‰
                    with self.profiler.time_block("æ•°æ®é›†åŠ è½½", trial.number):
                        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                        trial_dataset = MultimodalDataset(
                            csv_path=self.csv_path,
                            data_root_dir=trial_data_root_dir,
                            channels=target_channels,
                            align_channels=None,
                            alignment_model_path=None,
                            device=device,
                            print_info=False  # ä¸æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
                        )
                    
                    # åˆ›å»ºKæŠ˜åˆ†å‰²ï¼ˆä½¿ç”¨ä¸ main.py ç›¸åŒçš„æ–¹æ³•ï¼‰
                    with self.profiler.time_block("KæŠ˜åˆ†å‰²åˆ›å»º", trial.number):
                        seed = self.kwargs.get('seed', 42)
                        fixed_test_split = self.kwargs.get('fixed_test_split', None)
                        trial_k_fold_splits = create_k_fold_splits(
                            dataset=trial_dataset,
                            k=10,
                            seed=seed,
                            fixed_test_split=fixed_test_split
                        )
                else:
                    # ä½¿ç”¨å…±äº«çš„æ•°æ®é›†ï¼ˆå‘åå…¼å®¹ï¼‰
                    trial_dataset = dataset
                    trial_k_fold_splits = k_fold_splits
                    trial_data_root_dir = self.data_root_dir
                
                # 1. å»ºè®®å®éªŒå‚æ•°
                with self.profiler.time_block("å®éªŒå‚æ•°å»ºè®®", trial.number):
                    experiment_params = self.config_manager.suggest_experiment_params(trial)
                
                # 2. å»ºè®®æ¨¡å‹å‚æ•°ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹ï¼‰
                with self.profiler.time_block("æ¨¡å‹å‚æ•°å»ºè®®", trial.number):
                    model_params = self.config_manager.suggest_model_params(trial, self.model_type)
                
                # 3. åˆ›å»ºé…ç½®
                with self.profiler.time_block("é…ç½®åˆ›å»º", trial.number):
                    configs = self.config_manager.create_configs(
                        model_type=self.model_type,
                        data_root_dir=trial_data_root_dir,
                        csv_path=self.csv_path,
                        target_channels=target_channels,
                        experiment_params=experiment_params,
                        model_params=model_params,
                        trial_number=trial.number,
                        num_splits=10,
                        **self.kwargs
                    )
                
                # 3. åˆå§‹åŒ–è®­ç»ƒå™¨
                with self.profiler.time_block("è®­ç»ƒå™¨åˆå§‹åŒ–", trial.number):
                    trainer = Trainer(
                        configs=configs,
                        log_dir=os.path.join(self.results_dir, f'trial_{trial.number}')
                    )
                
                # 4. ä½¿ç”¨å‰ n_folds è¿›è¡Œå¿«é€ŸéªŒè¯
                fold_aucs = []
                total_training_time = 0
                for fold_idx in range(min(n_folds, len(trial_k_fold_splits))):
                    # è·å–å½“å‰foldçš„åˆ†å‰²
                    with self.profiler.time_block(f"Fold_{fold_idx}_æ•°æ®å‡†å¤‡", trial.number):
                        split = trial_k_fold_splits[fold_idx]
                        train_idx = split['train']
                        val_idx = split['val']
                        test_idx = split['test']
                        
                        # åˆ›å»ºå­æ•°æ®é›†
                        train_dataset = Subset(trial_dataset, train_idx)
                        val_dataset = Subset(trial_dataset, val_idx)
                        test_dataset = Subset(trial_dataset, test_idx)
                        
                        datasets = (train_dataset, val_dataset, test_dataset)
                    
                    # è®­ç»ƒå¹¶è·å–éªŒè¯AUC
                    try:
                        fold_start_time = time.time()
                        with self.profiler.time_block(f"Fold_{fold_idx}_è®­ç»ƒ", trial.number):
                            _, test_auc, val_auc, test_acc, val_acc = trainer.train_fold(
                                datasets=datasets,
                                fold_idx=fold_idx
                            )
                        fold_aucs.append(val_auc)
                        total_training_time += time.time() - fold_start_time
                        
                        # æŠ¥å‘Šä¸­é—´ç»“æœç»™ Optunaï¼ˆç”¨äºå‰ªæï¼‰
                        trial.report(val_auc, step=fold_idx)
                        
                        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‰ªæ
                        if trial.should_prune():
                            raise optuna.TrialPruned()
                            
                    except Exception as e:
                        import traceback
                        print(f"âš ï¸ Fold {fold_idx} è®­ç»ƒå¤±è´¥: {e}")
                        print(f"   é”™è¯¯è¯¦æƒ…:")
                        traceback.print_exc()
                        # è¿”å›ä¸€ä¸ªåŸºäºè¯•éªŒå‚æ•°çš„éšæœºåˆ†æ•°ï¼Œé¿å…æ‰€æœ‰è¯•éªŒè¿”å›ç›¸åŒåˆ†æ•°
                        random_auc = 0.3 + random.random() * 0.4  # 0.3-0.7ä¹‹é—´çš„éšæœºåˆ†æ•°
                        fold_aucs.append(random_auc)
                
                # 5. è®¡ç®—å¹³å‡AUC
                with self.profiler.time_block("ç»“æœè®¡ç®—", trial.number):
                    mean_auc = np.mean(fold_aucs) if fold_aucs else 0.5
                    
                    # 6. è®°å½•è¯•éªŒç»“æœ
                    trial_result = {
                        'trial_number': trial.number,
                        'experiment_params': experiment_params,
                        'model_params': model_params,
                        'mean_val_auc': mean_auc,
                        'fold_aucs': fold_aucs,
                        'timestamp': datetime.now().isoformat()
                    }
                    self.trial_results.append(trial_result)
                
                trial_total_time = time.time() - trial_start_time
                print(f"ğŸ¯ Trial {trial.number}: Mean Val AUC = {mean_auc:.4f} | æ€»è€—æ—¶: {trial_total_time:.2f}ç§’ | è®­ç»ƒè€—æ—¶: {total_training_time:.2f}ç§’ ({total_training_time/trial_total_time*100:.1f}%)")
                return mean_auc
                
            except optuna.TrialPruned:
                raise
            except Exception as e:
                print(f"âŒ Trial {trial.number} å¤±è´¥: {e}")
                # è¿”å›ä¸€ä¸ªåŸºäºè¯•éªŒå‚æ•°çš„éšæœºåˆ†æ•°ï¼Œé¿å…æ‰€æœ‰è¯•éªŒè¿”å›ç›¸åŒåˆ†æ•°
                random_auc = 0.3 + random.random() * 0.4  # 0.3-0.7ä¹‹é—´çš„éšæœºåˆ†æ•°
                return random_auc
        
        return objective
    
    def _start_realtime_visualization(self, study: optuna.Study):
        """å¯åŠ¨å®æ—¶å¯è§†åŒ–æœåŠ¡å™¨"""
        if not self.enable_realtime_viz:
            return
            
        def run_viz_server():
            try:
                # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯å‡†å¤‡å¯è§†åŒ–ï¼Œå®é™…å›¾è¡¨ä¼šåœ¨è¯•éªŒå®Œæˆåç”Ÿæˆ
                print(f"ğŸŒ å®æ—¶å¯è§†åŒ–å·²å‡†å¤‡å°±ç»ª")
                print(f"ğŸ“Š å›¾è¡¨å°†åœ¨è¯•éªŒå®Œæˆåç”Ÿæˆ")
                print(f"ğŸ’¡ æŸ¥çœ‹å®æ—¶ä¼˜åŒ–è¿›åº¦")
            except Exception as e:
                print(f"âš ï¸ å®æ—¶å¯è§†åŒ–å¯åŠ¨å¤±è´¥: {e}")
        
        self.viz_thread = threading.Thread(target=run_viz_server, daemon=True)
        self.viz_thread.start()
    
    def _save_realtime_plots(self, study: optuna.Study, trial_number: int):
        """ä¿å­˜å®æ—¶å›¾è¡¨"""
        try:
            viz_dir = os.path.join(self.results_dir, "plots")
            os.makedirs(viz_dir, exist_ok=True)
            
            # ä¿å­˜ä¼˜åŒ–å†å²å›¾
            if len(study.trials) > 0:
                fig1 = vis.plot_optimization_history(study)
                fig1.write_html(os.path.join(viz_dir, f"optimization_history.html"))
                print(f"ğŸ“Š ä¼˜åŒ–å†å²å›¾å·²ä¿å­˜: {os.path.join(viz_dir, 'optimization_history.html')}")
            
            # ä¿å­˜å‚æ•°é‡è¦æ€§å›¾
            if len(study.trials) > 10:  # éœ€è¦è¶³å¤Ÿçš„è¯•éªŒæ‰èƒ½è®¡ç®—é‡è¦æ€§
                try:
                    fig2 = vis.plot_param_importances(study)
                    fig2.write_html(os.path.join(viz_dir, f"param_importances.html"))
                    print(f"ğŸ“Š å‚æ•°é‡è¦æ€§å›¾å·²ä¿å­˜: {os.path.join(viz_dir, 'param_importances.html')}")
                except Exception as e:
                    print(f"âš ï¸ å‚æ•°é‡è¦æ€§å›¾ç”Ÿæˆå¤±è´¥: {e}")
                    
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å›¾è¡¨å¤±è´¥: {e}")
    
    def optimize(self,
                 target_channels: List[str] = None,
                 n_folds: int = 3,
                 study_name: str = None) -> optuna.Study:
        """
        æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            target_channels: ç›®æ ‡é€šé“åˆ—è¡¨
            n_folds: ç”¨äºä¼˜åŒ–çš„æŠ˜æ•°
            study_name: ç ”ç©¶åç§°
            
        Returns:
            Optuna Study å¯¹è±¡
        """
        print(f"ğŸš€ å¼€å§‹ {self.model_type} è¶…å‚æ•°ä¼˜åŒ–...")
        print(f"ğŸ“Š è¯•éªŒæ¬¡æ•°: {self.n_trials}")
        print(f"ğŸ“ ç»“æœç›®å½•: {self.results_dir}")
        
        # è®¾ç½®é»˜è®¤ç›®æ ‡é€šé“
        if target_channels is None:
            target_channels = parse_channels(['wsi', 'tma', 'clinical', 'pathological', 'blood', 'icd', 'tma_cell_density'])
        
        # ä¿å­˜ target_channels ä»¥ä¾¿åç»­ä½¿ç”¨
        self.target_channels = target_channels
        
        # å¦‚æœæŒ‡å®šäº† data_root_baseï¼Œè¯´æ˜æœ‰å¤šä¸ªæ•°æ®é›†å‰¯æœ¬ï¼Œå°†åœ¨ objective å‡½æ•°ä¸­æŒ‰ trial åˆ†é…
        # å¦åˆ™ï¼Œä½¿ç”¨å•ä¸ªæ•°æ®é›†ï¼ˆå‘åå…¼å®¹ï¼‰
        if self.data_root_base is not None:
            print(f"\nğŸ“‚ æ£€æµ‹åˆ°å¤šä¸ªæ•°æ®é›†å‰¯æœ¬ï¼ˆ{self.num_data_copies} ä¸ªï¼‰")
            print(f"ğŸ“ æ•°æ®é›†åŸºç¡€è·¯å¾„: {self.data_root_base}")
            print(f"ğŸ’¡ ä¸åŒ trial å°†ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†å‰¯æœ¬ï¼Œä»¥é”™å¼€è¯»å–")
            # ä¸åœ¨è¿™é‡ŒåŠ è½½æ•°æ®é›†ï¼Œè€Œæ˜¯åœ¨ objective å‡½æ•°ä¸­æŒ‰ trial åŠ è½½
            dataset = None
            k_fold_splits = None
        else:
            # åŠ è½½æ•°æ®é›†ï¼ˆå•ä¸ªæ•°æ®é›†ï¼Œå‘åå…¼å®¹ï¼‰
            print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
            with self.profiler.time_block("åˆå§‹æ•°æ®é›†åŠ è½½"):
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                dataset = MultimodalDataset(
                    csv_path=self.csv_path,
                    data_root_dir=self.data_root_dir,
                    channels=target_channels,
                    align_channels=None,  # ä¸ä½¿ç”¨å¯¹é½
                    alignment_model_path=None,  # ä¸ä½¿ç”¨å¯¹é½
                    device=device,
                    print_info=True
                )
            
            # åˆ›å»ºKæŠ˜åˆ†å‰²ï¼ˆä½¿ç”¨ä¸ main.py ç›¸åŒçš„æ–¹æ³•ï¼‰
            print(f"\nğŸ”„ åˆ›å»º {10}-fold äº¤å‰éªŒè¯åˆ†å‰²...")
            with self.profiler.time_block("åˆå§‹KæŠ˜åˆ†å‰²åˆ›å»º"):
                seed = self.kwargs.get('seed', 42)
                fixed_test_split = self.kwargs.get('fixed_test_split', None)
                k_fold_splits = create_k_fold_splits(
                    dataset=dataset,
                    k=10,
                    seed=seed,
                    fixed_test_split=fixed_test_split
                )
            print(f"âœ… åˆ›å»ºäº† {len(k_fold_splits)} ä¸ªfold")
        
        # åˆ›å»ºç›®æ ‡å‡½æ•°
        objective = self._create_objective_function(
            dataset=dataset,
            k_fold_splits=k_fold_splits,
            target_channels=target_channels,
            n_folds=n_folds
        )
        
        # åˆ›å»ºæˆ–åŠ è½½ç ”ç©¶
        if study_name is None:
            study_name = f"{self.model_type}_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ä½¿ç”¨ JournalStorage ä½œä¸ºå­˜å‚¨åç«¯ï¼ˆæ”¯æŒå¹¶è¡Œï¼Œæ— éœ€æ•°æ®åº“ï¼‰
        journal_path = os.path.join(self.results_dir, f"{study_name}.log")
        try:
            journal_backend = JournalFileBackend(journal_path)
            storage_url = JournalStorage(journal_backend)
            print(f"ğŸ“¦ ä½¿ç”¨ JournalStorage: {journal_path}")
            print(f"âœ… æ”¯æŒå¹¶è¡Œæ‰§è¡Œ (n_jobs={self.n_jobs})")
        except ImportError:
            raise ImportError(
                "JournalStorage ä¸å¯ç”¨ã€‚è¯·ç¡®ä¿ Optuna ç‰ˆæœ¬ >= 3.0ã€‚\n"
                "å®‰è£…å‘½ä»¤: pip install optuna>=3.0"
            )
        
        # åŠ è½½æˆ–åˆ›å»ºç ”ç©¶
        try:
            study = optuna.load_study(
                study_name=study_name,
                storage=storage_url,
                sampler=self.sampler,
                pruner=self.pruner
            )
            print(f"ğŸ“– åŠ è½½ç°æœ‰ç ”ç©¶: {study_name}")
        except KeyError:
            print(f"ğŸ†• åˆ›å»ºæ–°ç ”ç©¶: {study_name}")
            study = optuna.create_study(
                study_name=study_name,
                storage=storage_url,
                direction='maximize',  # æœ€å¤§åŒ–AUC
                sampler=self.sampler,
                pruner=self.pruner,
                load_if_exists=True
            )
        
        # å¯åŠ¨å®æ—¶å¯è§†åŒ–
        self.study = study
        self._start_realtime_visualization(study)
        
        # æ‰§è¡Œä¼˜åŒ–
        print(f"\nğŸ¯ å¼€å§‹ä¼˜åŒ– (ä½¿ç”¨å‰ {n_folds} folds)...")
        print(f"âš™ï¸  å¹¶è¡Œä½œä¸šæ•° (n_jobs): {self.n_jobs}")
        print(f"ğŸ“Š æ€»è¯•éªŒæ•° (n_trials): {self.n_trials}")
        print("")
        
        # ä½¿ç”¨ Optuna çš„æ ‡å‡†ä¼˜åŒ–æ–¹æ³•
        study.optimize(
            objective, 
            n_trials=self.n_trials, 
            n_jobs=self.n_jobs,
            timeout=self.timeout,
            show_progress_bar=True
        )
        
        # ä¿å­˜æœ€ç»ˆå›¾è¡¨
        self._save_realtime_plots(study, len(study.trials))
        
        # å¦‚æœå¯ç”¨äº†å®æ—¶å¯è§†åŒ–ï¼Œæ˜¾ç¤ºæœ€ç»ˆç»“æœ
        if self.enable_realtime_viz:
            print(f"\nğŸŒ å®æ—¶å¯è§†åŒ–åœ°å€: http://localhost:{self.viz_port}")
            print(f"ğŸ“ å®æ—¶å›¾è¡¨ä¿å­˜åœ¨: {os.path.join(self.results_dir, 'realtime_plots')}")
        
        # ä¿å­˜ç»“æœ
        self._save_results(study, study_name)
        
        # æ›´æ–°æœ€ä½³è¯•éªŒ
        self.best_trial = study.best_trial
        
        # æ‰“å°æ€§èƒ½åˆ†ææ‘˜è¦
        self.profiler.print_summary()
        
        # ä¿å­˜æ€§èƒ½åˆ†æç»“æœ
        perf_file = os.path.join(self.results_dir, f"{study_name}_performance.json")
        self.profiler.save_to_file(perf_file)
        
        print(f"\nğŸ‰ ä¼˜åŒ–å®Œæˆ!")
        print(f"ğŸ† æœ€ä½³è¯•éªŒ: {study.best_trial.number}")
        print(f"ğŸ“ˆ æœ€ä½³AUC: {study.best_value:.4f}")
        print(f"âš™ï¸ æœ€ä½³å‚æ•°: {study.best_params}")
        
        return study
    
    def _save_results(self, study: optuna.Study, study_name: str):
        """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
        # ä¿å­˜ç ”ç©¶åˆ°æ•°æ®åº“
        study_path = os.path.join(self.results_dir, f"{study_name}.db")
        print(f"ğŸ’¾ ç ”ç©¶å·²ä¿å­˜åˆ°: {study_path}")
        
        # ä¿å­˜è¯•éªŒç»“æœåˆ°JSON
        results_path = os.path.join(self.results_dir, f"{study_name}_results.json")
        results_data = {
            'study_name': study_name,
            'best_trial': {
                'number': study.best_trial.number,
                'value': study.best_value,
                'params': study.best_params
            },
            'n_trials': len(study.trials),
            'trial_results': self.trial_results,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
        # ä¿å­˜CSVæ ¼å¼çš„è¯•éªŒç»“æœ
        trials_df = study.trials_dataframe()
        csv_path = os.path.join(self.results_dir, f"{study_name}_trials.csv")
        trials_df.to_csv(csv_path, index=False)
        print(f"ğŸ“Š è¯•éªŒæ•°æ®å·²ä¿å­˜åˆ°: {csv_path}")
    
    def get_best_config(self) -> Dict[str, Any]:
        """è·å–æœ€ä½³é…ç½®"""
        if self.best_trial is None:
            raise ValueError("å°šæœªè¿›è¡Œä¼˜åŒ–ï¼Œè¯·å…ˆè¿è¡Œ optimize() æ–¹æ³•")
        
        # åˆ†ç¦»å®éªŒå‚æ•°å’Œæ¨¡å‹å‚æ•°
        experiment_params = {}
        model_params = {}
        
        # å®éªŒå‚æ•°é”®ï¼ˆä¼šè¢« Optuna ä¼˜åŒ–çš„å‚æ•°ï¼‰
        experiment_param_keys = [
            'lr', 'reg', 'opt', 'batch_size', 'max_epochs', 'early_stopping', 'dropout', 'seed'
        ]
        
        # ä» best_trial.params ä¸­æå–è¢«ä¼˜åŒ–çš„å‚æ•°
        for key, value in self.best_trial.params.items():
            if key in experiment_param_keys:
                experiment_params[key] = value
            else:
                model_params[key] = value
        
        # è®¾ç½®å›ºå®šå€¼ï¼ˆä¸ä¼šè¢« Optuna ä¼˜åŒ–ï¼Œä½†éœ€è¦åœ¨é…ç½®ä¸­ï¼‰
        experiment_params['opt'] = 'adam'  # å›ºå®šä¸º adam
        experiment_params['scheduler_config'] = {
            "type": "plateau",
            "mode": "min", 
            "patience": 15, 
            "factor": 0.5
        }
        experiment_params['alignment_model_path'] = self.kwargs.get('alignment_model_path', None)
        experiment_params['aligned_channels'] = self.kwargs.get('aligned_channels', None)
        
        return self.config_manager.create_configs(
            model_type=self.model_type,
            data_root_dir=self.data_root_dir,
            csv_path=self.csv_path,
            target_channels=self.target_channels if hasattr(self, 'target_channels') else None,
            experiment_params=experiment_params,
            model_params=model_params,
            **self.kwargs
        )

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Optuna è¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ”¯æŒå¤šç§æ¨¡å‹ç±»å‹ï¼‰')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--data_root_dir', type=str, required=True,
                       help='æ•°æ®æ ¹ç›®å½•ï¼ˆå•ä¸ªæ•°æ®é›†ç›®å½•ï¼Œå¦‚æœä½¿ç”¨å¤šä¸ªå‰¯æœ¬ï¼Œåˆ™ä½œä¸ºé»˜è®¤å€¼ï¼‰')
    parser.add_argument('--data_root_base', type=str, default=None,
                       help='æ•°æ®é›†ç›®å½•çš„åŸºç¡€è·¯å¾„ï¼ˆå¦‚ /home/zheng/zheng/publicï¼‰ï¼Œå¦‚æœæœ‰å¤šä¸ªæ•°æ®é›†å‰¯æœ¬')
    parser.add_argument('--num_data_copies', type=int, default=5,
                       help='æ•°æ®é›†å‰¯æœ¬æ•°é‡ï¼ˆå¦‚ 5 è¡¨ç¤ºæœ‰ 1, 2, 3, 4, 5 äº”ä¸ªå‰¯æœ¬ï¼‰')
    parser.add_argument('--csv_path', type=str, required=True,
                       help='CSVæ–‡ä»¶è·¯å¾„')
    
    # æ¨¡å‹ç±»å‹
    parser.add_argument('--model_type', type=str, 
                       choices=['mil', 'clam', 'auc_clam', 'clam_mlp', 'clam_mlp_detach',
                                'svd_gate_random_clam', 'svd_gate_random_clam_detach',
                                'deep_supervise_svd_gate_random', 'deep_supervise_svd_gate_random_detach',
                                'clip_gate_random_clam', 'clip_gate_random_clam_detach',
                                'gate_shared_mil', 'gate_mil', 'gate_auc_mil', 'gate_mil_detach'],
                       default='auc_clam',
                       help='æ¨¡å‹ç±»å‹ (default: auc_clam)')
    
    # ä¼˜åŒ–å‚æ•°
    parser.add_argument('--results_dir', type=str, default='./optuna_results',
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--n_trials', type=int, default=100,
                       help='ä¼˜åŒ–è¯•éªŒæ¬¡æ•°')
    parser.add_argument('--n_jobs', type=int, default=1,
                       help='å¹¶è¡Œä½œä¸šæ•°')
    parser.add_argument('--timeout', type=int, default=None,
                       help='è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰')
    parser.add_argument('--n_folds', type=int, default=3,
                       help='ç”¨äºä¼˜åŒ–çš„æŠ˜æ•°')
    parser.add_argument('--study_name', type=str, default=None,
                       help='ç ”ç©¶åç§°')
    
    # é‡‡æ ·å™¨å’Œå‰ªæé€‰é¡¹
    parser.add_argument('--sampler', type=str, choices=['tpe', 'random', 'cmaes'], 
                       default='tpe', help='é‡‡æ ·å™¨ç±»å‹')
    parser.add_argument('--no_pruner', action='store_true',
                       help='ç¦ç”¨å‰ªæå™¨')
    
    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument('--target_channels', type=str, nargs='+',
                       default=['wsi', 'tma', 'clinical', 'pathological', 'blood', 'icd', 'tma_cell_density'],
                       help='ç›®æ ‡é€šé“ï¼ˆä½¿ç”¨ main.py çš„é€šé“æ ¼å¼ï¼‰')
    parser.add_argument('--aligned_channels', type=str, nargs='+', default=None,
                       help='å¯¹é½é€šé“ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--alignment_model_path', type=str, default=None,
                       help='å¯¹é½æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--fixed_test_split', type=str, default=None,
                       help='å›ºå®šæµ‹è¯•é›†åˆ†å‰²æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸ main.py ä¿æŒä¸€è‡´ï¼‰')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­ (default: 42)')
    
    # æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument('--input_dim', type=int, default=1024,
                       help='è¾“å…¥ç»´åº¦ (default: 1024)')
    parser.add_argument('--n_classes', type=int, default=2,
                       help='ç±»åˆ«æ•° (default: 2)')
    parser.add_argument('--base_loss_fn', type=str, choices=['svm', 'ce'], default='ce',
                       help='åŸºç¡€æŸå¤±å‡½æ•° (default: ce)')
    
    # å®æ—¶å¯è§†åŒ–å‚æ•°
    parser.add_argument('--enable_realtime_viz', action='store_true',
                       help='å¯ç”¨å®æ—¶å¯è§†åŒ–')
    parser.add_argument('--viz_port', type=int, default=8080,
                       help='å¯è§†åŒ–ç«¯å£ (default: 8080)')
    
    args = parser.parse_args()
    
    # è§£æé€šé“ï¼ˆä½¿ç”¨ main.py çš„ parse_channelsï¼‰
    target_channels = parse_channels(args.target_channels)
    
    # åŠ è½½å›ºå®šæµ‹è¯•é›†åˆ†å‰²ï¼ˆå¦‚æœæä¾›ï¼‰
    fixed_test_split = None
    if args.fixed_test_split:
        from main import load_dataset_split
        fixed_test_split = load_dataset_split(args.fixed_test_split)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = AUCCLAMOptimizer(
        data_root_dir=args.data_root_dir,
        csv_path=args.csv_path,
        model_type=args.model_type,
        results_dir=args.results_dir,
        n_trials=args.n_trials,
        n_jobs=args.n_jobs,
        timeout=args.timeout,
        pruner=not args.no_pruner,
        sampler=args.sampler,
        enable_realtime_viz=args.enable_realtime_viz,
        viz_port=args.viz_port,
        data_root_base=args.data_root_base,
        num_data_copies=args.num_data_copies,
        input_dim=args.input_dim,
        n_classes=args.n_classes,
        base_loss_fn=args.base_loss_fn,
        alignment_model_path=args.alignment_model_path,
        aligned_channels=args.aligned_channels,
        fixed_test_split=fixed_test_split,
        seed=args.seed
    )
    
    # æ‰§è¡Œä¼˜åŒ–
    study = optimizer.optimize(
        target_channels=target_channels,
        n_folds=args.n_folds,
        study_name=args.study_name
    )
    
    print("\nğŸ¯ ä¼˜åŒ–å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.results_dir}")

if __name__ == "__main__":
    main()
