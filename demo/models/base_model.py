"""
Base model class
Define unified model interface and return format
"""

import torch
import torch.nn as nn
from abc import ABC, abstractmethod
from typing import Dict, Any, Union


class BaseModel(nn.Module, ABC):
    """
    Base model abstract class

    Define unified model interface, all models should inherit from this class and implement unified forward method
    Unified return format facilitates unified processing of training loops
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize base model

        Args:
            config: Model configuration dictionary
        """
        super().__init__()
        self.config = config
        self.device = config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')
        self.input_dim = config.get('input_dim', 1024)
        self.dropout = config.get('dropout', 0.25)
        self.n_classes = config.get('n_classes', 2)
        if config.get('base_loss_fn') is None or config.get('base_loss_fn') == 'ce':
            self.base_loss_fn = nn.CrossEntropyLoss()
        elif config.get('base_loss_fn') == 'svm':
            self.base_loss_fn = nn.SmoothTop1SVM(n_classes=self.n_classes)
        else:
            raise ValueError(f"Unsupported base loss function: {config.get('base_loss_fn')}")
    
    @abstractmethod
    def forward(self, input_data: Union[torch.Tensor, Dict[str, torch.Tensor]], **kwargs) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„å‰å‘ä¼ æ’­æ¥å£
        
        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œå¯ä»¥æ˜¯ï¼š
                - torch.Tensor: å•æ¨¡æ€ç‰¹å¾ [N, D]
                - Dict[str, torch.Tensor]: å¤šæ¨¡æ€æ•°æ®å­—å…¸ï¼Œå¦‚ {"features": tensor, "aligned_features": tensor}
            **kwargs: å…¶ä»–å‚æ•°ï¼Œæ”¯æŒï¼š
                - label: æ ‡ç­¾ï¼ˆç”¨äºå®ä¾‹è¯„ä¼°ï¼‰
                - instance_eval: æ˜¯å¦è¿›è¡Œå®ä¾‹è¯„ä¼°
                - return_features: æ˜¯å¦è¿”å›ç‰¹å¾
                - attention_only: æ˜¯å¦åªè¿”å›æ³¨æ„åŠ›æƒé‡
                - å…¶ä»–æ¨¡å‹ç‰¹å®šå‚æ•°
                
        Returns:
            Dict[str, Any]: ç»Ÿä¸€çš„ç»“æœå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
                - 'logits': æ¨¡å‹è¾“å‡ºlogits [1, n_classes] æˆ– [N, n_classes]
                - 'probabilities': é¢„æµ‹æ¦‚ç‡ [1, n_classes] æˆ– [N, n_classes]  
                - 'predictions': é¢„æµ‹ç±»åˆ« [1] æˆ– [N]
                - 'attention_weights': æ³¨æ„åŠ›æƒé‡ï¼ˆå¦‚æœé€‚ç”¨ï¼‰[1, N] æˆ– [n_classes, N]
                - 'features': ç‰¹å¾è¡¨ç¤ºï¼ˆå¦‚æœreturn_features=Trueï¼‰
                - 'additional_loss': é¢å¤–æŸå¤±å€¼ï¼ˆå¦‚æœè®¡ç®—äº†é¢å¤–æŸå¤±ï¼‰
                - å…¶ä»–æ¨¡å‹ç‰¹å®šè¾“å‡º
        """
        pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        return {
            'model_type': self.__class__.__name__,
            'input_dim': self.input_dim,
            'dropout': self.dropout,
            'n_classes': self.n_classes,
            'base_loss_fn': self.base_loss_fn,
            'total_parameters': sum(p.numel() for p in self.parameters()),
            'trainable_parameters': sum(p.numel() for p in self.parameters() if p.requires_grad)
        }
    
    @abstractmethod
    def _process_input_data(self, input_data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        å¤„ç†è¾“å…¥æ•°æ®ï¼Œå°†å¤šæ¨¡æ€æ•°æ®è½¬æ¢ä¸ºç»Ÿä¸€çš„å¼ é‡æ ¼å¼
        
        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œå¯ä»¥æ˜¯å¼ é‡æˆ–å­—å…¸
            
        Returns:
            torch.Tensor: å¤„ç†åçš„ç‰¹å¾å¼ é‡ [N, D]
        """
        pass
    
    def _create_result_dict(self, 
                          logits: torch.Tensor,
                          probabilities: torch.Tensor, 
                          predictions: torch.Tensor,
                          **kwargs) -> Dict[str, Any]:
        """
        åˆ›å»ºç»Ÿä¸€çš„ç»“æœå­—å…¸
        
        Args:
            logits: æ¨¡å‹è¾“å‡ºlogits [1, n_classes] æˆ– [N, n_classes]
            probabilities: é¢„æµ‹æ¦‚ç‡ [1, n_classes] æˆ– [N, n_classes]
            predictions: é¢„æµ‹ç±»åˆ« [1] æˆ– [N]
            **kwargs: å…¶ä»–è¾“å‡ºï¼Œå¦‚ï¼š
                - attention_weights: æ³¨æ„åŠ›æƒé‡ [1, N] æˆ– [n_classes, N]
                - features: ç‰¹å¾è¡¨ç¤º [1, D] æˆ– [N, D]
                - additional_loss: é¢å¤–æŸå¤±å€¼ [1] æˆ– [N]
                - å…¶ä»–æ¨¡å‹ç‰¹å®šè¾“å‡º
                
        Returns:
            Dict[str, Any]: ç»Ÿä¸€æ ¼å¼çš„ç»“æœå­—å…¸
        """
        result = {
            'logits': logits,
            'probabilities': probabilities,
            'predictions': predictions
        }
        
        # æ·»åŠ æ‰€æœ‰å…¶ä»–è¾“å‡º
        # ğŸ”’ ç¡®ä¿å­—å…¸é”®é¡ºåºç¡®å®šæ€§ï¼šä½¿ç”¨ sorted() å¯¹é”®è¿›è¡Œæ’åº
        for key, value in sorted(kwargs.items()):
            if value is not None:
                result[key] = value
        
        return result
    
    @abstractmethod
    def loss_fn(self, logits: torch.Tensor, labels: torch.Tensor, result: Dict[str, float]) -> torch.Tensor:
        """
        è®¡ç®—æŸå¤±
        
        Args:
            logits: é¢„æµ‹çš„logits [N, C]
            labels: çœŸå®æ ‡ç­¾ [N]
            result: ç»“æœå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
        """
        pass
