#!/usr/bin/env python3
"""
å¤šæ¨¡æ€å¯¹é½è®­ç»ƒè„šæœ¬
ä½¿ç”¨UNIæ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾å¯¹é½
"""

import torch
from torch.utils.data import DataLoader
import numpy as np
import argparse
import logging
import json
from pathlib import Path

# å¯¼å…¥æ¨¡å‹å’Œè®­ç»ƒå™¨
from alignment_model import MultiModalAlignmentModel
from trainer import MultiModalAlignmentTrainer
from alignment_dataset import (
    create_tma_aligned_with_neg_dataset,
    build_collate_fn,
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

tma_dir = "/home/zheng/zheng/mini2/hancock_data/TMA/TMA_Core_encodings"
modality_names = ["CD3", "CD8", "CD56", "CD68", "CD163", "HE", "MHC1", "PDL1"]
feature_dim = 1024

def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    """
    parser = argparse.ArgumentParser(description="å¤šæ¨¡æ€å¯¹é½è®­ç»ƒ")
    parser.add_argument("--align_mode", type=str, default="intersection",
                       help="å¯¹é½æ¨¡å¼")
    parser.add_argument("--pattern", type=str, default="tma_uni_tile_1024_{marker}.npz",
                       help="æ–‡ä»¶ååŒ¹é…æ¨¡å¼ï¼Œä½¿ç”¨ {marker} ä½œä¸ºå ä½ç¬¦ï¼Œä¾‹å¦‚: 'tma_uni_tile_1024_{marker}.npz'")
    parser.add_argument("--mismatch_ratio", type=float, default=1.0,
                       help="è´Ÿæ ·æœ¬æ± å¤§å°ç³»æ•°")
    parser.add_argument("--seed", type=int, default=42,
                       help="è´Ÿæ ·æœ¬æ± éšæœºç§å­")
    parser.add_argument("--lambda1", type=float, default=1.0,
                       help="labmda1")
    parser.add_argument("--lambda2", type=float, default=0.1,
                       help="lambda2")
    parser.add_argument("--tau1", type=float, default=0.1,
                       help="tau1")
    parser.add_argument("--tau2", type=float, default=0.05,
                       help="tau2")
    parser.add_argument("--learning_rate", type=float, default=1e-4,
                       help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-5,
                       help="æƒé‡è¡°å‡")
    parser.add_argument("--max_steps", type=int, default=100000,
                       help="æœ€å¤§è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--batch_size", type=int, default=128,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--save_path", type=str, default="best_multimodal_alignment_model.pth",
                       help="æ¨¡å‹ä¿å­˜è·¯å¾„")
    parser.add_argument("--num_workers", type=int, default=4, help="DataLoader workers")
    parser.add_argument("--log_interval", type=int, default=100, help="æ—¥å¿—è®°å½•é—´éš”ï¼ˆæ­¥æ•°ï¼‰")
    parser.add_argument("--val_interval", type=int, default=500, help="éªŒè¯é—´éš”ï¼ˆæ­¥æ•°ï¼‰")
    parser.add_argument("--num_layers", type=int, default=1, 
                       help="å¯¹é½å±‚çš„å±‚æ•°ï¼Œé»˜è®¤ä¸º1ï¼ˆå•å±‚çº¿æ€§å˜æ¢ï¼‰")
    parser.add_argument("--val_max_batches", type=int, default=None, help="éªŒè¯æœ€å¤šæ‰¹æ¬¡æ•°")
    parser.add_argument("--loss_type", type=str, default="volume", help="æŸå¤±ç±»å‹ volume, rank1")
    parser.add_argument("--loss2_chunk_size", type=int, default=None, help="loss2 åˆ†å—å¤§å°ï¼ˆè¡Œå—å°ºå¯¸ï¼‰")
    parser.add_argument("--verbose_timing", action="store_true", help="å¯ç”¨è¯¦ç»†æ€§èƒ½åˆ†æï¼ˆé»˜è®¤å…³é—­ï¼‰")
    parser.add_argument("--early_stopping_patience", type=int, default=10, help="Early stoppingè€å¿ƒå€¼ï¼ˆéªŒè¯lossä¸æ”¹å–„çš„æ­¥æ•°ï¼Œ0è¡¨ç¤ºç¦ç”¨ï¼‰")
    parser.add_argument("--early_stopping_min_delta", type=float, default=1e-4, help="Early stoppingæœ€å°æ”¹å–„é˜ˆå€¼")
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºå¤šæ¨¡æ€å¯¹é½æ¨¡å‹
    logger.info(f"ğŸ—ï¸ åˆ›å»ºå¤šæ¨¡æ€å¯¹é½æ¨¡å‹...")
    
    model = MultiModalAlignmentModel(
        modality_names=modality_names,
        feature_dim=feature_dim,
        num_layers=args.num_layers
    )
    
    trainer = MultiModalAlignmentTrainer(
        model=model,
        device=device,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        loss_type=args.loss_type,
        val_max_batches=args.val_max_batches,
        loss2_chunk_size=args.loss2_chunk_size,
        verbose_timing=args.verbose_timing,
        early_stopping_patience=args.early_stopping_patience,
        early_stopping_min_delta=args.early_stopping_min_delta,
    )
    
    # åˆ›å»ºå¸¦è´Ÿæ ·æœ¬æ± çš„æ•°æ®é›†
    logger.info("ğŸ“Š åŠ è½½ TMA æ•°æ®é›†ï¼ˆå¸¦å…¨å±€è´Ÿæ ·æœ¬æ± ï¼‰...")
    logger.info(f"   - æ–‡ä»¶åŒ¹é…æ¨¡å¼: {args.pattern}")
    base_ds = create_tma_aligned_with_neg_dataset(
        base_dir=tma_dir,
        modality_names=modality_names,
        align_mode=args.align_mode,
        filename_template=args.pattern,
        mismatch_ratio=args.mismatch_ratio,
        seed=args.seed,
    )

    # ä½¿ç”¨ tuple è¿›è¡Œåˆ‡åˆ†ï¼ˆç¤ºä¾‹ï¼šè¿™é‡Œéšæœºåˆ’åˆ† 8:1:1ï¼Œä½ ä¹Ÿå¯ä»¥é¢„å…ˆç”Ÿæˆ ids å¹¶ä¼ å…¥ï¼‰
    # ç°åœ¨ normalized_keys æ˜¯ 5 ç»´çš„ (block, x, y, patient, patch_id)
    all_tuples = base_ds.normalized_keys
    rng = np.random.RandomState(args.seed)
    idx = np.arange(len(all_tuples))
    rng.shuffle(idx)
    n = len(idx)
    n_train = int(n * 0.8)
    n_val = int(n * 0.1)
    train_ids = [all_tuples[i] for i in idx[:n_train]]
    val_ids = [all_tuples[i] for i in idx[n_train:n_train+n_val]]
    test_ids = [all_tuples[i] for i in idx[n_train+n_val:]]

    splits = base_ds.split_by_ids_with_neg(
        {
            'train': train_ids,
            'val': val_ids,
            'test': test_ids,
        },
        id_type='tuple',  # ä½¿ç”¨å®Œæ•´é”® (block, x, y, patient, patch_id) è¿›è¡Œåˆ‡åˆ†
        mismatch_ratio=args.mismatch_ratio,
        seed=args.seed,
    )

    train_ds = splits['train']
    val_ds = splits['val']
    test_ds = splits['test']

    # åˆ›å»º collate_fnï¼ˆè´Ÿæ ·æœ¬æ•° = ceil(batch_size * mismatch_ratio)ï¼‰
    train_collate = build_collate_fn(train_ds, ratio=args.mismatch_ratio)
    val_collate = build_collate_fn(val_ds, ratio=args.mismatch_ratio)
    test_collate = build_collate_fn(test_ds, ratio=args.mismatch_ratio)

    train_loader = DataLoader(
        train_ds, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=train_collate
    )
    val_loader = DataLoader(
        val_ds, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=args.num_workers,
        collate_fn=val_collate
    )
    test_loader = DataLoader(
        test_ds, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=args.num_workers,
        collate_fn=test_collate
    )
    
    logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_loader.dataset)} | éªŒè¯: {len(val_loader.dataset)} | æµ‹è¯•: {len(test_loader.dataset)}")
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆStep æ¨¡å¼ï¼‰
    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    history = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        max_steps=args.max_steps,
        save_path=args.save_path,
        log_interval=args.log_interval,
        val_interval=args.val_interval,
    )
    
    logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼è®°å½•äº† {len(history['train_losses'])} æ­¥çš„è®­ç»ƒæ•°æ®")
    
    # ğŸ’¾ ä¿å­˜è®­ç»ƒå†å²
    history_save_path = Path(args.save_path).with_suffix('.history.json')
    logger.info(f"ğŸ’¾ ä¿å­˜è®­ç»ƒå†å²åˆ°: {history_save_path}")
    
    # è½¬æ¢ tensor ä¸º list ä»¥ä¾¿ JSON åºåˆ—åŒ–
    history_to_save = {
        'train_losses': history['train_losses'],
        'val_losses': history['val_losses'],
        'val_steps': history['val_steps'],
        'train_svd_values': [svd.cpu().tolist() for svd in history['train_svd_values']],
        'val_svd_values': [svd.cpu().tolist() for svd in history['val_svd_values']],
        'config': {
            'max_steps': args.max_steps,
            'batch_size': args.batch_size,
            'learning_rate': args.learning_rate,
            'weight_decay': args.weight_decay,
            'lambda1': args.lambda1,
            'lambda2': args.lambda2,
            'tau1': args.tau1,
            'tau2': args.tau2,
            'mismatch_ratio': args.mismatch_ratio,
            'pattern': args.pattern,
            'align_mode': args.align_mode,
            'seed': args.seed,
            'num_layers': args.num_layers,
            'loss_type': args.loss_type,
        }
    }
    
    with open(history_save_path, 'w') as f:
        json.dump(history_to_save, f, indent=2)
    
    logger.info(f"âœ… è®­ç»ƒå†å²å·²ä¿å­˜: {history_save_path}")


if __name__ == "__main__":
    main()
