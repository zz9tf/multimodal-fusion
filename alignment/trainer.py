"""
å¤šæ¨¡æ€å¯¹é½è®­ç»ƒå™¨æ¨¡å—
ä»UNI_finetune.pyä¸­åˆ†ç¦»å‡ºæ¥çš„è®­ç»ƒå™¨ç±»
"""

import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Tuple
import logging
from tqdm import tqdm
import torch.nn.functional as F
import time
import numpy as np

# å¯¼å…¥æ¨¡å‹
from alignment_model import MultiModalAlignmentModel

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MultiModalAlignmentTrainer:
    """
    å¤šæ¨¡æ€å¯¹é½è®­ç»ƒå™¨ - ç”¨äºå¤šä¸ªmodalityä¹‹é—´çš„alignment
    """
    
    def __init__(self, 
                 model: MultiModalAlignmentModel,
                 device: str = "cuda",
                 learning_rate: float = 1e-4,
                 weight_decay: float = 1e-5,
                 loss_type: str = "rank1",
                 tau1: float = 0.1,
                 tau2: float = 0.1,
                 lambda1: float = 1.0,
                 lambda2: float = 0.1,
                 mismatch_ratio: float = 1.0,
                 modality_names_for_mismatch: Optional[List[str]] = None,
                 val_max_batches: Optional[int] = None,
                 loss2_chunk_size: Optional[int] = None,
                 verbose_timing: bool = False,
                 early_stopping_patience: int = 10,
                 early_stopping_min_delta: float = 1e-4):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: å¤šæ¨¡æ€å¯¹é½æ¨¡å‹
            device: è®­ç»ƒè®¾å¤‡
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            loss_type: æŸå¤±å‡½æ•°ç±»å‹ï¼Œ"volume", "rank1"ï¼Œé»˜è®¤ "rank1"
            tau1: æ¸©åº¦å‚æ•°
            tau2: æ¸©åº¦å‚æ•°
            lambda1: æŸå¤±å‡½æ•°å‚æ•°
            lambda2: æŸå¤±å‡½æ•°å‚æ•°
            mismatch_ratio: mismatchä¸matchçš„æ¯”ä¾‹ï¼Œ1.0è¡¨ç¤º1:1ï¼Œ2.0è¡¨ç¤º2:1
            modality_names_for_mismatch: å¯é€‰ï¼ŒæŒ‡å®šç”¨äºmismatchçš„æ¨¡æ€åç§°ï¼Œé»˜è®¤ä½¿ç”¨ model.modality_names
            val_max_batches: éªŒè¯æ—¶æœ€å¤§æ‰¹æ¬¡æ•°
            loss2_chunk_size: loss2åˆ†å—å¤§å°
            verbose_timing: æ˜¯å¦å¯ç”¨è¯¦ç»†æ€§èƒ½åˆ†æ
            early_stopping_patience: early stoppingçš„è€å¿ƒå€¼ï¼ˆéªŒè¯lossä¸æ”¹å–„çš„æ­¥æ•°ï¼‰
            early_stopping_min_delta: early stoppingçš„æœ€å°æ”¹å–„é˜ˆå€¼
        """
        self.model = model.to(device)
        self.device = device
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.tau1 = tau1
        self.tau2 = tau2
        self.lambda1 = lambda1
        self.lambda2 = lambda2
        self.mismatch_ratio = mismatch_ratio
        self.modality_names_for_mismatch = modality_names_for_mismatch or model.modality_names
        self.val_max_batches = val_max_batches
        self.loss2_chunk_size = loss2_chunk_size
        self.verbose_timing = verbose_timing
        self.early_stopping_patience = early_stopping_patience
        self.early_stopping_min_delta = early_stopping_min_delta
        
        # Early stopping ç›¸å…³çŠ¶æ€
        self.best_val_loss = float('inf')
        self.early_stopping_counter = 0
        self.early_stopping_triggered = False
        
        # æ€§èƒ½åˆ†æç›¸å…³ï¼ˆä»…åœ¨verbose_timing=Trueæ—¶å¯ç”¨ï¼‰
        if self.verbose_timing:
            self.timing_stats = {
                'data_loading': [],
                'forward_pass': [],
                'loss_computation': [],
                'loss1_computation': [],
                'loss2_computation': [],
                'loss3_computation': [],
                'backward_pass': [],
                'optimizer_step': [],
                'total_step': []
            }
        else:
            self.timing_stats = None
        
        if loss_type not in ["volume", "rank1"]:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±ç±»å‹: {loss_type}ï¼Œæ”¯æŒçš„ç±»å‹: 'volume', 'rank1'")
        self.loss_type = loss_type
        
        alignment_params = []
        for modality_name in model.modality_names:
            alignment_params.extend(list(model.alignment_layers[modality_name].parameters()))
        
        self.optimizer = optim.AdamW(
            alignment_params,
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100, eta_min=1e-6
        )
        
        logger.info(f"âœ… å¤šæ¨¡æ€å¯¹é½è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - è®¾å¤‡: {device}")
        logger.info(f"   - å­¦ä¹ ç‡: {learning_rate}")
        logger.info(f"   - æƒé‡è¡°å‡: {weight_decay}")
        logger.info(f"   - ä¼˜åŒ–å™¨: AdamW")
        logger.info(f"   - è°ƒåº¦å™¨: CosineAnnealingLR")
        logger.info(f"   - æŸå¤±å‡½æ•°: {loss_type.upper()} Loss(rank1)")
        logger.info(f"   - Mismatchæ¯”ä¾‹: {mismatch_ratio}:1")
        logger.info(f"   - æ¨¡æ€æ•°é‡: {model.num_modalities}")
        logger.info(f"   - æ¨¡æ€åç§°: {model.modality_names}")
        if self.val_max_batches is not None:
            logger.info(f"   - æ¯æ¬¡éªŒè¯æœ€å¤šæ‰¹æ¬¡æ•°: {self.val_max_batches}")
        if self.loss2_chunk_size is not None:
            logger.info(f"   - loss2 åˆ†å—å¤§å°: {self.loss2_chunk_size}")
        if self.verbose_timing:
            logger.info(f"   - è¯¦ç»†æ€§èƒ½åˆ†æ: å¯ç”¨")
        if self.early_stopping_patience > 0:
            logger.info(f"   - Early Stopping: å¯ç”¨ (patience={self.early_stopping_patience}, min_delta={self.early_stopping_min_delta})")
    
    def _compute_loss_with_metrics(self, aligned_features: Dict[str, torch.Tensor], 
                                    aligned_negatives: Optional[Dict[str, torch.Tensor]] = None):
        """
        è®¡ç®—æŸå¤±å¹¶è¿”å› SVD ç‰¹å¾å€¼ç”¨äºç›‘æ§
        
        Returns:
            loss: æŸå¤±å€¼
            svd_values: SVD ç‰¹å¾å€¼ Tensor[num_modalities]ï¼Œå¦‚æœä¸é€‚ç”¨åˆ™è¿”å› None
        """
        if self.loss_type == "volume":
            return self._compute_volume_loss_with_metrics(aligned_features)
        elif self.loss_type == "rank1":
            return self._compute_rank1_loss_with_metrics(aligned_features, aligned_negatives)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±ç±»å‹: {self.loss_type}")
        
    def _volume_computation(self, language, *inputs):
        """
        General function to compute volume for contrastive learning loss functions.
        Compute the volume metric for each vector in language batch and all the other modalities listed in *inputs.

        Args:
        - language (torch.Tensor): Tensor of shape (batch_size1, dim)
        - *inputs (torch.Tensor): Variable number of tensors of shape (batch_size2, dim)

        Returns:
        - torch.Tensor: Tensor of shape (batch_size1, batch_size2) representing the volume for each pair.
        """
        batch_size1 = language.shape[0]
        batch_size2 = inputs[0].shape[0]

        # Compute pairwise dot products for language with itself
        ll = torch.einsum('bi,bi->b', language, language).unsqueeze(1).expand(-1, batch_size2)

        # Compute pairwise dot products for language with each input
        l_inputs = [language @ input.T for input in inputs]

        # Compute pairwise dot products for each input with themselves and with each other
        input_dot_products = []
        for i, input1 in enumerate(inputs):
            row = []
            for j, input2 in enumerate(inputs):
                dot_product = torch.einsum('bi,bi->b', input1, input2).unsqueeze(0).expand(batch_size1, -1)
                row.append(dot_product)
            input_dot_products.append(row)

        # Stack the results to form the Gram matrix for each pair
        G = torch.stack([
            torch.stack([ll] + l_inputs, dim=-1),
            *[torch.stack([l_inputs[i]] + input_dot_products[i], dim=-1) for i in range(len(inputs))]
        ], dim=-2)
        
        evals = torch.linalg.eigvalsh(G.to(torch.float64))        # [B1,B2,K], å‡åº
        evals = evals.clamp_min(0.0).to(G.dtype)

        # Compute the determinant for each Gram matrix
        gram_det = torch.det(G.float())

        # Compute the square root of the absolute value of the determinants
        res = torch.sqrt(torch.abs(gram_det))
        return res, evals
    
    def _compute_volume_loss_with_metrics(self, aligned_features: dict):
        """
        aligned_features: dict[str, Tensor]ï¼Œæ¯ä¸ª Tensor å½¢çŠ¶ [B, D]ï¼Œç´¢å¼• i å¤„å„æ¨¡æ€ä¸€ä¸€å¯¹åº”
        è¿”å›: vol: [B]ï¼ˆæ¯ä¸ªæ ·æœ¬ä¸€ä¸ª ä½“ç§¯ æˆ– log-ä½“ç§¯ï¼‰, svd_values: [K]
        """
        feature_list = list(aligned_features.values())
        volume, evals = self._volume_computation(feature_list[0], *feature_list[1:])
        # 2) æ¸©åº¦ç¼©æ”¾å¹¶å–è´Ÿå·ä½œä¸º logitsï¼ˆè¶Šå°è¶Šå¥½äº¤ç»™ CEï¼‰
        logits_ab = - volume / self.tau1                        # anchor->others
        logits_ba = - volume.t() / self.tau1                    # others->anchor

        # 3) ç›®æ ‡æŒ‡å®šå¯¹è§’
        B = volume.size(0)
        targets = torch.arange(B, device=volume.device)

        # 4) äº¤å‰ç†µï¼ˆå¯å¸¦ label_smoothingï¼‰
        loss = (F.cross_entropy(logits_ab, targets, label_smoothing=0.1) \
            + F.cross_entropy(logits_ba, targets, label_smoothing=0.1)) / 2
        loss = loss.mean()
        
        # å°†ç‰¹å¾å€¼ä» [B1, B2, K] æŒ‰æ‰¹ç»´èšåˆä¸º [K] å¹¶æŒ‰é™åºæ’åºï¼ˆç”¨äºæ—¥å¿—å±•ç¤ºï¼‰
        svd_values, _ = torch.sort(evals.mean(dim=(0, 1)).detach(), descending=True)
        return loss, svd_values
    
    def _compute_rank1_loss_with_metrics(self, aligned_features: Dict[str, torch.Tensor], 
                                          aligned_negatives: Optional[Dict[str, torch.Tensor]] = None):
        """
        è®¡ç®— rank1 æŸå¤±å¹¶è¿”å› SVD ç‰¹å¾å€¼ï¼ˆå¸¦è¯¦ç»†æ—¶é—´åˆ†æï¼‰
        
        Returns:
            loss: æŸå¤±å€¼
            svd_values: SVD ç‰¹å¾å€¼ Tensor[num_modalities]
        """
        # 1. SVD è®¡ç®—å’Œ loss1
        loss1_start = time.perf_counter() if self.verbose_timing else None
        
        feature_list = list(aligned_features.values())
        features = torch.stack(feature_list, dim=-1)  # [batch_size, feature_dim, num_modalities]
        
        # L2 å½’ä¸€åŒ–ï¼šx <- x / (||x||_2 + Îµ)
        eps = 1e-8
        l2_norm = torch.norm(features, p=2, dim=1, keepdim=True)  # [batch_size, 1, num_modalities]
        features = features / (l2_norm + eps)
        
        # U: [batch_size, feature_dim, num_modalities]
        # S(diag): [batch_size, num_modalities]
        # _: [batch_size, feature_dim, num_modalities]
        U, S, _ = torch.linalg.svd(features)
        
        # ğŸ“Š è®°å½• SVD ç‰¹å¾å€¼ï¼šå¯¹ batch ç»´åº¦æ±‚å¹³å‡ï¼ˆç”¨äºè®°å½•å•ä¸ª batch çš„ä»£è¡¨å€¼ï¼‰
        svd_values = S.mean(dim=0).detach()  # [num_modalities]
        
        loss1 = F.cross_entropy(S / self.tau1, torch.zeros(S.shape[0]).to(S.device).long())
        
        if self.verbose_timing:
            loss1_time = time.perf_counter() - loss1_start
            self.timing_stats['loss1_computation'].append(loss1_time)
        
        # 2. loss2 è®¡ç®—
        loss2_start = time.perf_counter() if self.verbose_timing else None
        
        U1 = U[:, :, 0] # dominate projection [batch_size, feature_dim]
        # ç»„å†…çŸ©é˜µè®¡ç®—ï¼šæŒ‰ loss2_chunk_size å°† batch åˆ†ç»„ï¼Œä»…ç»„å†…åš softmax/CE
        batch_count = U1.shape[0]
        if self.loss2_chunk_size is None or self.loss2_chunk_size >= batch_count:
            loss2 = F.cross_entropy((U1 @ U1.T) / self.tau2, torch.arange(batch_count, device=U1.device).long())
        else:
            c = max(1, int(self.loss2_chunk_size))
            full = (batch_count // c) * c
            loss2_sum = U1.new_tensor(0.0)
            if full > 0:
                groups = U1[:full].view(-1, c, U1.shape[1])  # [G, c, D]
                logits_gc = torch.einsum('gxd,gyd->gxy', groups, groups) / self.tau2  # [G, c, c]
                targets_gc = torch.arange(c, device=U1.device).expand(logits_gc.shape[0], c) # [G, c]
                loss2_sum = loss2_sum + F.cross_entropy(
                    logits_gc.reshape(-1, c), targets_gc.reshape(-1), reduction='sum'
                )
            if full < batch_count:
                tail = U1[full:]
                c_tail = tail.shape[0]
                logits_tail = (tail @ tail.T) / self.tau2
                targets_tail = torch.arange(c_tail, device=U1.device)
                loss2_sum = loss2_sum + F.cross_entropy(logits_tail, targets_tail, reduction='sum')
            loss2 = loss2_sum / batch_count

        if self.verbose_timing:
            loss2_time = time.perf_counter() - loss2_start
            self.timing_stats['loss2_computation'].append(loss2_time)

        if self.lambda2 == 0:
            return loss1 + self.lambda1 * loss2, svd_values

        # 3. loss3 (loss_IM) è®¡ç®—
        loss3_start = time.perf_counter() if self.verbose_timing else None
        
        batch_size = feature_list[0].shape[0]
        positive_labels = torch.ones(batch_size, device=self.device)
        
        def fuse(feat_dict: Dict[str, torch.Tensor]) -> torch.Tensor:
            # å°†å¤šæ¨¡æ€ç‰¹å¾æ‹¼æ¥ä¸ºå•å‘é‡ [N, d*K]
            return torch.cat(list(feat_dict.values()), dim=1)

        if aligned_negatives is None:
            raise RuntimeError("Negative features not provided by dataset. Ensure DataLoader yields 'features_neg' per batch.")
        neg_fused = fuse(aligned_negatives)

        pos_fused = fuse(aligned_features)
        all_features = torch.cat([pos_fused, neg_fused], dim=0)
        negative_labels = torch.zeros(neg_fused.shape[0], device=self.device)
        all_labels = torch.cat([positive_labels, negative_labels], dim=0)

        pred_M = self.model.mlp_predictor(all_features)
        loss_IM = F.binary_cross_entropy(pred_M.squeeze(), all_labels)
        
        if self.verbose_timing:
            loss3_time = time.perf_counter() - loss3_start
            self.timing_stats['loss3_computation'].append(loss3_time)
        
        total_loss = loss1 + self.lambda1 * loss2 + self.lambda2 * loss_IM
        return total_loss, svd_values
    
    def _get_next_batch(self, train_iter, train_loader):
        """è·å–ä¸‹ä¸€ä¸ªbatchï¼Œå¦‚æœdataloaderç”¨å®Œåˆ™é‡æ–°å¼€å§‹"""
        try:
            batch = next(train_iter)
        except StopIteration:
            train_iter = iter(train_loader)
            batch = next(train_iter)
            self.scheduler.step()  # æ¯ä¸ªepochç»“æŸæ›´æ–°å­¦ä¹ ç‡
        return batch
    
    def _update_progress_bar(self, progress_bar, loss: float, svd_values: Optional[torch.Tensor]):
        """æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºï¼ˆå½“å‰ batch çš„åŸå§‹å€¼ï¼‰"""
        postfix_dict = {'loss': f'{loss:.4f}'}
        if svd_values is not None:
            # æ˜¾ç¤ºå‰3ä¸ªç‰¹å¾å€¼ï¼ˆå·²ç»æ˜¯ batch å†…å¹³å‡è¿‡çš„å€¼ï¼‰
            for i in range(min(3, len(svd_values))):
                postfix_dict[f'Ïƒ{i+1}'] = f'{svd_values[i].item():.3f}'
        progress_bar.set_postfix(postfix_dict)
        progress_bar.update(1)
    
    def _run_validation(self, val_loader: DataLoader, val_max_batches: Optional[int] = None) -> Tuple[float, Optional[torch.Tensor]]:
        """
        è¿è¡ŒéªŒè¯å¹¶è¿”å›å¹³å‡losså’Œå¹³å‡SVDå€¼
        
        Returns:
            (avg_loss, avg_svd): å¹³å‡éªŒè¯æŸå¤±å’Œå¹³å‡SVDç‰¹å¾å€¼
        """
        self.model.eval()
        val_result = self._validate_full(val_loader, val_max_batches)
        
        self.model.train()
        
        # è®¡ç®—å¹³å‡ loss
        avg_loss = sum(val_result['batch_losses']) / len(val_result['batch_losses'])
        
        # è®¡ç®—å¹³å‡ SVD å€¼
        avg_svd = None
        if val_result['batch_svd_values']:
            avg_svd = torch.stack(val_result['batch_svd_values']).mean(dim=0)
        
        return avg_loss, avg_svd
    
    def _save_checkpoint(self, save_path: str, step: int, val_loss: float):
        """ä¿å­˜æ¨¡å‹checkpoint"""
        torch.save({
            'step': step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
        }, save_path)
        logger.info(f"âœ… [Step {step}] ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss: {val_loss:.4f})")
    
    def _check_early_stopping(self, val_loss: float) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥early stopping
        
        Args:
            val_loss: å½“å‰éªŒè¯æŸå¤±
            
        Returns:
            bool: Trueè¡¨ç¤ºåº”è¯¥early stoppingï¼ŒFalseè¡¨ç¤ºç»§ç»­è®­ç»ƒ
        """
        if self.early_stopping_patience <= 0:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
        if val_loss < self.best_val_loss - self.early_stopping_min_delta:
            # æœ‰æ”¹å–„ï¼Œé‡ç½®è®¡æ•°å™¨
            self.best_val_loss = val_loss
            self.early_stopping_counter = 0
            logger.info(f"ğŸ¯ [Early Stop] éªŒè¯æŸå¤±æ”¹å–„: {val_loss:.4f} (æœ€ä½³: {self.best_val_loss:.4f})")
            return False
        else:
            # æ²¡æœ‰æ”¹å–„ï¼Œå¢åŠ è®¡æ•°å™¨
            self.early_stopping_counter += 1
            logger.info(f"â³ [Early Stop] éªŒè¯æŸå¤±æ— æ”¹å–„: {val_loss:.4f} (æœ€ä½³: {self.best_val_loss:.4f}, è®¡æ•°: {self.early_stopping_counter}/{self.early_stopping_patience})")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°patience
            if self.early_stopping_counter >= self.early_stopping_patience:
                self.early_stopping_triggered = True
                logger.info(f"ğŸ›‘ [Early Stop] è§¦å‘æ—©åœï¼éªŒè¯æŸå¤±è¿ç»­ {self.early_stopping_patience} æ¬¡éªŒè¯æ— æ”¹å–„")
                return True
        
        return False
    
    def _log_progress(self, global_step: int, max_steps: int, log_interval: int, history: Dict):
        """æ‰“å°è®­ç»ƒè¿›åº¦æ—¥å¿—"""
        # è®¡ç®—æœ€è¿‘ log_interval æ­¥çš„å¹³å‡
        recent_losses = history['train_losses'][-log_interval:]
        avg_loss = sum(recent_losses) / len(recent_losses)
        
        log_msg = f"Step {global_step}/{max_steps} | Loss: {avg_loss:.4f}"
        
        # SVD å€¼
        if history['train_svd_values']:
            recent_svd = history['train_svd_values'][-log_interval:]
            if recent_svd:
                avg_svd = torch.stack(recent_svd).mean(dim=0)
                svd_msg = [f"Ïƒ{i+1}={avg_svd[i].item():.3f}" for i in range(min(3, len(avg_svd)))]
                log_msg += f" | SVD: [{', '.join(svd_msg)}]"
        
        # æœ€è¿‘çš„éªŒè¯loss
        if history['val_losses']:
            log_msg += f" | Val: {history['val_losses'][-1]:.4f}"
        
        logger.info(log_msg)
    
    def _train_step(self, batch) -> Tuple[float, Optional[torch.Tensor]]:
        """
        æ‰§è¡Œå•ä¸ªè®­ç»ƒæ­¥éª¤ï¼ˆå¯é€‰æ€§èƒ½åˆ†æï¼‰
        
        Args:
            batch: æ•°æ®æ‰¹æ¬¡
            
        Returns:
            (loss, svd_values): æŸå¤±å€¼å’ŒSVDç‰¹å¾å€¼
        """
        if self.verbose_timing:
            step_start = time.perf_counter()
            
            # 1. æ•°æ®åŠ è½½å’Œå¤„ç†
            data_start = time.perf_counter()
            if isinstance(batch, dict) and 'features' in batch:
                pos_dict = {k: v.to(self.device) for k, v in batch['features'].items()}
                neg_dict = {k: v.to(self.device) for k, v in batch.get('features_neg', {}).items()} if 'features_neg' in batch else None
                pos_mask = {k: v.to(self.device).float() for k, v in batch.get('mask', {}).items()} if 'mask' in batch else None
                neg_mask = {k: v.to(self.device).float() for k, v in batch.get('mask_neg', {}).items()} if 'mask_neg' in batch else None
            else:
                pos_dict = {k: v.to(self.device) for k, v in batch.items()}
                neg_dict = None
                pos_mask = None
                neg_mask = None
            data_time = time.perf_counter() - data_start
            
            self.optimizer.zero_grad()
            
            # 2. å‰å‘ä¼ æ’­
            forward_start = time.perf_counter()
            aligned_pos = self.model(pos_dict)
            if pos_mask is not None:
                for m, x in aligned_pos.items():
                    if m in pos_mask:
                        aligned_pos[m] = x * pos_mask[m].unsqueeze(1)
            
            # è´Ÿæ ·æœ¬
            if self.lambda2 == 0:
                aligned_neg = None
            else:
                if neg_dict is None:
                    raise RuntimeError("Dataset must provide features_neg for negative samples.")
                aligned_neg = self.model(neg_dict)
                if aligned_neg is not None and neg_mask is not None:
                    for m, x in aligned_neg.items():
                        if m in neg_mask:
                            aligned_neg[m] = x * neg_mask[m].unsqueeze(1)
            forward_time = time.perf_counter() - forward_start
            
            # 3. æŸå¤±è®¡ç®—
            loss_start = time.perf_counter()
            loss, svd_values = self._compute_loss_with_metrics(aligned_pos, aligned_negatives=aligned_neg)
            loss_time = time.perf_counter() - loss_start
            
            # 4. åå‘ä¼ æ’­
            backward_start = time.perf_counter()
            loss.backward()
            backward_time = time.perf_counter() - backward_start
            
            # 5. ä¼˜åŒ–å™¨æ­¥éª¤
            optimizer_start = time.perf_counter()
            self.optimizer.step()
            optimizer_time = time.perf_counter() - optimizer_start
            
            total_time = time.perf_counter() - step_start
            
            # è®°å½•æ—¶é—´ç»Ÿè®¡
            self.timing_stats['data_loading'].append(data_time)
            self.timing_stats['forward_pass'].append(forward_time)
            self.timing_stats['loss_computation'].append(loss_time)
            self.timing_stats['backward_pass'].append(backward_time)
            self.timing_stats['optimizer_step'].append(optimizer_time)
            self.timing_stats['total_step'].append(total_time)
            
            return loss.item(), svd_values
        else:
            # æ ‡å‡†è®­ç»ƒæ­¥éª¤ï¼ˆæ— æ€§èƒ½åˆ†æï¼‰
            if isinstance(batch, dict) and 'features' in batch:
                pos_dict = {k: v.to(self.device) for k, v in batch['features'].items()}
                neg_dict = {k: v.to(self.device) for k, v in batch.get('features_neg', {}).items()} if 'features_neg' in batch else None
                pos_mask = {k: v.to(self.device).float() for k, v in batch.get('mask', {}).items()} if 'mask' in batch else None
                neg_mask = {k: v.to(self.device).float() for k, v in batch.get('mask_neg', {}).items()} if 'mask_neg' in batch else None
            else:
                pos_dict = {k: v.to(self.device) for k, v in batch.items()}
                neg_dict = None
                pos_mask = None
                neg_mask = None
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            aligned_pos = self.model(pos_dict)
            if pos_mask is not None:
                for m, x in aligned_pos.items():
                    if m in pos_mask:
                        aligned_pos[m] = x * pos_mask[m].unsqueeze(1)
            
            # è´Ÿæ ·æœ¬
            if self.lambda2 == 0:
                aligned_neg = None
            else:
                if neg_dict is None:
                    raise RuntimeError("Dataset must provide features_neg for negative samples.")
                aligned_neg = self.model(neg_dict)
                if aligned_neg is not None and neg_mask is not None:
                    for m, x in aligned_neg.items():
                        if m in neg_mask:
                            aligned_neg[m] = x * neg_mask[m].unsqueeze(1)
            
            # è®¡ç®—æŸå¤±
            loss, svd_values = self._compute_loss_with_metrics(aligned_pos, aligned_negatives=aligned_neg)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            return loss.item(), svd_values
    
    def print_timing_analysis(self, num_steps: int = 100):
        """
        æ‰“å°æ€§èƒ½åˆ†ææŠ¥å‘Š
        
        Args:
            num_steps: ç”¨äºåˆ†æçš„æ­¥æ•°ï¼ˆå–æœ€è¿‘çš„Næ­¥ï¼‰
        """
        if not self.verbose_timing or not self.timing_stats or not self.timing_stats['total_step']:
            logger.info("ğŸ“Š æ€§èƒ½åˆ†ææœªå¯ç”¨æˆ–æ— æ•°æ®")
            return
            
        logger.info("=" * 80)
        logger.info("ğŸ“Š è®­ç»ƒæ€§èƒ½åˆ†ææŠ¥å‘Š")
        logger.info("=" * 80)
        
        # å–æœ€è¿‘çš„Næ­¥è¿›è¡Œåˆ†æ
        recent_steps = min(num_steps, len(self.timing_stats['total_step']))
        
        for stage, times in self.timing_stats.items():
            if not times:
                continue
                
            recent_times = times[-recent_steps:]
            avg_time = np.mean(recent_times)
            std_time = np.std(recent_times)
            min_time = np.min(recent_times)
            max_time = np.max(recent_times)
            
            # è®¡ç®—å æ€»æ—¶é—´çš„ç™¾åˆ†æ¯”
            total_avg = np.mean(self.timing_stats['total_step'][-recent_steps:])
            percentage = (avg_time / total_avg) * 100 if total_avg > 0 else 0
            
            stage_name = {
                'data_loading': 'æ•°æ®åŠ è½½',
                'forward_pass': 'å‰å‘ä¼ æ’­',
                'loss_computation': 'æŸå¤±è®¡ç®—',
                'loss1_computation': 'Loss1è®¡ç®—',
                'loss2_computation': 'Loss2è®¡ç®—',
                'loss3_computation': 'Loss3è®¡ç®—',
                'backward_pass': 'åå‘ä¼ æ’­',
                'optimizer_step': 'ä¼˜åŒ–å™¨æ›´æ–°',
                'total_step': 'æ€»æ­¥æ—¶é—´'
            }.get(stage, stage)
            
            logger.info(f"ğŸ”¹ {stage_name:12} | "
                       f"å¹³å‡: {avg_time*1000:6.2f}ms | "
                       f"æ ‡å‡†å·®: {std_time*1000:5.2f}ms | "
                       f"èŒƒå›´: [{min_time*1000:5.2f}, {max_time*1000:5.2f}]ms | "
                       f"å æ¯”: {percentage:5.1f}%")
        
        # æ€§èƒ½ç“¶é¢ˆåˆ†æ
        logger.info("\nğŸ” æ€§èƒ½ç“¶é¢ˆåˆ†æ:")
        bottleneck_stages = []
        for stage in ['data_loading', 'forward_pass', 'loss_computation', 'loss1_computation', 'loss2_computation', 'loss3_computation', 'backward_pass', 'optimizer_step']:
            if stage in self.timing_stats and self.timing_stats[stage]:
                recent_times = self.timing_stats[stage][-recent_steps:]
                avg_time = np.mean(recent_times)
                bottleneck_stages.append((stage, avg_time))
        
        # æŒ‰è€—æ—¶æ’åº
        bottleneck_stages.sort(key=lambda x: x[1], reverse=True)
        
        stage_names = {
            'data_loading': 'æ•°æ®åŠ è½½',
            'forward_pass': 'å‰å‘ä¼ æ’­', 
            'loss_computation': 'æŸå¤±è®¡ç®—',
            'loss1_computation': 'Loss1è®¡ç®—',
            'loss2_computation': 'Loss2è®¡ç®—',
            'loss3_computation': 'Loss3è®¡ç®—',
            'backward_pass': 'åå‘ä¼ æ’­',
            'optimizer_step': 'ä¼˜åŒ–å™¨æ›´æ–°'
        }
        
        for i, (stage, avg_time) in enumerate(bottleneck_stages):
            emoji = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "ğŸ“Š"
            logger.info(f"  {emoji} {stage_names.get(stage, stage)}: {avg_time*1000:.2f}ms")
        
        logger.info("=" * 80)
    
    @torch.no_grad()
    def _validate_full(self, val_loader: DataLoader, val_max_batches: Optional[int] = None) -> Dict[str, any]:
        """
        éªŒè¯æ¨¡å‹ï¼Œè¿”å›æ¯ä¸ªbatchçš„metrics
        
        Returns:
            {
                'batch_losses': List[float],
                'batch_svd_values': List[Tensor],
                'batch_indices': List[int],
            }
        """
        self.model.eval()
        batch_losses = []
        batch_svd_values = []
        batch_indices = []
        
        for batch_idx, batch in enumerate(tqdm(val_loader, desc="Validation", mininterval=1.0)):
            if val_max_batches is not None and batch_idx >= val_max_batches:
                break
            if isinstance(batch, dict) and 'features' in batch:
                pos_dict = {k: v.to(self.device) for k, v in batch['features'].items()}
                neg_dict = {k: v.to(self.device) for k, v in batch.get('features_neg', {}).items()} if 'features_neg' in batch else None
                pos_mask = {k: v.to(self.device).float() for k, v in batch.get('mask', {}).items()} if 'mask' in batch else None
                neg_mask = {k: v.to(self.device).float() for k, v in batch.get('mask_neg', {}).items()} if 'mask_neg' in batch else None
            else:
                pos_dict = {k: v.to(self.device) for k, v in batch.items()}
                neg_dict = None
                pos_mask = None
                neg_mask = None
            
            aligned_pos = self.model(pos_dict)
            if pos_mask is not None:
                for m, x in aligned_pos.items():
                    if m in pos_mask:
                        aligned_pos[m] = x * pos_mask[m].unsqueeze(1)
            
            if self.lambda2 == 0:
                aligned_neg = None
            else:
                if neg_dict is None:
                    raise RuntimeError("Dataset must provide features_neg for negative samples.")
                aligned_neg = self.model(neg_dict)
                if aligned_neg is not None and neg_mask is not None:
                    for m, x in aligned_neg.items():
                        if m in neg_mask:
                            aligned_neg[m] = x * neg_mask[m].unsqueeze(1)
            
            loss, batch_svd = self._compute_loss_with_metrics(aligned_pos, aligned_negatives=aligned_neg)
            
            # è®°å½•å½“å‰ batch çš„ metrics
            batch_losses.append(loss.item())
            if batch_svd is not None:
                batch_svd_values.append(batch_svd)
            batch_indices.append(batch_idx)
        
        # è¿”å›æ¯ä¸ª batch çš„åŸå§‹ metrics
        return {
            'batch_losses': batch_losses,
            'batch_svd_values': batch_svd_values,
            'batch_indices': batch_indices,
        }
    
    def train(self, 
              train_loader: DataLoader,
              val_loader: Optional[DataLoader] = None,
              max_steps: int = 10000,
              save_path: str = "best_model.pth",
              log_interval: int = 100,
              val_interval: int = 500) -> Dict[str, List[float]]:
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆStep æ¨¡å¼ - ç°ä»£åšæ³•ï¼‰
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
            max_steps: æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆæ¯ä¸ª batch ç®—ä¸€æ­¥ï¼‰
            save_path: æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
            log_interval: æ¯éš”å¤šå°‘æ­¥è®°å½•æ—¥å¿—
            val_interval: æ¯éš”å¤šå°‘æ­¥è¿›è¡ŒéªŒè¯
            
        Returns:
            è®­ç»ƒå†å²è®°å½•å­—å…¸
            
        Example:
            trainer.train(train_loader, val_loader, max_steps=100000, log_interval=1000, val_interval=5000)
        """
        # é‡ç½®early stoppingçŠ¶æ€
        self.best_val_loss = float('inf')
        self.early_stopping_counter = 0
        self.early_stopping_triggered = False
        
        history = {
            'train_losses': [],        # æ¯ä¸ª step çš„ loss
            'train_svd_values': [],    # æ¯ä¸ª step çš„ SVD å€¼
            'val_losses': [],          # éªŒè¯çš„ lossï¼ˆåœ¨ val_interval æ—¶è®°å½•ï¼‰
            'val_svd_values': [],      # éªŒè¯çš„ SVD å€¼
            'val_steps': [],           # åœ¨å“ªäº› step è¿›è¡Œäº†éªŒè¯
        }
        
        logger.info("=" * 80)
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ - å…± {max_steps} æ­¥")
        logger.info(f"   - æ—¥å¿—é—´éš”: æ¯ {log_interval} æ­¥")
        logger.info(f"   - éªŒè¯é—´éš”: æ¯ {val_interval} æ­¥")
        logger.info("=" * 80)
        
        self.model.train()
        global_step = 0
        train_iter = iter(train_loader)
        
        progress_bar = tqdm(total=max_steps, desc="Training")
        
        while global_step < max_steps:
            # è·å–ä¸‹ä¸€ä¸ª batch
            batch = self._get_next_batch(train_iter, train_loader)
            
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            loss, svd_values = self._train_step(batch)
            
            # ğŸ“Š è®°å½• metrics
            history['train_losses'].append(loss)
            if svd_values is not None:
                history['train_svd_values'].append(svd_values)
            
            global_step += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            self._update_progress_bar(progress_bar, loss, svd_values)
            
            # ğŸ” å®šæœŸéªŒè¯
            if val_loader is not None and global_step % val_interval == 0:
                val_loss, val_svd = self._run_validation(val_loader, self.val_max_batches)
                history['val_losses'].append(val_loss)
                history['val_steps'].append(global_step)
                if val_svd is not None:
                    history['val_svd_values'].append(val_svd)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < self.best_val_loss:
                    self._save_checkpoint(save_path, global_step, val_loss)
                
                # æ£€æŸ¥early stopping
                if self._check_early_stopping(val_loss):
                    logger.info(f"ğŸ›‘ [Step {global_step}] Early stoppingè§¦å‘ï¼Œè®­ç»ƒæå‰ç»“æŸ")
                    break
            
            # ğŸ“ å®šæœŸæ—¥å¿—è¾“å‡º
            if global_step % log_interval == 0:
                self._log_progress(global_step, max_steps, log_interval, history)
        
        progress_bar.close()
        
        # è®­ç»ƒç»“æŸåçš„æ€»ç»“
        if self.early_stopping_triggered:
            logger.info("=" * 80)
            logger.info("ğŸ›‘ è®­ç»ƒå› Early Stoppingæå‰ç»“æŸ")
            logger.info(f"   - æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            logger.info(f"   - æ€»è®­ç»ƒæ­¥æ•°: {global_step}/{max_steps}")
            logger.info(f"   - èŠ‚çœæ­¥æ•°: {max_steps - global_step}")
            logger.info("=" * 80)
        else:
            logger.info("=" * 80)
            logger.info("âœ… è®­ç»ƒæ­£å¸¸å®Œæˆï¼")
            logger.info(f"   - æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            logger.info(f"   - æ€»è®­ç»ƒæ­¥æ•°: {global_step}/{max_steps}")
            logger.info("=" * 80)

        # æ‰“å°æ€§èƒ½åˆ†ææŠ¥å‘Šï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
        if self.verbose_timing:
            self.print_timing_analysis()
        
        return history