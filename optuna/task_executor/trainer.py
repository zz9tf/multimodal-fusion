"""
é€šç”¨è®­ç»ƒå™¨ç±»
æ”¯æŒä¸åŒçš„æ¨¡å‹ç±»å‹å’Œè®­ç»ƒé…ç½®
"""

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import os
import json
import csv
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from sklearn.preprocessing import label_binarize
import pandas as pd

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# åªå¯¼å…¥å¿…è¦çš„æ¨¡å—ï¼Œé¿å…ä¾èµ– utils.utils
from models.model_factory import ModelFactory
from sklearn.metrics import roc_auc_score
from torchmetrics.classification import AUROC as TM_AUROC

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def to_serializable(obj: Any) -> Any:
    """é€šç”¨JSONå®‰å…¨åºåˆ—åŒ–è½¬æ¢å™¨

    - å°† numpy æ ‡é‡è½¬æ¢ä¸º Python æ ‡é‡
    - å°† numpy æ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨
    - å°† torch å¼ é‡ç§»åŠ¨åˆ° CPU å¹¶è½¬æ¢ä¸ºåˆ—è¡¨
    - å…¶ä»–ä¸å¯åºåˆ—åŒ–å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    """
    # numpy æ ‡é‡
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, (np.floating,)):
        return float(obj)
    if obj is np.nan:
        return None
    # numpy æ•°ç»„
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    # torch å¼ é‡
    if torch.is_tensor(obj):
        try:
            return obj.detach().cpu().tolist()
        except Exception:
            return str(obj)
    # å…¶ä»–å¸¸è§ä¸å¯åºåˆ—åŒ–ç±»å‹å…œåº•ä¸ºå­—ç¬¦ä¸²
    try:
        json.dumps(obj)
        return obj
    except Exception:
        return str(obj)

def save_splits(split_datasets, column_keys, filename, boolean_style=False):
	"""
	ä¿å­˜æ•°æ®é›†åˆ†å‰²ä¿¡æ¯ï¼ˆä½¿ç”¨patient_id/case_idè€Œéç´¢å¼•ï¼Œç¡®ä¿å¯å¤ç°æ€§ï¼‰
	
	å…³é”®ä¿®å¤ï¼šä»Subsetå¯¹è±¡ä¸­æå–åŸå§‹æ•°æ®é›†çš„case_idsï¼Œä½¿ç”¨å®é™…çš„case_idè€Œéç´¢å¼•
	è¿™æ ·å³ä½¿æ•°æ®é›†é¡ºåºä¸åŒï¼Œä¹Ÿèƒ½é€šè¿‡case_idæ­£ç¡®åŒ¹é…åˆ’åˆ†
	"""
	try:
		# è·å–æ¯ä¸ªåˆ†å‰²çš„case_idsï¼ˆä»Subsetä¸­æå–åŸå§‹æ•°æ®é›†çš„case_idsï¼‰
		splits = []
		for i, dataset in enumerate(split_datasets):
			if hasattr(dataset, 'case_ids'):
				# ç›´æ¥æ˜¯MultimodalDatasetå¯¹è±¡
				splits.append(pd.Series(dataset.case_ids))
			elif hasattr(dataset, 'dataset') and hasattr(dataset, 'indices'):
				# æ˜¯Subsetå¯¹è±¡ï¼Œéœ€è¦ä»åŸå§‹æ•°æ®é›†æå–case_ids
				base_dataset = dataset.dataset
				indices = dataset.indices
				
				if hasattr(base_dataset, 'case_ids'):
					# ä»åŸå§‹æ•°æ®é›†çš„case_idsä¸­æå–å¯¹åº”çš„case_id
					base_case_ids = base_dataset.case_ids
					if isinstance(base_case_ids, list):
						case_ids = [base_case_ids[idx] for idx in indices]
					else:
						# å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼ˆå¦‚numpy arrayï¼‰ï¼Œè½¬æ¢ä¸ºlist
						case_ids = [base_case_ids[idx] for idx in indices]
					splits.append(pd.Series(case_ids))
				else:
					# fallback: ä½¿ç”¨ç´¢å¼•
					splits.append(pd.Series([f"sample_{j}" for j in indices]))
			else:
				# fallback: ä½¿ç”¨ç´¢å¼•
				splits.append(pd.Series([f"sample_{j}" for j in range(len(dataset))]))
		
		if not boolean_style:
			# åˆ›å»ºDataFrameï¼Œæ¯åˆ—æ˜¯ä¸€ä¸ªåˆ†å‰²çš„case_ids
			# ä½¿ç”¨æœ€é•¿çš„åˆ†å‰²ä½œä¸ºDataFrameçš„é•¿åº¦ï¼Œè¾ƒçŸ­çš„ç”¨NaNå¡«å……
			max_len = max(len(s) for s in splits) if splits else 0
			
			# åˆ›å»ºå­—å…¸ï¼Œæ¯ä¸ªé”®å¯¹åº”ä¸€ä¸ªåˆ†å‰²çš„case_ids
			data_dict = {}
			for i, col_key in enumerate(column_keys):
				if i < len(splits):
					case_ids = splits[i].tolist()
					# å¡«å……NaNä½¿å…¶é•¿åº¦ä¸€è‡´
					while len(case_ids) < max_len:
						case_ids.append(None)
					data_dict[col_key] = case_ids
				else:
					data_dict[col_key] = [None] * max_len
			
			df = pd.DataFrame(data_dict)
			# ç§»é™¤å…¨NaNçš„è¡Œ
			df = df.dropna(how='all')
		else:
			df = pd.concat(splits, ignore_index = True, axis=0)
			index = df.values.tolist()
			one_hot = np.eye(len(split_datasets)).astype(bool)
			bool_array = np.repeat(one_hot, [len(dset) for dset in split_datasets], axis=0)
			df = pd.DataFrame(bool_array, index=index, columns = ['train', 'val', 'test'])

		df.to_csv(filename, index=False)
		print(f"âœ… ä¿å­˜åˆ†å‰²ä¿¡æ¯åˆ°: {filename} (ä½¿ç”¨case_id)")
	except Exception as e:
		print(f"âš ï¸ ä¿å­˜åˆ†å‰²ä¿¡æ¯å¤±è´¥: {e}")
		import traceback
		traceback.print_exc()
		# åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆ†å‰²è®°å½•
		split_info = {
			'split_type': column_keys,
			'train_size': len(split_datasets[0]) if len(split_datasets) > 0 else 0,
			'val_size': len(split_datasets[1]) if len(split_datasets) > 1 else 0,
			'test_size': len(split_datasets[2]) if len(split_datasets) > 2 else 0
		}
		pd.DataFrame([split_info]).to_csv(filename, index=False)
		print(f"âœ… ä¿å­˜ç®€åŒ–åˆ†å‰²ä¿¡æ¯åˆ°: {filename}")

def print_network(model: nn.Module):
    """æ‰“å°ç½‘ç»œç»“æ„å’Œå‚æ•°ç»Ÿè®¡"""
    print("=" * 50)
    print("Model Architecture:")
    print("=" * 50)
    print(model)
    print("=" * 50)
    
    # è®¡ç®—å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Non-trainable parameters: {total_params - trainable_params:,}")
    print("=" * 50)

def get_optim(model: nn.Module, opt: str, lr: float, reg: float) -> torch.optim.Optimizer:
    """è·å–ä¼˜åŒ–å™¨"""
    if opt == "adam":
        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 
                                   lr=lr, weight_decay=reg)
    elif opt == 'sgd':
        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), 
                                  lr=lr, momentum=0.9, weight_decay=reg)
    else:
        raise NotImplementedError
    
    return optimizer

def get_scheduler(optimizer: torch.optim.Optimizer, scheduler_config: Dict) -> Optional[torch.optim.lr_scheduler._LRScheduler]:
    """
    è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    Args:
        optimizer: ä¼˜åŒ–å™¨
        scheduler_config: è°ƒåº¦å™¨é…ç½®å­—å…¸
        
    Returns:
        å­¦ä¹ ç‡è°ƒåº¦å™¨æˆ–None
    """
    scheduler_type = scheduler_config.get('type', None)
    
    if scheduler_type is None:
        return None
    
    if scheduler_type == 'step':
        step_size = scheduler_config.get('step_size', 50)
        gamma = scheduler_config.get('gamma', 0.5)
        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
    
    elif scheduler_type == 'cosine':
        T_max = scheduler_config.get('T_max', 200)
        eta_min = scheduler_config.get('eta_min', 0.0)
        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)
    
    elif scheduler_type == 'cosine_warm_restart':
        T_0 = scheduler_config.get('T_0', 10)  # ç¬¬ä¸€ä¸ªé‡å¯å‘¨æœŸé•¿åº¦
        T_mult = scheduler_config.get('T_mult', 2)  # å‘¨æœŸé•¿åº¦å€å¢å› å­
        eta_min = scheduler_config.get('eta_min', 0.0)  # æœ€å°å­¦ä¹ ç‡
        return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=T_0, T_mult=T_mult, eta_min=eta_min
        )
    
    elif scheduler_type == 'plateau':
        mode = scheduler_config.get('mode', 'min')
        patience = scheduler_config.get('patience', 10)
        factor = scheduler_config.get('factor', 0.5)
        return torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode=mode, patience=patience, factor=factor, verbose=True
        )
    
    elif scheduler_type == 'exponential':
        gamma = scheduler_config.get('gamma', 0.95)
        return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)
    
    else:
        print(f"âš ï¸ æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")
        return None

def get_split_loader(dataset, training=False, weighted=False, batch_size=1, generator=None):
    """è·å–æ•°æ®åŠ è½½å™¨
    
    Args:
        dataset: æ•°æ®é›†
        training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        weighted: æ˜¯å¦ä½¿ç”¨åŠ æƒé‡‡æ ·
        batch_size: batchå¤§å°
        generator: éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆç”¨äºç¡®ä¿é‡‡æ ·é¡ºåºä¸€è‡´ï¼‰
    """
    if training:
        if weighted:
            weights = make_weights_for_balanced_classes_split(dataset)
            sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights), generator=generator)
            return torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler)
        else:
            return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=generator)
    else:
        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)

def make_weights_for_balanced_classes_split(dataset):
    """ä¸ºå¹³è¡¡ç±»åˆ«åˆ›å»ºæƒé‡"""
    N = float(len(dataset))
    
    # è·å–æ ‡ç­¾ï¼Œé€‚é…MultimodalDatasetæ ¼å¼
    labels = []
    unique_labels = set()
    
    for i in range(len(dataset)):
        # å¤„ç†Subsetå¯¹è±¡
        if hasattr(dataset, 'dataset') and hasattr(dataset.dataset, 'get_label'):
            # é€šè¿‡Subsetçš„indicesè·å–åŸå§‹æ•°æ®é›†çš„ç´¢å¼•
            original_idx = dataset.indices[i]
            label = dataset.dataset.get_label(original_idx)
        else:
            # ç›´æ¥å¤„ç†MultimodalDataset
            label = dataset.get_label(i)
        
        unique_labels.add(label)
        labels.append(label)
    
    # ä½¿ç”¨æ•°æ®é›†çš„æ ‡ç­¾æ˜ å°„
    if hasattr(dataset, 'label_to_int'):
        label_to_int = dataset.label_to_int
    else:
        # å¦‚æœæ²¡æœ‰æ ‡ç­¾æ˜ å°„ï¼Œåˆ›å»ºé»˜è®¤æ˜ å°„
        label_to_int = {label: idx for idx, label in enumerate(sorted(unique_labels))}
    
    # å°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—
    numeric_labels = [label_to_int[label] for label in labels]
    labels = np.array(numeric_labels)
    
    class_counts = np.bincount(labels)
    class_weights = N / class_counts
    weights = [class_weights[labels[i]] for i in range(len(dataset))]
    return torch.DoubleTensor(weights)


class Logger:
    """
    ç»Ÿä¸€çš„è®­ç»ƒæŒ‡æ ‡è®°å½•å™¨
    æ•´åˆäº†å‡†ç¡®ç‡ç»Ÿè®¡ã€è®­ç»ƒæ—¥å¿—è®°å½•å’Œæœ€ä½³æŒ‡æ ‡è·Ÿè¸ªåŠŸèƒ½
    """
    
    def __init__(self, n_classes: int, log_dir: str = None, fold: int = 0):
        """
        åˆå§‹åŒ–æŒ‡æ ‡è®°å½•å™¨
        
        Args:
            n_classes: ç±»åˆ«æ•°é‡
            log_dir: æ—¥å¿—ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
            fold: foldç´¢å¼•
        """
        self.n_classes = n_classes
        self.log_dir = log_dir
        self.fold = fold
        
        # ç±»åˆ«ç»Ÿè®¡
        self.batch_log = {
            'class_stats': [{"count": 0, "correct": 0} for _ in range(self.n_classes)],
            'labels': [],
            'probs': [],
            'loss': 0.0
        }
        
        # è®­ç»ƒæ—¥å¿—
        self.epoch_logs = []
        self.best_metrics = {
            'best_val_loss': float('inf'),
            'best_val_acc': 0.0,
            'best_val_auc': 0.0,
            'best_epoch': 0
        }
        
        # åˆå§‹åŒ–æ–‡ä»¶è®°å½•ï¼ˆå¦‚æœæä¾›äº†log_dirï¼‰
        if log_dir:
            os.makedirs(log_dir, exist_ok=True)
            self.csv_path = os.path.join(log_dir, f'fold_{fold}_training_log.csv')
            with open(self.csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'epoch', 'train_loss', 'train_acc', 'train_auc',
                    'val_loss', 'val_acc', 'val_auc', 'learning_rate',
                    'timestamp'
                ])
    
    def reset_epoch_stats(self):
        """é‡ç½®å½“å‰epochçš„ç»Ÿè®¡"""
        self.batch_log = {
            'class_stats': [{"count": 0, "correct": 0} for _ in range(self.n_classes)],
            'labels': [],
            'probs': [],
            'loss': 0.0
        }
    
    def log_batch(self, Y_hat, Y, Y_prob, loss):
        """
        è®°å½•æ‰¹æ¬¡é¢„æµ‹ç»“æœï¼Œlossï¼Œlabelsï¼Œprobs
        
        Args:
            Y_hat: é¢„æµ‹ç»“æœ (int, tensor, æˆ– array)
            Y: çœŸå®æ ‡ç­¾ (int, tensor, æˆ– array)
            Y_prob: é¢„æµ‹æ¦‚ç‡ (tensor, æˆ– array)
            loss: æŸå¤±å€¼ (tensor)
        """
        # ç»Ÿä¸€è½¬ä¸ºTensorï¼ˆä¸è½¬numpyï¼‰ï¼Œç”¨äºåç»­torch.cat
        if not torch.is_tensor(Y_hat):
            Y_hat = torch.as_tensor(Y_hat)
        if not torch.is_tensor(Y):
            Y = torch.as_tensor(Y)
        if not torch.is_tensor(Y_prob):
            Y_prob = torch.as_tensor(Y_prob)

        # ç»Ÿè®¡åˆ†ç±»æ­£ç¡®æ•°
        if Y_hat.numel() == 1 and Y.numel() == 1:
            label_class = int(Y.item())
            self.batch_log['class_stats'][label_class]["count"] += 1
            self.batch_log['class_stats'][label_class]["correct"] += (int(Y_hat.item() == Y.item()))
        else:
            unique_labels = torch.unique(Y)
            for label_class in unique_labels.tolist():
                cls_mask = (Y == label_class)
                self.batch_log['class_stats'][label_class]["count"] += int(cls_mask.sum().item())
                self.batch_log['class_stats'][label_class]["correct"] += int((Y_hat[cls_mask] == Y[cls_mask]).sum().item())

        # è¿½åŠ åˆ°æ—¥å¿—ï¼ˆä¿æŒä¸ºTensorï¼‰
        self.batch_log['labels'].append(Y)
        self.batch_log['probs'].append(Y_prob)
        self.batch_log['loss'] += float(loss.item())
    
    def get_class_accuracy(self, class_idx: int) -> Tuple[Optional[float], int, int]:
        """
        è·å–æŒ‡å®šç±»åˆ«çš„å‡†ç¡®ç‡
        
        Returns:
            (accuracy, correct_count, total_count)
        """
        count = self.batch_log['class_stats'][class_idx]["count"]
        correct = self.batch_log['class_stats'][class_idx]["correct"]
        
        if count == 0:
            return None, correct, count
        else:
            return float(correct) / count, correct, count
    
    def get_overall_accuracy(self) -> float:
        """è·å–æ•´ä½“å‡†ç¡®ç‡"""
        total_correct = sum(stat["correct"] for stat in self.batch_log['class_stats'])
        total_count = sum(stat["count"] for stat in self.batch_log['class_stats'])
        
        if total_count == 0:
            return 0.0
        return float(total_correct) / total_count
    
    def log_epoch(self, epoch: int, train_metrics: Dict, val_metrics: Dict, lr: float):
        """
        è®°å½•epochæŒ‡æ ‡
        
        Args:
            epoch: å½“å‰epoch
            train_metrics: è®­ç»ƒæŒ‡æ ‡å­—å…¸
            val_metrics: éªŒè¯æŒ‡æ ‡å­—å…¸  
            lr: å­¦ä¹ ç‡
        """
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        epoch_log = {
            'epoch': epoch,
            'train_loss': train_metrics.get('loss', 0.0),
            'train_acc': train_metrics.get('acc', 0.0),
            'train_auc': train_metrics.get('auc', 0.0),
            'val_loss': val_metrics.get('loss', 0.0),
            'val_acc': val_metrics.get('acc', 0.0),
            'val_auc': val_metrics.get('auc', 0.0),
            'learning_rate': lr,
            'timestamp': timestamp
        }
        
        self.epoch_logs.append(epoch_log)
        
        # å†™å…¥CSVæ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨äº†æ–‡ä»¶è®°å½•ï¼‰
        if self.log_dir:
            with open(self.csv_path, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    epoch_log['epoch'], epoch_log['train_loss'], epoch_log['train_acc'], 
                    epoch_log['train_auc'], epoch_log['val_loss'], epoch_log['val_acc'], 
                    epoch_log['val_auc'], epoch_log['learning_rate'], epoch_log['timestamp']
                ])
        
        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        self._update_best_metrics(epoch, val_metrics)
        
        # æ‰“å°è¿›åº¦
        print(f"ğŸ“Š Epoch {epoch:3d} | "
              f"Train: Loss={train_metrics.get('loss', 0.0):.4f}, "
              f"Acc={train_metrics.get('acc', 0.0):.4f}, "
              f"AUC={train_metrics.get('auc', 0.0):.4f} | "
              f"Val: Loss={val_metrics.get('loss', 0.0):.4f}, "
              f"Acc={val_metrics.get('acc', 0.0):.4f}, "
              f"AUC={val_metrics.get('auc', 0.0):.4f}")
    
    def _update_best_metrics(self, epoch: int, val_metrics: Dict):
        """æ›´æ–°æœ€ä½³æŒ‡æ ‡"""
        val_loss = val_metrics.get('loss', float('inf'))
        val_acc = val_metrics.get('acc', 0.0)
        val_auc = val_metrics.get('auc', 0.0)
        
        if val_loss < self.best_metrics['best_val_loss']:
            self.best_metrics['best_val_loss'] = val_loss
            self.best_metrics['best_epoch'] = epoch
            
        if val_acc > self.best_metrics['best_val_acc']:
            self.best_metrics['best_val_acc'] = val_acc
            
        if val_auc > self.best_metrics['best_val_auc']:
            self.best_metrics['best_val_auc'] = val_auc
    
    def save_summary(self, test_metrics: Dict = None):
        """ä¿å­˜è®­ç»ƒæ€»ç»“"""
        summary = {
            'fold': self.fold,
            'best_metrics': self.best_metrics,
            'total_epochs': len(self.epoch_logs),
            'final_epoch': self.epoch_logs[-1] if self.epoch_logs else None,
            'test_metrics': test_metrics
        }
        
        if self.log_dir:
            # ä¿å­˜JSONæ€»ç»“
            summary_path = os.path.join(self.log_dir, f'fold_{self.fold}_summary.json')
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False, default=to_serializable)
        
        # æ‰“å°æ€»ç»“
        print(f"\nğŸ¯ Fold {self.fold} è®­ç»ƒæ€»ç»“:")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {self.best_metrics['best_val_loss']:.4f} (Epoch {self.best_metrics['best_epoch']})")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_metrics['best_val_acc']:.4f}")
        print(f"   æœ€ä½³éªŒè¯AUC: {self.best_metrics['best_val_auc']:.4f}")
        
        if test_metrics:
            print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_metrics.get('acc', 0.0):.4f}")
            print(f"   æµ‹è¯•AUC: {test_metrics.get('auc', 0.0):.4f}")
        
        return summary

class EarlyStopping:
    """
    æ—©åœæœºåˆ¶
    
    æ”¯æŒæ ¹æ®ä»»æ„æŒ‡æ ‡ï¼ˆscoreï¼‰è¿›è¡Œæ—©åœï¼Œå¯ä»¥æ˜¯ lossã€AUCã€accuracy ç­‰
    é€šè¿‡ mode å‚æ•°æŒ‡å®šæ˜¯æœ€å¤§åŒ–è¿˜æ˜¯æœ€å°åŒ–æŒ‡æ ‡
    """
    
    def __init__(self, 
                 patience: int = 20, 
                 stop_epoch: int = 50, 
                 verbose: bool = False,
                 mode: str = 'max',
                 min_delta: float = 0.0):
        """
        åˆå§‹åŒ–æ—©åœæœºåˆ¶
        
        Args:
            patience: å®¹å¿å¤šå°‘ä¸ª epoch æ²¡æœ‰æ”¹å–„
            stop_epoch: æœ€æ—©åœ¨ç¬¬å‡ ä¸ª epoch ä¹‹åæ‰å…è®¸æ—©åœ
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            mode: 'max' è¡¨ç¤ºæœ€å¤§åŒ–æŒ‡æ ‡ï¼ˆå¦‚ AUCã€accuracyï¼‰ï¼Œ'min' è¡¨ç¤ºæœ€å°åŒ–æŒ‡æ ‡ï¼ˆå¦‚ lossï¼‰
            min_delta: æ”¹å–„çš„æœ€å°é˜ˆå€¼ï¼Œåªæœ‰è¶…è¿‡è¿™ä¸ªé˜ˆå€¼æ‰è®¤ä¸ºæ˜¯æ”¹å–„
        """
        self.patience = patience
        self.stop_epoch = stop_epoch
        self.verbose = verbose
        self.mode = mode.lower()
        self.min_delta = min_delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        
        # æ ¹æ®æ¨¡å¼è®¾ç½®åˆå§‹æœ€ä½³å€¼
        if self.mode == 'max':
            self.best_score = -np.Inf
        elif self.mode == 'min':
            self.best_score = np.Inf
        else:
            raise ValueError(f"mode å¿…é¡»æ˜¯ 'max' æˆ– 'min'ï¼Œå½“å‰ä¸º: {mode}")

    def __call__(self, epoch: int, score: float, model: nn.Module, ckpt_name: str = 'checkpoint.pt') -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        
        Args:
            epoch: å½“å‰ epoch ç¼–å·
            score: å½“å‰æŒ‡æ ‡å€¼ï¼ˆå¯ä»¥æ˜¯ lossã€AUCã€accuracy ç­‰ï¼‰
            model: æ¨¡å‹å¯¹è±¡
            ckpt_name: æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„
            
        Returns:
            æ˜¯å¦åº”è¯¥æ—©åœ
        """
        # åˆ¤æ–­æ˜¯å¦æ”¹å–„
        if self.mode == 'max':
            # æœ€å¤§åŒ–æ¨¡å¼ï¼šscore è¶Šå¤§è¶Šå¥½
            is_better = score > (self.best_score + self.min_delta)
        else:
            # æœ€å°åŒ–æ¨¡å¼ï¼šscore è¶Šå°è¶Šå¥½
            is_better = score < (self.best_score - self.min_delta)
        
        if is_better:
            # æœ‰æ”¹å–„ï¼Œæ›´æ–°æœ€ä½³å€¼å¹¶ä¿å­˜æ¨¡å‹
            self.best_score = score
            self.save_checkpoint(score, model, ckpt_name)
            self.counter = 0
        else:
            # æ²¡æœ‰æ”¹å–„ï¼Œå¢åŠ è®¡æ•°å™¨
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
            if self.counter >= self.patience and epoch > self.stop_epoch:
                self.early_stop = True
        
        return self.early_stop

    def save_checkpoint(self, score: float, model: nn.Module, ckpt_name: str):
        """
        ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            score: å½“å‰æŒ‡æ ‡å€¼
            model: æ¨¡å‹å¯¹è±¡
            ckpt_name: æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„
        """
        if self.verbose:
            mode_str = 'increased' if self.mode == 'max' else 'decreased'
            print(f'Validation score {mode_str} ({self.best_score:.6f} --> {score:.6f}).  Saving model ...')
        torch.save(model.state_dict(), ckpt_name)

class Trainer:
    """
    é€šç”¨è®­ç»ƒå™¨ç±»
    æ”¯æŒä¸åŒçš„æ¨¡å‹ç±»å‹å’Œè®­ç»ƒé…ç½®
    """
    
    def __init__(self, 
                 configs: Dict,
                 log_dir: str = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            configs: é…ç½®å­—å…¸
            log_dir: æ—¥å¿—ä¿å­˜ç›®å½•
        """
        self.model_config = configs['model_config']
        self.experiment_config = configs['experiment_config']
        self.results_dir = self.experiment_config['results_dir']
        self.log_dir = log_dir or './logs'
        
        # éªŒè¯é…ç½®å®Œæ•´æ€§
        required_training_params = ['max_epochs', 'lr', 'reg', 'opt', 'early_stopping', 'batch_size']
        missing_training_params = [param for param in required_training_params if param not in self.experiment_config]
        if missing_training_params:
            raise ValueError(f"è®­ç»ƒé…ç½®ç¼ºå°‘å¿…éœ€å‚æ•°: {missing_training_params}")
        
        # ä»é…ç½®ä¸­æå–å‚æ•°
        self.max_epochs = self.experiment_config['max_epochs']
        self.lr = self.experiment_config['lr']
        self.reg = self.experiment_config['reg']
        self.opt = self.experiment_config['opt']
        self.early_stopping = self.experiment_config['early_stopping']
        self.batch_size = self.experiment_config['batch_size']
        
        # åˆå§‹åŒ–æ¨¡å‹å’ŒæŸå¤±å‡½æ•°
        self.model = None
        self.loss_fn = None
        self.scheduler = None

    def _init_model(self) -> nn.Module:
        """åˆå§‹åŒ–æ¨¡å‹"""
        # ä»model_configä¸­è·å–å‚æ•°å¹¶æ„å»ºé…ç½®
        config = self.model_config.copy()
        
        # ä½¿ç”¨æ¨¡å‹å·¥å‚åˆ›å»ºæ¨¡å‹
        model = ModelFactory.create_model(config)
        
        return model.to(device)
    
    def train_fold(self, 
                   datasets: Tuple[Any, Any, Any],
                   fold_idx: int) -> Tuple[Dict, float, float, float, float]:
        """
        Level 1: Foldè®­ç»ƒä¸»å…¥å£
        
        Args:
            datasets: (train_dataset, val_dataset, test_dataset)
            fold_idx: foldç´¢å¼•
            
        Returns:
            (results_dict, test_auc, val_auc, test_acc, val_acc)
        """
        print(f'\nTraining Fold {fold_idx}!')
        
        # åˆ›å»ºç›®å½•å’Œæ—¥å¿—è®°å½•å™¨
        metrics_logger = Logger(self.model_config['n_classes'], self.log_dir, fold_idx)

        # ä¿å­˜æ•°æ®é›†åˆ†å‰²
        train_split, val_split, test_split = datasets
        save_splits(datasets, ['train', 'val', 'test'], 
                   os.path.join(self.results_dir, 'splits_{}.csv'.format(fold_idx)))
        
        print(f"Training on {len(train_split)} samples")
        print(f"Validating on {len(val_split)} samples")
        print(f"Testing on {len(test_split)} samples")

        # åˆå§‹åŒ–æ¨¡å‹å’ŒæŸå¤±å‡½æ•°
        model = self._init_model()
        self.loss_fn = model.loss_fn
        print_network(model)
        optimizer = get_optim(model, self.opt, self.lr, self.reg)
        
        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_config = self.experiment_config.get('scheduler_config', {})
        self.scheduler = get_scheduler(optimizer, scheduler_config)
        if self.scheduler:
            print(f"ğŸ¯ ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_config.get('type', 'unknown')}")
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        seed = self.experiment_config['seed']
        train_loader = get_split_loader(train_split, training=True, weighted=True, batch_size=1, generator=torch.Generator().manual_seed(seed))
        val_loader = get_split_loader(val_split, training=False, weighted=False, batch_size=1, generator=torch.Generator().manual_seed(seed))
        test_loader = get_split_loader(test_split, training=False, weighted=False, batch_size=1, generator=torch.Generator().manual_seed(seed))

        # åˆå§‹åŒ–æ—©åœ
        # ä»é…ç½®ä¸­è·å–æ—©åœå‚æ•°ï¼Œæ”¯æŒè‡ªå®šä¹‰æŒ‡æ ‡å’Œæ¨¡å¼
        early_stopping_config = self.experiment_config.get('early_stopping_config', {})
        
        if self.early_stopping:
            # å¦‚æœ early_stopping æ˜¯å­—å…¸ï¼Œä½¿ç”¨å­—å…¸ä¸­çš„é…ç½®ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            if isinstance(self.early_stopping, dict):
                config = {**early_stopping_config, **self.early_stopping}
            else:
                config = early_stopping_config
            
            # è·å–é…ç½®å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
            patience = config.get('patience', 25)
            stop_epoch = config.get('stop_epoch', 10)
            verbose = config.get('verbose', True)
            mode = config.get('mode', 'max')  # 'max' for auc/acc, 'min' for loss
            min_delta = config.get('min_delta', 0.0)
            metric = config.get('metric', 'auc')  # 'auc', 'acc', 'loss'
            
            early_stopping_obj = EarlyStopping(
                patience=patience, 
                stop_epoch=stop_epoch, 
                verbose=verbose,
                mode=mode,
                min_delta=min_delta
            )
            # ä¿å­˜ metric é…ç½®ï¼Œç”¨äºåç»­é€‰æ‹©æŒ‡æ ‡
            early_stopping_obj.metric = metric
        else:
            early_stopping_obj = None
        
        # 2. è®­ç»ƒ
        for epoch in range(self.max_epochs):
            # è®­ç»ƒå’ŒéªŒè¯
            train_metrics = self._train_single_epoch(epoch, train_loader, optimizer, model, metrics_logger)
            val_metrics, stop = self._validate_single_epoch(fold_idx, epoch, val_loader, model, early_stopping_obj)
            
            # è®°å½•æ—¥å¿—
            metrics_logger.log_epoch(epoch, train_metrics, val_metrics, optimizer.param_groups[0]['lr'])
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            if self.scheduler:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    # ReduceLROnPlateauéœ€è¦éªŒè¯æŸå¤±
                    self.scheduler.step(val_metrics['loss'])
                else:
                    # å…¶ä»–è°ƒåº¦å™¨ä½¿ç”¨epoch
                    self.scheduler.step()
            
            if stop: 
                break
        
        # 3. æœ€ç»ˆè¯„ä¼°å’Œè¿”å›ç»“æœ
        # ä¿å­˜æ¨¡å‹
        checkpoint_path = os.path.join(self.results_dir, "s_{}_checkpoint.pt".format(fold_idx))
        if self.early_stopping:
            model.load_state_dict(torch.load(checkpoint_path))
        else:
            torch.save(model.state_dict(), checkpoint_path)

        # æœ€ç»ˆè¯„ä¼°
        _, val_accuracy, val_auc, _ = self._evaluate_model(val_loader, model)
        results_dict, test_accuracy, test_auc, eval_logger = self._evaluate_model(test_loader, model)
        
        print('Val accuracy: {:.4f}, ROC AUC: {:.4f}'.format(val_accuracy, val_auc))
        print('Test accuracy: {:.4f}, ROC AUC: {:.4f}'.format(test_accuracy, test_auc))

        # æ‰“å°å„ç±»åˆ«å‡†ç¡®ç‡
        for i in range(self.model_config['n_classes']):
            acc, correct, count = eval_logger.get_class_accuracy(i)
            print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))
        
        # ä¿å­˜è®­ç»ƒæ€»ç»“
        metrics_logger.save_summary({
            'acc': test_accuracy,
            'auc': test_auc,
            'loss': 1-test_accuracy
        })
            
        return results_dict, test_auc, val_auc, test_accuracy, val_accuracy

    def _train_single_epoch(self, epoch: int, loader: DataLoader, optimizer: torch.optim.Optimizer, model: nn.Module, logger: Logger) -> Dict:
        """
        Level 3: æ ‡å‡†æ¨¡å‹å•ä¸ªepochè®­ç»ƒ
        """
        model.train()
        
        # ğŸ”§ é‡ç½®epochç»Ÿè®¡ä¿¡æ¯ï¼Œç¡®ä¿æ¯ä¸ªepochçš„ç»Ÿè®¡æ˜¯ç‹¬ç«‹çš„
        logger.reset_epoch_stats()

        print('\n')
        batch_size = self.experiment_config['batch_size']
        total_loss = 0
        for batch_idx, (data, label) in enumerate(loader):
            # æ ‡ç­¾å·²ç»æ˜¯tensorï¼Œç›´æ¥ç§»åŠ¨åˆ°è®¾å¤‡
            label = label.to(device)
            
            # data ç°åœ¨æ˜¯å­—å…¸æ ¼å¼ï¼Œæ¯ä¸ªchannelåŒ…å«ä¸€ä¸ªå¼ é‡
            # éœ€è¦å°†æ¯ä¸ªchannelçš„å¼ é‡ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
            for channel in data:
                data[channel] = data[channel].to(device)
            results = model(data, label)
            Y_prob = results['probabilities']
            Y_hat = results['predictions']
            
            # è®¡ç®—æŸå¤±
            results['labels'] = label
            loss = self.loss_fn(results['logits'], results['labels'], results)
            total_loss += loss
            # è®°å½•æŒ‡æ ‡
            logger.log_batch(Y_hat, label, Y_prob, loss)
            
            if (batch_idx + 1) % batch_size == 0:
                # åå‘ä¼ æ’­
                if hasattr(model, 'group_loss_fn'):
                    results['group_loss'] = model.group_loss_fn(results)
                    total_loss += results['group_loss']
                total_loss = total_loss/batch_size
                results['total_loss'] = total_loss.item()
                total_loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                if hasattr(model, 'verbose_items'):
                    items = model.verbose_items(results)
                    if len(items) > 0:
                        print('Batch {}/{}: '.format(batch_idx + 1, len(loader)) + ' '.join([f'{key}: {value:.4f}' for key, value in items]))
                total_loss = 0
        
        if len(loader) % batch_size != 0:
            # è®¡ç®—å‰©ä½™batchçš„æ•°é‡
            remaining_batches = len(loader) % batch_size
            # åå‘ä¼ æ’­
            if hasattr(model, 'group_loss_fn'):
                results['group_loss'] = model.group_loss_fn(results)
                total_loss += results['group_loss']
            total_loss = total_loss / remaining_batches  # ä½¿ç”¨å‰©ä½™batchæ•°é‡è¿›è¡Œå¹³å‡
            results['total_loss'] = total_loss.item()
            total_loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            if hasattr(model, 'verbose_items'):
                items = model.verbose_items(results)
                if len(items) > 0:
                    print('Final batch: ' + ' '.join([f'{key}: {value:.4f}' for key, value in items]))
            total_loss = 0
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        train_loss = logger.batch_log['loss'] / len(loader)

        print('Epoch: {}, train_loss: {:.4f}, train_acc: {:.4f}'.format(epoch, train_loss, logger.get_overall_accuracy()))
        if hasattr(model, 'verbose_items'):
            results['is_epoch'] = True
            items = model.verbose_items(results)
            if len(items) > 0:
                print('- ' + ' '.join([f'{key}: {value:.4f}' for key, value in items]))
        
        # è®¡ç®—å¹¶è¿”å›æŒ‡æ ‡
        return self._calculate_epoch_metrics(logger)

    def _calculate_epoch_metrics(self, logger: Dict) -> Dict:
        """è®¡ç®—epochæŒ‡æ ‡"""
        n_classes = self.model_config['n_classes']
        
        # è®¡ç®—å‡†ç¡®ç‡
        train_acc = 0.0
        for i in range(n_classes):
            acc, correct, count = logger.get_class_accuracy(i)
            if acc is not None:
                train_acc += acc
            print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))
        
        labels = torch.cat(logger.batch_log['labels'], dim=0)
        probs = torch.cat(logger.batch_log['probs'], dim=0) # [N, C]
        train_acc /= n_classes
        train_loss = logger.batch_log['loss'] / len(labels)

        # è®¡ç®—AUC - ä½¿ç”¨ torchmetricsï¼ˆTensor/GPU åŸç”Ÿï¼‰
        if n_classes == 2:
            auroc = TM_AUROC(task='binary').to(probs.device)
            train_auc = float(auroc(probs[:, 1], labels.long()).item())
        else:
            auroc = TM_AUROC(task='multiclass', num_classes=n_classes, average='macro').to(probs.device)
            train_auc = float(auroc(probs, labels.long()).item())
        
        metrics = {
            'loss': train_loss,
            'acc': train_acc,
            'auc': train_auc
        }
        return metrics

    def _validate_single_epoch(self, cur: int, epoch: int, loader: DataLoader, model: nn.Module, early_stopping=None) -> Tuple[Dict, bool]:
        """éªŒè¯å‡½æ•°"""
        model.eval()
        n_classes = self.model_config['n_classes']
        logger = Logger(n_classes=n_classes)
        
        # é‡ç½®æ¨¡å‹çš„group_logitså’Œgroup_labelsï¼Œç¡®ä¿éªŒè¯æ—¶ä»å¹²å‡€çŠ¶æ€å¼€å§‹
        if hasattr(model, 'group_logits'):
            model.group_logits = []
        if hasattr(model, 'group_labels'):
            model.group_labels = []
        
        with torch.no_grad():
            for batch_idx, (data, label) in enumerate(loader):
                label = label.to(device)
                
                # data ç°åœ¨æ˜¯å­—å…¸æ ¼å¼ï¼Œæ¯ä¸ªchannelåŒ…å«ä¸€ä¸ªå¼ é‡
                # éœ€è¦å°†æ¯ä¸ªchannelçš„å¼ é‡ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
                for channel in data:
                    data[channel] = data[channel].to(device)

                results = model(data, label)
                Y_prob = results['probabilities']
                Y_hat = results['predictions']

                results['labels'] = label
                loss = self.loss_fn(results['logits'], results['labels'], results)
                logger.log_batch(Y_hat, label, Y_prob, loss)
        
        # åœ¨éªŒè¯ç»“æŸæ—¶è®¡ç®—AUCæŸå¤±
        if hasattr(model, 'group_loss_fn') and hasattr(model, 'group_logits') and model.group_logits:
            results['group_loss'] = model.group_loss_fn(results)
            logger.batch_log['loss'] += results['group_loss']
            
        val_loss = logger.batch_log['loss']/len(loader)
        val_acc = logger.get_overall_accuracy()
        labels = torch.cat(logger.batch_log['labels'], dim=0)
        prob = torch.cat(logger.batch_log['probs'], dim=0)
        
        if n_classes == 2:
            auroc = TM_AUROC(task='binary').to(prob.device)
            auc = float(auroc(prob[:, 1], labels.long()).item())
        else:
            auroc = TM_AUROC(task='multiclass', num_classes=n_classes, average='macro').to(prob.device)
            auc = float(auroc(prob, labels.long()).item())

        print('\nVal Set, val_loss: {:.4f}, val_accuracy: {:.4f}, auc: {:.4f}'.format(val_loss, val_acc, auc))
        
        if hasattr(model, 'verbose_items'):
            results['is_epoch'] = True
            results['total_loss'] = val_loss
            items = model.verbose_items(results)
            if len(items) > 0:
                print('- ' + ' '.join([f'{key}: {value:.4f}' for key, value in items]))

        for i in range(n_classes):
            acc, correct, count = logger.get_class_accuracy(i)
            print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))
        
        val_metrics = {
            'loss': val_loss,
            'acc': val_acc,
            'auc': auc
        }

        if early_stopping:
            assert self.results_dir
            # æ ¹æ®é…ç½®çš„ metric é€‰æ‹©ä½¿ç”¨çš„æŒ‡æ ‡
            metric_name = getattr(early_stopping, 'metric', 'auc')
            if metric_name == 'loss':
                score = val_loss
            elif metric_name == 'auc':
                score = auc
            elif metric_name == 'acc' or metric_name == 'accuracy':
                score = val_acc
            else:
                # é»˜è®¤ä½¿ç”¨ auc
                score = auc
                print(f"âš ï¸ è­¦å‘Š: æœªçŸ¥çš„æ—©åœæŒ‡æ ‡ '{metric_name}'ï¼Œä½¿ç”¨é»˜è®¤å€¼ 'auc'")
            
            early_stopping(epoch, score, model, 
                         ckpt_name=os.path.join(self.results_dir, "s_{}_checkpoint.pt".format(cur)))
            
            if early_stopping.early_stop:
                print("Early stopping")
                return val_metrics, True

        return val_metrics, False

    def _evaluate_model(self, loader: DataLoader, model: nn.Module, drop_prob: Optional[float] = None) -> Tuple[Dict, float, float, Logger]:
        """
        æ¨¡å‹è¯„ä¼°æ€»ç»“
        
        Args:
            loader: æ•°æ®åŠ è½½å™¨
            model: æ¨¡å‹
            drop_prob: æ¨¡æ€ä¸¢å¼ƒæ¦‚ç‡ï¼ˆ0.0-1.0ï¼‰ï¼Œåœ¨ forward æ—¶ä¼ å…¥æ¨¡å‹
        """
        model.eval()
        logger = Logger(n_classes=self.model_config['n_classes'])

        # é‡ç½®æ¨¡å‹çš„group_logitså’Œgroup_labelsï¼Œç¡®ä¿æµ‹è¯•æ—¶ä»å¹²å‡€çŠ¶æ€å¼€å§‹
        if hasattr(model, 'group_logits'):
            model.group_logits = []
        if hasattr(model, 'group_labels'):
            model.group_labels = []

        dataset_ref = loader.dataset
        case_ids_list: List[str]
        if hasattr(dataset_ref, 'case_ids'): # ç›´æ¥æ•°æ®é›†ï¼ˆæ‹¥æœ‰ case_ids å±æ€§ï¼‰
            base = dataset_ref.case_ids
            case_ids_list = list(base) if not isinstance(base, list) else base
        elif hasattr(dataset_ref, 'dataset'):
            case_ids_list = dataset_ref.dataset.case_ids
        else:
            raise ValueError(f"Expected dataset with case_ids attribute, got {type(dataset_ref)}")  
        patient_results = {}
 
        for batch_idx, (data, label) in enumerate(loader):
            label = label.to(device)
            for channel in data:
                data[channel] = data[channel].to(device)
            case_id = case_ids_list[batch_idx]
            with torch.inference_mode():
                # ä¼ å…¥ drop_prob å‚æ•°
                if drop_prob is not None:
                    results = model(data, label, drop_prob=drop_prob)
                else:
                    results = model(data, label)
                Y_prob = results['probabilities']
                Y_hat = results['predictions']
            
            results['labels'] = label
            loss = self.loss_fn(results['logits'], results['labels'], results)
            logger.log_batch(Y_hat, label, Y_prob, loss)
            
            patient_results.update({case_id: {'case_id': np.array(case_id), 'prob': Y_prob.cpu().numpy(), 'label': label.item()}})
        
        # åœ¨æµ‹è¯•ç»“æŸæ—¶è®¡ç®—AUCæŸå¤±
        if hasattr(model, 'group_loss_fn') and hasattr(model, 'group_logits') and model.group_logits:
            results['group_loss'] = model.group_loss_fn(results)
            logger.batch_log['loss'] += results['group_loss']
        
        test_loss = logger.batch_log['loss']/len(loader)
        test_acc = logger.get_overall_accuracy()
        
        if hasattr(model, 'verbose_items'):
            results['is_epoch'] = True
            results['total_loss'] = test_loss
            items = model.verbose_items(results)
            if len(items) > 0:
                print('- ' + ' '.join([f'{key}: {value:.4f}' for key, value in items]))
        
        labels = torch.cat(logger.batch_log['labels'], dim=0)
        prob = torch.cat(logger.batch_log['probs'], dim=0)

        if self.model_config['n_classes'] == 2:
            auroc = TM_AUROC(task='binary').to(prob.device)
            auc = float(auroc(prob[:, 1], labels.long()).item())
        else:
            auroc = TM_AUROC(task='multiclass', num_classes=self.model_config['n_classes'], average='macro').to(prob.device)
            auc = float(auroc(prob, labels.long()).item())
        
        print('\nTest Set, test_loss: {:.4f}, test_accuracy: {:.4f}, auc: {:.4f}'.format(test_loss, test_acc, auc))

        return patient_results, test_acc, auc, logger

    def evaluate_fold(self,
                      datasets: Tuple[Any, Any, Any],
                      fold_idx: int,
                      checkpoint_path: str,
                      drop_prob: Optional[float] = None) -> Tuple[Dict, float, Optional[float], float, Optional[float]]:
        """
        ä»…è¯„æµ‹æ¥å£ï¼šåŠ è½½æŒ‡å®šcheckpointï¼Œåœ¨ç»™å®šdatasetsçš„æµ‹è¯•é›†ä¸Šè¯„æµ‹ã€‚

        Args:
            datasets: (train_dataset, val_dataset, test_dataset) å…ƒç»„ï¼Œæµ‹è¯•é›†å°†è¢«ç”¨äºè¯„æµ‹
            fold_idx: å½“å‰foldç´¢å¼•ï¼ˆç”¨äºæ—¥å¿—æ‰“å°/å…¼å®¹æ¥å£ï¼‰
            checkpoint_path: æ¨¡å‹æƒé‡è·¯å¾„ï¼ˆæ¨èä¸º train_fold ä¿å­˜çš„ s_{fold}_checkpoint.ptï¼‰
            drop_prob: æ¨¡æ€ä¸¢å¼ƒæ¦‚ç‡ï¼ˆ0.0-1.0ï¼‰ï¼Œåœ¨ forward æ—¶ä¼ å…¥æ¨¡å‹

        Returns:
            (results_dict, test_auc, None, test_acc, None) ä¸ train_fold ç»“æœå½¢å¼å¯¹é½ï¼ˆéªŒè¯æŒ‡æ ‡ç½®ä¸º Noneï¼‰
        """
        print(f"\n[Evaluate] Fold {fold_idx} | checkpoint: {checkpoint_path}")

        # æ¯æ¬¡è¯„æµ‹éƒ½é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¸å¤ç”¨ä¹‹å‰çš„æ¨¡å‹çŠ¶æ€ï¼‰
        model = self._init_model()
        print(f"ğŸ”§ åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹ï¼Œid={id(model)}")
        self.loss_fn = model.loss_fn  # æ›´æ–° loss_fn ä¸ºå½“å‰æ¨¡å‹çš„
        
        # åŠ è½½checkpointï¼ˆä¸è®­ç»ƒæ—¶çš„loadæ–¹å¼ä¸€è‡´ï¼‰
        state = torch.load(checkpoint_path, map_location=device)
        print(f"ğŸ“¦ checkpointåŠ è½½æˆåŠŸï¼Œstate_dict keysæ•°é‡: {len(state.keys())}")
        
        if hasattr(model, 'transfer_layer') and hasattr(model, 'create_transfer_layer'):
            # ä»checkpointä¸­æ‰¾åˆ°æ‰€æœ‰transfer_layerçš„é€šé“
            transfer_layer_channels = {}
            for key in state.keys():
                if 'transfer_layer.' in key:
                    # æå–é€šé“åå’Œæƒé‡ç±»å‹ï¼Œä¾‹å¦‚ "transfer_layer.clinical=val.weight" -> ("clinical=val", "weight")
                    parts = key.split('.')
                    if len(parts) >= 3:
                        channel_name = parts[1]  # ä¾‹å¦‚ "clinical=val"
                        weight_type = parts[2]  # "weight" æˆ– "bias"
                        
                        if channel_name not in transfer_layer_channels:
                            transfer_layer_channels[channel_name] = {}
                        transfer_layer_channels[channel_name][weight_type] = state[key]
            
            # æ ¹æ®checkpointä¸­çš„æƒé‡åˆ›å»ºå¯¹åº”çš„transfer_layer
            if hasattr(model, 'output_dim'):
                output_dim = model.output_dim
                print(f"ğŸ”§ é¢„åˆ›å»º {len(transfer_layer_channels)} ä¸ªtransfer_layerä»¥åŒ¹é…checkpoint...")
                for channel_name, weights in transfer_layer_channels.items():
                    if channel_name not in model.transfer_layer:
                        # ä»weightçš„å½¢çŠ¶æ¨æ–­input_dim: weightå½¢çŠ¶æ˜¯ [output_dim, input_dim]
                        if 'weight' in weights:
                            weight_tensor = weights['weight']
                            if len(weight_tensor.shape) == 2:
                                input_dim = weight_tensor.shape[1]  # ç¬¬äºŒç»´æ˜¯input_dim
                                # åˆ›å»ºtransfer_layer
                                transfer_layer = model.create_transfer_layer(input_dim)
                                model.transfer_layer[channel_name] = transfer_layer
                                print(f"   âœ… åˆ›å»º transfer_layer.{channel_name} (input_dim={input_dim}, output_dim={output_dim})")
                            else:
                                print(f"   âš ï¸ æ— æ³•æ¨æ–­ {channel_name} çš„input_dim: weightå½¢çŠ¶å¼‚å¸¸ {weight_tensor.shape}")
                        else:
                            print(f"   âš ï¸ checkpointä¸­ç¼ºå°‘ {channel_name}.weightï¼Œæ— æ³•åˆ›å»ºtransfer_layer")
        
        # åˆ†æcheckpointä¸­çš„æƒé‡ç±»å‹
        transfer_layer_keys = [k for k in state.keys() if 'transfer_layer.' in k]
        core_keys = [k for k in state.keys() if 'transfer_layer.' not in k]
        
        print(f"ğŸ“Š checkpointæƒé‡åˆ†æ:")
        print(f"   æ ¸å¿ƒæƒé‡: {len(core_keys)} ä¸ª")
        print(f"   transfer_layeræƒé‡: {len(transfer_layer_keys)} ä¸ª")
        
        # ç°åœ¨æ‰€æœ‰éœ€è¦çš„transfer_layeréƒ½å·²åˆ›å»ºï¼Œå°è¯•ä½¿ç”¨strict=Trueç¡®ä¿å®Œå…¨åŒ¹é…
        # å¦‚æœè¿˜æœ‰ä¸åŒ¹é…ï¼Œå†é™çº§åˆ°strict=False
        try:
            missing_keys, unexpected_keys = model.load_state_dict(state, strict=True)
            print(f"âœ… ä½¿ç”¨strict=TrueæˆåŠŸåŠ è½½æ‰€æœ‰æƒé‡ï¼ˆå®Œå…¨åŒ¹é…ï¼‰")
        except RuntimeError as e:
            # å¦‚æœstrict=Trueå¤±è´¥ï¼Œä½¿ç”¨strict=Falseä½†ä¼šè¯¦ç»†æŠ¥å‘Š
            print(f"âš ï¸ strict=TrueåŠ è½½å¤±è´¥: {e}")
            print(f"ğŸ”§ é™çº§åˆ°strict=FalseåŠ è½½...")
            missing_keys, unexpected_keys = model.load_state_dict(state, strict=False)
        
        # æ£€æŸ¥æ ¸å¿ƒæƒé‡æ˜¯å¦éƒ½åŠ è½½äº†
        model_core_keys = set([k for k in model.state_dict().keys() if 'transfer_layer.' not in k])
        checkpoint_core_keys = set(core_keys)
        loaded_core_keys = model_core_keys & checkpoint_core_keys
        
        if missing_keys:
            missing_core = [k for k in missing_keys if 'transfer_layer.' not in k]
            missing_transfer = [k for k in missing_keys if 'transfer_layer.' in k]
            if missing_core:
                print(f"âš ï¸ è­¦å‘Šï¼šç¼ºå°‘ä»¥ä¸‹æ ¸å¿ƒæƒé‡ï¼ˆå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼‰: {len(missing_core)} ä¸ª")
                for key in missing_core[:5]:
                    print(f"    - {key}")
                if len(missing_core) > 5:
                    print(f"    ... è¿˜æœ‰ {len(missing_core) - 5} ä¸ª")
            if missing_transfer:
                print(f"â„¹ï¸ ä¿¡æ¯ï¼šç¼ºå°‘ä»¥ä¸‹transfer_layeræƒé‡ï¼ˆå°†åœ¨forwardæ—¶åŠ¨æ€åˆ›å»ºï¼‰: {len(missing_transfer)} ä¸ª")
        
        if unexpected_keys:
            unexpected_transfer = [k for k in unexpected_keys if 'transfer_layer.' in k]
            unexpected_other = [k for k in unexpected_keys if 'transfer_layer.' not in k]
            if unexpected_transfer:
                print(f"â„¹ï¸ ä¿¡æ¯ï¼šcheckpointä¸­æœ‰é¢å¤–çš„transfer_layeræƒé‡ï¼ˆå·²å¿½ç•¥ï¼Œä¸å½±å“è¯„æµ‹ï¼‰: {len(unexpected_transfer)} ä¸ª")
            if unexpected_other:
                print(f"âš ï¸ è­¦å‘Šï¼šcheckpointä¸­æœ‰æ„å¤–çš„å…¶ä»–æƒé‡: {len(unexpected_other)} ä¸ª")
                for key in unexpected_other[:5]:
                    print(f"    - {key}")
        
        # éªŒè¯æ ¸å¿ƒæƒé‡åŠ è½½æƒ…å†µ
        print(f"âœ… æ ¸å¿ƒæƒé‡åŠ è½½: {len(loaded_core_keys)}/{len(checkpoint_core_keys)} ä¸ª")
        if len(loaded_core_keys) == len(checkpoint_core_keys):
            print(f"âœ… æ‰€æœ‰æ ¸å¿ƒæƒé‡å·²æˆåŠŸåŠ è½½")
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šéƒ¨åˆ†æ ¸å¿ƒæƒé‡æœªåŠ è½½: {len(checkpoint_core_keys) - len(loaded_core_keys)} ä¸ª")
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()

        # ä»…æ„é€ æµ‹è¯•é›†æ•°æ®åŠ è½½å™¨
        _, _, test_split = datasets
        test_loader = get_split_loader(test_split, training=False, weighted=False, batch_size=1)

        # è¯„æµ‹ï¼ˆä¼ å…¥ drop_probï¼‰
        results_dict, test_acc, test_auc, _ = self._evaluate_model(test_loader, model, drop_prob=drop_prob)
        return results_dict, float(test_auc), None, float(test_acc), None

    def evaluate_with_checkpoint(self,
                                 datasets: Tuple[Any, Any, Any],
                                 fold_idx: int,
                                 checkpoint_path: str,
                                 drop_prob: Optional[float] = None) -> Tuple[Dict, float, Optional[float], float, Optional[float]]:
        """
        å…¼å®¹åï¼šç›´æ¥è°ƒç”¨ evaluate_foldã€‚
        
        Args:
            datasets: æ•°æ®é›†å…ƒç»„
            fold_idx: foldç´¢å¼•
            checkpoint_path: checkpointè·¯å¾„
            drop_prob: æ¨¡æ€ä¸¢å¼ƒæ¦‚ç‡ï¼ˆ0.0-1.0ï¼‰ï¼Œåœ¨ forward æ—¶ä¼ å…¥æ¨¡å‹
        """
        return self.evaluate_fold(datasets=datasets, fold_idx=fold_idx, checkpoint_path=checkpoint_path, drop_prob=drop_prob)
